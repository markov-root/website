(window.webpackJsonp=window.webpackJsonp||[]).push([[43],{397:function(e,t,a){"use strict";a.r(t);var n=a(17),o=Object(n.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("h1",{attrs:{id:"capabilities"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#capabilities"}},[e._v("#")]),e._v(" Capabilities")]),e._v(" "),t("BlogMeta"),e._v(" "),t("h1",{attrs:{id:"introduction"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#introduction"}},[e._v("#")]),e._v(" Introduction")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/qCW_Image_1.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[e._v("State-of-the-Art AI: We begin with a brief introduction to the current advancements in artificial intelligence as of 2024. We aim to acquaint readers with the latest breakthroughs across various domains such as language processing, vision, and robotics.")]),e._v(" "),t("p",[e._v("Foundation Models: The second section focuses on foundation models, the paradigm powering the state-of-the-art systems introduced in the previous section. We explain the key techniques underpinning the huge success of these models such as self-supervised learning, zero-shot learning, and fine-tuning. The section concludes by looking at the risks that the foundation model paradigm could pose such as power centralization, homogenization, and the potential for emergent capabilities.")]),e._v(" "),t("p",[e._v('Terminology: Before diving deeper, we establish the definitions that this book will be working with. This section explains why "capabilities" rather than "intelligence" is a more pragmatic measure for discussing AI risks. We also delineate key terms within the AI debate, such as Artificial General Intelligence (AGI), Artificial Super Intelligence (ASI), and Transformative AI (TAI). The section concludes by introducing the (t,n)-AGI framework which allows us to more concretely measure the level of AI capabilities on a continuous scale, rather than having to rely on discrete thresholds.')]),e._v(" "),t("p",[e._v("Leveraging Computation: In this section, we explore the importance of computation in AI's progress introducing the three main variables that govern the capabilities of today's foundation models - compute, data and parameter count. We explore scaling laws and hypotheses that predict the future capabilities of AI based on current scaling trends of these variables, offering insights into the computational strategies that could pave the way to AGI.")]),e._v(" "),t("p",[e._v("Forecasting: Finally, the chapter addresses the challenge of forecasting AI's future, using biological anchors as a method to estimate the computational needs for transformative AI. This section sets the groundwork for discussing AI takeoff dynamics, including speed, polarity, and homogeneity, offering a comprehensive view of potential futures shaped by AI development.")]),e._v(" "),t("h1",{attrs:{id:"state-of-the-art-ai"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#state-of-the-art-ai"}},[e._v("#")]),e._v(" State-of-the-Art AI")]),e._v(" "),t("p",[e._v("Over the last decade, the field of artificial intelligence (AI) has experienced a profound transformation, largely attributed to the successes in deep learning. This remarkable progress has redefined the boundaries of AI capabilities, challenging many preconceived notions of what machines can achieve. The following sections detail some of these advancements.")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/V7p_Image_1.jpeg",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*üòó* Once a benchmark is published, it takes less and less time to solve it. This can illustrate the accelerating progress in AI and how quickly AI benchmarks are ‚Äúsaturating‚Äù, and starting to surpass human performance on a variety of tasks. ("),t("strong",[t("a",{attrs:{href:"https://www.science.org/content/article/computers-ace-iq-tests-still-make-dumb-mistakes-can-different-tests-help",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("h2",{attrs:{id:"language"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#language"}},[e._v("#")]),e._v(" Language")]),e._v(" "),t("p",[t("strong",[e._v("Language-based tasks.")]),e._v(" There have been transformative changes in sequence and language-based tasks, primarily through the development of large language models (LLMs). Early language models in 2018 struggled to construct coherent sentences. The evolution from these to the advanced capabilities of GPT-3 (Generative Pre-Trained Transformer) and ChatGPT within less than 5 years is remarkable. These models demonstrate not only an improved capacity for generating text but also for responding to complex queries with nuanced, common-sense reasoning. Their performance in various question-answering tasks, including those requiring strategic thinking, has been particularly impressive.")]),e._v(" "),t("p",[e._v("**GPT-4. **One of the state-of-the-art language models in 2024 is OpenAI‚Äôs LLM GPT-4. In contrast with the text-only GPT-3 and follow-ups, GPT-4 is multimodal: it was trained on both text and images. This means that it can now not only generate text based on images but has also gained some other capabilities. GPT-4 saw an upgraded context window with up to 32k tokens (tokens ‚âà words). The short-term memory limit of an LLM can be thought of as the model's ability to retain information from previous tokens within a certain context window. GPT-4 is trained via next-token prediction (autoregressive self-supervised learning). In 2018 GPT-1 was barely able to count to 10, while in 2024 GPT-4 can implement complex programmatic functions among other things.")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/CAe_Image_2.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v('*: a list of "Nowhere near solved‚Äù [...] issues in AI, from "A brief history of AI", published in January 2021 ('),t("strong",[t("a",{attrs:{href:"https://www.amazon.com/Brief-History-Artificial-Intelligence-Where/dp/1250770742",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1)]),e._v("). They also say: ‚ÄúAt present, we have no idea how to get computers to do the tasks at the bottom of the list‚Äù. But everything in the category ‚ÄúNowhere near solved‚Äù has been solved by GPT-4 ("),t("strong",[t("a",{attrs:{href:"https://arxiv.org/abs/2303.12712",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1)]),e._v("), except human-level general intelligence.*")]),e._v(" "),t("p",[t("strong",[e._v("Scaling")]),e._v(". Remarkably, GPT-4 is trained using roughly the same methods as GPT-1, 2, and 3. The only significant difference is the size of the model and the data given to it during training. The size of the model has gone from 1.5B parameters to hundreds of billions of parameters, and datasets have become similarly larger and more diverse.")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/uW1_Image_3.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: How fast is AI Improving? ("),t("strong",[t("a",{attrs:{href:"https://theaidigest.org/progress-and-dangers",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[e._v("We have observed that just an expansion in scale has contributed to enhanced performance. This includes improvements in the ability to generate contextually appropriate responses, and highly diverse text across a range of domains. It has also contributed to overall improved understanding, and coherence. Most of those advances in the GPT series come from increasing the size and computation power behind the models, rather than fundamental shifts in architecture or training.")]),e._v(" "),t("p",[e._v("Here are some of the capabilities that have been emerging in the last few years:")]),e._v(" "),t("ul",[t("li",[t("p",[t("strong",[e._v("Few-shot and Zero-shot Learning")]),e._v(". The model's proficiency at understanding and executing tasks with minimal or no prior examples. 'Few-shot' means accomplishing the task after having seen a few examples in the context window, while 'Zero-shot' indicates performing the task without any specific examples ("),t("a",{attrs:{href:"https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v("). This also includes induction capabilities, i.e. identifying patterns and generalizing rules not present in the training, but only present in the current context window ("),t("a",{attrs:{href:"https://arxiv.org/abs/2005.14165",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(").")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Metacognition")]),e._v(". This refers to the ability to recognize its own knowledge and limitations, for example, being able to know the probability of the truth of something ("),t("a",{attrs:{href:"https://arxiv.org/abs/2207.05221",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(").")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Theory of Mind")]),e._v(". The capability to attribute mental states to itself and others, which helps in predicting human behaviors and responses for more nuanced interactions ("),t("a",{attrs:{href:"https://arxiv.org/abs/2302.02083",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(", "),t("a",{attrs:{href:"https://arxiv.org/abs/2402.06044",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(").")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Tool Use")]),e._v(". Being able to interact with external tools, like using a calculator or browsing the internet, expanding its problem-solving abilities ("),t("a",{attrs:{href:"https://arxiv.org/abs/2307.16789",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(").")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Self-correction")]),e._v(". The model's ability to identify and correct its own mistakes, which is crucial for improving the accuracy of AI-generated content ("),t("a",{attrs:{href:"https://arxiv.org/abs/2303.11366",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(").")])])]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/vR6_Image_4.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: An example of a mathematical problem solved by GPT-4 using Chain of Thought (CoT), from the paper ‚ÄúSparks of Artificial General Intelligence‚Äù ("),t("strong",[t("a",{attrs:{href:"https://arxiv.org/abs/2303.12712",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1)]),e._v(").*")]),e._v(" "),t("ul",[t("li",[t("p",[t("strong",[e._v("Reasoning")]),e._v(". The advancements in LLMs have also led to significant improvements in the ability to process and generate logical chains of thought and reasoning. This is particularly important in problem-solving tasks where a straightforward answer isn't immediately available, and a step-by-step reasoning process is required. ("),t("a",{attrs:{href:"https://arxiv.org/abs/2303.12712",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(")")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Programming ability")]),e._v(". In coding, AI models have progressed from basic code autocompletion to writing sophisticated, functional programs.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Scientific & Mathematical ability")]),e._v(". In mathematics, AI's have assisted in the subfield of automatic theorem proving for decades. Today's models continue to assist in solving complex problems. AI can even achieve a gold medal level in the mathematical Olympiad by solving geometry problems ("),t("a",{attrs:{href:"https://www.nature.com/articles/s41586-023-06747-5",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(").")])])]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/mPJ_Image_5.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: GPT-4 solves some tasks that GPT-3.5 was unable to, like the uniform bar examination, where GPT-4 scores 90% compared to 10% for GPT-3.5. GPT-4 is also capable of vision processing, and the added vision component had only a minor impact, but it helped others tremendously.  ("),t("strong",[t("a",{attrs:{href:"https://openai.com/research/gpt-4",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("h2",{attrs:{id:"image-generation"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#image-generation"}},[e._v("#")]),e._v(" Image Generation")]),e._v(" "),t("p",[e._v("The leap forward in image generation is not just in accuracy, but also in the ability to handle complex, real-world images. The latter, particularly with the advent of Generative Adversarial Networks (GANs) in 2014, has shown an astounding rate of progress. The quality of images generated by AI has evolved from simple, blurry representations to highly detailed and creative scenes, often in response to intricate language prompts.")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/dwX_Image_6.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: An example of state-of-the-art image recognition. The Segment Anything Model (SAM) by Meta‚Äôs FAIR (Fundamental AI Research) lab, can classify and segment visual data at highly precise levels. The detection is performed without the need to annotate images. ("),t("strong",[t("a",{attrs:{href:"https://viso.ai/deep-learning/segment-anything-model-sam-explained/",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/PBp_Image_7.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: An example of the evolution of image generation. At the top left, starting from GANs (Generative Adversarial Networks) to the bottom right, an image from MidJourney V5.*")]),e._v(" "),t("p",[e._v("The rate of progress within a single year alone is quite astounding as is seen from the improvements between the V1 of the MidJourney image generation model in early 2022, to the V6 in December 2023.")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/wh2_Image_8.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: MidJourney AI image generation over 2022-2023. Prompt: high-quality photography of a young Japanese woman smiling, backlighting, natural pale light, film camera, by Rinko Kawauchi, HDR ("),t("strong",[t("a",{attrs:{href:"https://goldpenguin.org/blog/midjourney-v1-to-v6-evolution/",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("h2",{attrs:{id:"multi-cross-modality"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#multi-cross-modality"}},[e._v("#")]),e._v(" Multi & Cross modality")]),e._v(" "),t("p",[e._v("AI systems are becoming increasingly multimodal. This means that they can process images, text, audio, vision, and robotics using the same model. So they are trained using multiple different ‚Äúmodes‚Äù and can translate between them after deployment.")]),e._v(" "),t("p",[t("strong",[e._v("Cross-modality")]),e._v(". A model is called cross-modal when the input of a model is in one modality (e.g. text) and the output is in another modality (e.g. image). The section on computer vision showed fast progress between 2014 and 2020 in cross-modality. We went from text-to-image models only capable of generating black-and-white pixelated images of faces, to models capable of generating an image of any textual prompt. More examples of cross-modality include OpenAIs Whisper ("),t("a",{attrs:{href:"https://cdn.openai.com/papers/whisper.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(") which is capable of speech-to-text transcription.")]),e._v(" "),t("p",[t("strong",[e._v("Multi-modality")]),e._v(". A model is called multi-modal when both the inputs and outputs of a model can be in more than one modality. E.g. audio-to-text, video-to-text, text-to-image, etc‚Ä¶")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/ndf_Image_10.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure:")])]),e._v("* Image-to-text and text-to-image multimodality from the Flamingo model. ("),t("strong",[t("a",{attrs:{href:"https://arxiv.org/abs/2204.14198",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[e._v("DeepMind‚Äôs 2022 Flamingo model, could be ‚Äú"),t("em",[e._v("rapidly adapted to various image/video understanding tasks")]),e._v("‚Äù and ‚Äú"),t("em",[e._v("is also capable of multi-image visual dialogue")]),e._v("‚Äù. ("),t("a",{attrs:{href:"https://arxiv.org/abs/2204.14198",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(') Similarly, DeepMind‚Äôs 2022 Gato model, was called a "Generalist Agent". It was a single network with the same weights which could ‚Äú'),t("em",[e._v("play Atari, caption images, chat, stack blocks with a real robot arm, and much more")]),e._v("‚Äù. ("),t("a",{attrs:{href:"https://arxiv.org/abs/2205.06175",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(") Continuing this trend, DeepMind‚Äôs 2023 Google Gemini model could be called a Large Multimodal Model (LMM). The paper described Gemini as ‚Äú"),t("em",[e._v("natively multimodal")]),e._v("‚Äù and claimed to be able to ‚Äú"),t("em",[e._v("seamlessly combine their capabilities across modalities (e.g. extracting information and spatial layout out of a table, a chart, or a figure) with the strong reasoning capabilities of a language model (e.g. its state-of-art-performance in math and coding)")]),e._v("‚Äù("),t("a",{attrs:{href:"https://arxiv.org/abs/2312.11805",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(")")]),e._v(" "),t("h2",{attrs:{id:"robotics"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#robotics"}},[e._v("#")]),e._v(" Robotics")]),e._v(" "),t("p",[e._v("The field of robotics has also been progressing alongside artificial intelligence. In this section, we provide a couple of examples where these two fields are merging, highlighting some robots using inspiration from machine learning techniques to make advancements.")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/VZl_Image_9.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure:")])]),e._v("* Researchers used Model-Free Reinforcement Learning to automatically learn quadruped locomotion in only 20 minutes in the real world instead of in simulated environments. The Figure shows examples of learned gaits on a variety of real-world terrains. ("),t("strong",[t("a",{attrs:{href:"https://arxiv.org/abs/2208.07860",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[t("strong",[e._v("Advances in robotics")]),e._v(". At the forefront of robotic advancements is PaLM-E, a general-purpose, embodied model with 562 billion parameters that integrates vision, language, and robot data for real-time manipulator control and excels in language tasks involving geospatial reasoning. ("),t("a",{attrs:{href:"https://arxiv.org/abs/2303.03378",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(")")]),e._v(" "),t("p",[e._v("Simultaneously, developments in vision-language models have led to breakthroughs in fine-grained robot control, with models like RT-2 showing significant capabilities in object manipulation and multimodal reasoning. RT-2 demonstrates how we can use LLM-inspired prompting methods (chain-of-thought), to learn a self-contained model that can both plan long-horizon skill sequences and predict robot actions. ("),t("a",{attrs:{href:"https://deepmind.google/discover/blog/rt-2-new-model-translates-vision-and-language-into-action/",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(")")]),e._v(" "),t("p",[e._v("Mobile ALOHA is another example of combining modern machine learning techniques with robotics. Trained using supervised behavioral cloning, the robot can autonomously perform complex tasks ‚Äú"),t("em",[e._v("such as sauteing and serving a piece of shrimp, opening a two-door wall cabinet to store heavy cooking pots, calling and entering an elevator, and lightly rinsing a used pan using a kitchen faucet.")]),e._v("‚Äù ("),t("a",{attrs:{href:"https://arxiv.org/abs/2401.02117",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(") Such advancements not only demonstrate the increasing sophistication and applicability of robotic systems but also highlight the potential for further groundbreaking developments in autonomous technologies.")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/3A4_Image_10.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure:")])]),e._v("* DeepMinds RT-2 can both plan long-horizon skill sequences and predict robot actions using inspiration from LLM prompting techniques (chain-of-thought). ("),t("strong",[t("a",{attrs:{href:"https://deepmind.google/discover/blog/rt-2-new-model-translates-vision-and-language-into-action/",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("h2",{attrs:{id:"playing-games"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#playing-games"}},[e._v("#")]),e._v(" Playing Games")]),e._v(" "),t("p",[t("strong",[e._v("AI and board games.")]),e._v(" AI has made continuous progress in game playing for decades. Starting from AIs beating the world champion at chess in 1997 ("),t("a",{attrs:{href:"https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v("), Scrabble in 2006 ("),t("a",{attrs:{href:"https://content.time.com/time/specials/packages/article/0,28804,2049187_2049195_2049083,00.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(") to DeepMind‚Äôs "),t("a",{attrs:{href:"https://www.deepmind.com/research/highlighted-research/alphago",target:"_blank",rel:"noopener noreferrer"}},[e._v("AlphaGo"),t("OutboundLink")],1),e._v(" in 2016, which was good enough to defeat the world champion in the game of Go, a game assumed to be notoriously difficult for AI. Within a year, the next model "),t("a",{attrs:{href:"https://www.deepmind.com/blog/alphazero-shedding-new-light-on-chess-shogi-and-go",target:"_blank",rel:"noopener noreferrer"}},[e._v("AlphaZero"),t("OutboundLink")],1),e._v(" trained through self-play had mastered multiple games of Go, chess, and shogi reaching a superhuman level after less than three days of training.")]),e._v(" "),t("p",[t("strong",[e._v("AI and video games.")]),e._v(" We started using machine learning techniques on simple Atari games in 2013 ("),t("a",{attrs:{href:"https://arxiv.org/abs/1312.5602",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v("). By 2019, OpenAI Five defeated the world champions at DOTA2 ("),t("a",{attrs:{href:"https://openai.com/research/openai-five-defeats-dota-2-world-champions",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v("), while in the same year, DeepMind‚Äôs AlphaStar beat professional esports players at StarCraft II ("),t("a",{attrs:{href:"https://www.deepmind.com/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v("). Both these games require thousands of actions in a row at a high number of actions per minute. In 2020 DeepMind MuZero model, described as ‚Äú"),t("em",[e._v("a significant step forward in the pursuit of general-purpose algorithms")]),e._v("‚Äù ("),t("a",{attrs:{href:"https://www.deepmind.com/blog/muzero-mastering-go-chess-shogi-and-atari-without-rules",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v("), was capable of playing Atari games, Go, chess, and shogi without even being told the rules.")]),e._v(" "),t("p",[e._v("In recent years, AI's capability has extended to open-ended environments like Minecraft, showcasing an ability to perform complex sequences of actions. In strategy games, Meta‚Äôs Cicero displayed intricate strategic negotiation and deception skills in natural language for the game Diplomacy ("),t("a",{attrs:{href:"https://arxiv.org/abs/2210.05492",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(").")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/KMJ_Image_11.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: A map of diplomacy and the dialog box where the AI negotiates. ("),t("strong",[t("a",{attrs:{href:"https://www.youtube.com/watch?v=u5192bvUS7k&t=2216s",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[e._v('??? note "Example of Voyager: Planning and Continuous Learning in Minecraft with GPT-4"')]),e._v(" "),t("tab",[t("p",[e._v("Voyager ("),t("a",{attrs:{href:"https://arxiv.org/abs/2305.16291",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(") stands as a particularly impressive example of the capabilities of AI in continuous learning environments. This AI is designed to play Minecraft, a task that involves a significant degree of planning and adaptive learning. What makes Voyager so remarkable is its ability to learn continuously and progressively within the game's environment, using GPT-4 contextual reasoning abilities to plan and write the code necessary for each new challenge. Starting from scratch in a single game session, Voyager initially learns to navigate the virtual world, engage and defeat enemies, and remember all these skills in its long-term memory. As the game progresses, it continues to learn and store new skills, leading up to the challenging task of mining diamonds, a complex activity that requires a deep understanding of the game mechanics and strategic planning. The ability of Voyager to integrate new information continuously and utilize it effectively showcases the potential of AI in managing complex, changing environments and performing tasks that require a long-term buildup of knowledge and skills.")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/ccN_Image_12.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: "),t("strong",[e._v("Voyager discovers new Minecraft items and skills continually by self-driven exploration, significantly outperforming the baselines. (")]),t("a",{attrs:{href:"https://arxiv.org/abs/2305.16291",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v("**)*")])]),e._v(" "),t("h1",{attrs:{id:"foundation-models"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#foundation-models"}},[e._v("#")]),e._v(" Foundation Models")]),e._v(" "),t("p",[t("strong",[e._v("What are foundation models?")]),e._v(' Foundation models represent a fundamental shift in how we develop AI systems. Rather than building specialized models for specific tasks, we now train large-scale models that serve as a "foundation" for many different applications. These models can be fine-tuned or prompted to perform specific tasks, similar to how we can build many different types of buildings using the same base structure ('),t("a",{attrs:{href:"https://arxiv.org/abs/2108.07258",target:"_blank",rel:"noopener noreferrer"}},[e._v("Bommasani et al., 2022"),t("OutboundLink")],1),e._v(").")]),e._v(" "),t("p",[e._v('???+ note "Foundation Models - Video Introduction"')]),e._v(" "),t("tab",[t("iframe",{staticStyle:{width:"100%","aspect-ratio":"16 / 9"},attrs:{frameborder:"0",allowfullscreen:"",src:"https://www.youtube.com/embed/kK3NmQT241w"}}),e._v(" "),t("p",[e._v('!!! warning "This video is optional and not necessary to understand the text."')])]),e._v(" "),t("p",[t("strong",[e._v("What makes foundation models important for AI safety?")]),e._v(" The reason we start this entire book by talking about foundation models is because this shift toward centralized general-purpose systems, rather than narrow specialized ones, introduces new risks around emergence, dual-use, and capability generalization. Their ability to learn broad, transferable capabilities has led to increasingly sophisticated behaviors emerging from relatively simple training objectives ("),t("a",{attrs:{href:"https://arxiv.org/abs/2206.07682",target:"_blank",rel:"noopener noreferrer"}},[e._v("Wei et al., 2022"),t("OutboundLink")],1),e._v("). Complex capabilities, combined generality and scale, means we need to seriously consider safety risks that previously seemed theoretical or distant.")]),e._v(" "),t("p",[t("strong",[e._v("Why did the paradigm of foundation models come about?")]),e._v(" Training specialized AI models for every task was inefficient and limiting. Traditional approaches required collecting labeled datasets and building custom models for each new application. This meant progress was bottlenecked by the need for human-labeled data and the inability to transfer knowledge between tasks effectively. Foundation models solved these limitations through self-supervised learning on massive unlabeled datasets - allowing them to learn general-purpose capabilities that can be adapted to specific tasks. Advances in specialized hardware and parallelism (e.g., large clusters of NVIDIA GPUs), new developments in neural network architectures (e.g. transformers), and the increased easy access to vast amounts of online data all contributed to solving this problem. The short answer is foundation models came about due to the bitter lesson. We talk about this a lot more in the next section.")]),e._v(" "),t("p",[t("strong",[e._v("How did the foundation model paradigm emerge?")]),e._v(" The traditional approach of training specialized AI models for every task proved inefficient and limiting. Progress was bottlenecked by the need for human-labeled data and the inability to transfer knowledge between tasks effectively. Foundation models overcame these limitations through self-supervised learning on massive unlabeled datasets. This breakthrough was enabled by advances in specialized hardware like GPUs, new neural architectures like transformers, and increased access to vast amounts of online data ("),t("a",{attrs:{href:"https://arxiv.org/abs/2001.08361",target:"_blank",rel:"noopener noreferrer"}},[e._v("Kaplan et al., 2020"),t("OutboundLink")],1),e._v('). This progression follows what we call the "bitter lesson" in AI development, which we\'ll discuss in detail in the next section.')]),e._v(" "),t("p",[t("strong",[e._v("What are examples of foundation models?")]),e._v(" Foundation models now span multiple domains of artificial intelligence. In language processing, models like GPT-4 and Claude have demonstrated sophisticated capabilities in understanding and generating human language, from simple conversations to complex reasoning tasks ("),t("a",{attrs:{href:"https://arxiv.org/abs/2303.08774",target:"_blank",rel:"noopener noreferrer"}},[e._v("OpenAI, 2023"),t("OutboundLink")],1),e._v("). In computer vision, models like DALL-E 3 and Stable Diffusion have revolutionized image understanding and generation, creating highly realistic images from text descriptions ("),t("a",{attrs:{href:"https://cdn.openai.com/papers/dall-e-3.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Betker et al., 2023"),t("OutboundLink")],1),e._v("). We're seeing an increasing trend toward multi-modal foundation models like GPT-4V and Gemini that can seamlessly work across different types of data - processing and generating text, images, code, and more ("),t("a",{attrs:{href:"https://arxiv.org/abs/2312.11805",target:"_blank",rel:"noopener noreferrer"}},[e._v("Google, 2023"),t("OutboundLink")],1),e._v("). Even in reinforcement learning, where traditionally models were trained for specific tasks, we're seeing foundation models like Gato demonstrate the ability to learn general-purpose behaviors that can be adapted to various tasks ("),t("a",{attrs:{href:"https://arxiv.org/abs/2205.06175",target:"_blank",rel:"noopener noreferrer"}},[e._v("Reed et al., 2022"),t("OutboundLink")],1),e._v(").")]),e._v(" "),t("p",[t("strong",[e._v("Why do we distinguish between foundation models and frontier models?")]),e._v(" When discussing AI safety, it's important to understand the distinction between foundation models and frontier models. Frontier models represent the cutting edge of AI capabilities - they are the most advanced models in their respective domains. While many frontier models are also foundation models (like Claude 3.5 Sonnet), this isn't always the case. For example, AlphaFold, while being a frontier model in protein structure prediction, isn't typically considered a foundation model because it's specialized for a single task rather than serving as a general foundation for multiple applications ("),t("a",{attrs:{href:"https://pubmed.ncbi.nlm.nih.gov/34265844/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Jumper et al., 2021"),t("OutboundLink")],1),e._v("). This distinction becomes particularly important because most AI safety research and regulation frameworks focus on frontier models due to their advanced capabilities. When discussions about AI safety reference \"foundation models,\" they're typically referring specifically to frontier foundation models - those foundation models that also represent the current state-of-the-art in capabilities. Understanding this distinction helps us better target and implement safety measures where they're most critically needed.")]),e._v(" "),t("h2",{attrs:{id:"training"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#training"}},[e._v("#")]),e._v(" Training")]),e._v(" "),t("p",[t("strong",[e._v("How are foundation models trained differently from traditional AI systems?")]),e._v(" One key innovation of foundation models is their training paradigm. Generally, foundation models use a two-stage training process. First, they go through what we call a pre-training, and then second, they can be adapted through various mechanisms like fine-tuning or scaffolding to perform specific tasks. Rather than learning from human-labeled examples for specific tasks, these models learn by finding patterns in huge amounts of unlabeled data. This shift toward self-supervised learning on massive datasets fundamentally changes not just how models learn, but also what kinds of capabilities and risks might emerge ("),t("a",{attrs:{href:"https://arxiv.org/abs/2108.07258",target:"_blank",rel:"noopener noreferrer"}},[e._v("Bommasani et al., 2022"),t("OutboundLink")],1),e._v("). From a safety perspective, this means we need to understand both how these training methods work and how they might lead to unexpected behaviors.")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/uxE-.png",alt:"https://www.artificialintelligence.news/pathal/uploads/2021/09/2021-foundationmodel-1024x692.png"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v('*: Bommasani Rishi et. al. (2022) "'),t("strong",[t("a",{attrs:{href:"https://arxiv.org/pdf/2108.07258.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("On the Opportunities and Risks of Foundation Models"),t("OutboundLink")],1)]),e._v('"*')]),e._v(" "),t("p",[t("strong",[e._v("What is pre-training?")]),e._v(" Pre-training is the initial phase where the model learns general patterns and knowledge from massive datasets of millions or billions of examples. During this phase, the model isn't trained for any specific task - instead, it develops broad capabilities that can later be specialized. This generality is both powerful and concerning from a safety perspective. While it enables the model to adapt to many different tasks, it also means we can't easily predict or constrain what the model might learn to do ("),t("a",{attrs:{href:"https://arxiv.org/abs/2109.13916",target:"_blank",rel:"noopener noreferrer"}},[e._v("Hendrycks et al., 2022"),t("OutboundLink")],1),e._v(").")]),e._v(" "),t("p",[t("strong",[e._v("How does self-supervised learning enable pre-training?")]),e._v(' Self-supervised learning (SSL) is the key technical innovation that makes foundation models possible. This is how we actually implement the pre-training phase. Unlike traditional supervised learning, which requires human-labeled data, SSL leverages the inherent structure of the data itself to create training signals. For example, instead of manually labeling images, we might just hide part of a full image we already have and ask a model to predict what the rest should be. So it might predict the bottom half of an image given the top half, learning about which objects often appear together. As an example, it might learn that images with trees and grass at the top often have more grass, or maybe a path, at the bottom. It learns about objects and their context ‚Äî trees and grass often appear in parks, dogs are often found in these environments, paths are usually horizontal, and so on. These learned representations can then be used for a wide variety of tasks that the model was not explicitly trained for, like identifying dogs in images, or recognizing parks - all without any human-provided labels! The same concept applies in language, a model might predict the next word in a sentence, such as "The cat sat on the ____," learning grammar, syntax, and context as long as we repeat this over huge amounts of text.')]),e._v(" "),t("p",[t("strong",[e._v("What is fine-tuning")]),e._v("**?** After pre-training, foundation models can be adapted through two main approaches: fine-tuning and prompting. Fine-tuning involves additional training on a specific task or dataset to specialize the model's capabilities. For example, we might use Reinforcement Learning from Human Feedback (RLHF) to make language models better at following instructions or being more helpful. Prompting, on the other hand, involves providing the model with carefully crafted inputs that guide it toward desired behaviors without additional training. We'll discuss these adaptation methods in more detail in Chapter 8 when we explore scalable oversight.")]),e._v(" "),t("p",[t("strong",[e._v("Why does this training process matter for AI safety?")]),e._v(" The training process of foundation models creates several unique safety challenges. First, the self-supervised nature of pre-training means we have limited control over what the model learns - it might develop unintended capabilities or behaviors. Second, the adaptation process needs to reliably preserve any safety properties we've established during pre-training. Finally, the massive scale of training data and compute makes it difficult to thoroughly understand or audit what the model has learned. Many of the safety challenges we'll discuss throughout this book - from goal misgeneralization to scalable oversight - are deeply connected to how these models are trained and adapted.")]),e._v(" "),t("h2",{attrs:{id:"properties"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#properties"}},[e._v("#")]),e._v(" Properties")]),e._v(" "),t("p",[t("strong",[e._v("Why do we need to understand the properties of foundation models?")]),e._v(" Besides just understanding the training process, we also need to understand the key defining characteristics or the abilities of these models. These properties often determine both the capabilities and potential risks of these systems. They help explain why foundation models pose unique safety challenges compared to traditional AI systems. Their ability to transfer knowledge, generalize across many different domains, and develop emergent capabilities means we can't rely on traditional safety approaches that assume narrow, predictable behavior.")]),e._v(" "),t("p",[t("strong",[e._v("What is transfer learning?")]),e._v(" Transfer learning is one of the most fundamental properties of foundation models - their ability to transfer knowledge learned during pre-training to new tasks and domains. Rather than starting from scratch for each task, we can leverage the general knowledge these models have already acquired ("),t("a",{attrs:{href:"https://arxiv.org/abs/2108.07258",target:"_blank",rel:"noopener noreferrer"}},[e._v("Bommasani et al., 2022"),t("OutboundLink")],1),e._v("). This property enables rapid adaptation and deployment, it also means that both capabilities and safety risks can transfer in unexpected ways. For instance, a model might transfer not just useful knowledge but also harmful biases or undesired behaviors to new applications.")]),e._v(" "),t("p",[t("strong",[e._v("What are zero-shot and few-shot learning?")]),e._v(" The ability to perform new tasks with very few examples, or even no examples at all. For example, GPT-4 can solve novel reasoning problems just from a natural language description of the task ("),t("a",{attrs:{href:"https://arxiv.org/abs/2303.08774",target:"_blank",rel:"noopener noreferrer"}},[e._v("OpenAI, 2023"),t("OutboundLink")],1),e._v("). This emergent ability to generalize to new situations is powerful but concerning from a safety perspective. If models can adapt to novel situations in unexpected ways, it becomes harder to predict and control their behavior in deployment.")]),e._v(" "),t("p",[t("strong",[e._v("Why is generality?")]),e._v(" Generalization in foundation models works differently from traditional AI systems. Rather than just generalizing within a narrow domain, these models can generalize capabilities across domains in surprising ways. However, this generalization of capabilities often happens without a corresponding generalization of goals or constraints - a critical safety concern we'll explore in detail in our chapter on goal misgeneralization. For example, a model might generalize its ability to manipulate text in unexpected ways without maintaining the safety constraints we intended ("),t("a",{attrs:{href:"https://arxiv.org/abs/2109.13916",target:"_blank",rel:"noopener noreferrer"}},[e._v("Hendrycks et al., 2022"),t("OutboundLink")],1),e._v(").")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/Wif_Image_14.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v('*: Bommasani Rishi et. al. (2022) "'),t("strong",[t("a",{attrs:{href:"https://arxiv.org/pdf/2108.07258.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("On the Opportunities and Risks of Foundation Models"),t("OutboundLink")],1)]),e._v('"*')]),e._v(" "),t("p",[t("strong",[e._v("Why is multi-modality?")]),e._v(" Models can work with multiple types of data (text, images, audio, video) simultaneously. This isn't just about handling different types of data. A better description is that they can make connections across modalities in sophisticated ways ("),t("a",{attrs:{href:"https://arxiv.org/abs/2312.11805",target:"_blank",rel:"noopener noreferrer"}},[e._v("Google, 2023"),t("OutboundLink")],1),e._v("). From a safety perspective, multi-modality introduces new challenges because it expands the ways models can interact with and influence the world. A safety failure in one modality might manifest through another in unexpected ways.")]),e._v(" "),t("p",[e._v('??? quote "Sam Altman (CEO of OpenAI) in a conversation with Bill Gates ('),t("a",{attrs:{href:"https://www.linkedin.com/pulse/altman-multimodality-important-david-cronshaw-5fz0c",target:"_blank",rel:"noopener noreferrer"}},[e._v("Cronshaw, 2024"),t("OutboundLink")],1),e._v(')"')]),e._v(" "),t("tab",[t("p",[t("em",[e._v("Multimodality will definitely be important. Speech in, speech out, images, eventually video. Clearly, people really want that. Customizability and personalization will also be very important.")])])]),e._v(" "),t("h2",{attrs:{id:"risks"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#risks"}},[e._v("#")]),e._v(" Risks")]),e._v(" "),t("p",[t("strong",[e._v("What makes foundation models hard to control?")]),e._v(" The difficulty of controlling these models stems from three interconnected challenges. First, once trained, their internal representations and behaviors are extremely difficult to modify in targeted ways. Unlike traditional software where we can directly edit specific functions, changing one behavior in a foundation model risks unpredictable effects on other capabilities. Second, when these models are deployed across many applications, maintaining control becomes a distributed systems problem. Safety failures can propagate through multiple systems before they're detected. Third, their black-box nature makes it extremely difficult to understand why they make specific decisions or how they might behave in novel situations - a challenge we'll explore deeply in our chapter on interpretability.")]),e._v(" "),t("p",[t("strong",[e._v("How do resource requirements limit development and access?")]),e._v(" Training foundation models requires massive computational resources, creating a delicate balance between cost and accessibility. While adapting an existing model might be relatively affordable, the substantial initial training costs risk centralizing power among a few well-resourced entities. This concentration of power raises important questions about oversight and responsible development, that we'll address in our chapter on governance. For example, a single training run of GPT-4 sized models can cost tens or hundreds of millions of dollars, effectively limiting who can participate in their development. Continued scaling has also brought up many concerns around the environmental impact of AI training runs. ("),t("a",{attrs:{href:"https://arxiv.org/abs/2104.10350",target:"_blank",rel:"noopener noreferrer"}},[e._v("Patterson et al., 2023"),t("OutboundLink")],1),e._v(").")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/yAa_Image_17.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: The rising costs of training frontier AI models ("),t("strong",[t("a",{attrs:{href:"https://arxiv.org/abs/2405.21015",target:"_blank",rel:"noopener noreferrer"}},[e._v("Cottier et al., 2024"),t("OutboundLink")],1)]),e._v(") *")]),e._v(" "),t("p",[t("strong",[e._v("What risks come from homogenization?")]),e._v(" Homogenization occurs when many AI systems are derived from the same foundation models. This creates a systemic risk - if a foundation model has biases or failure modes, these could propagate to all models fine-tuned from it ("),t("a",{attrs:{href:"https://arxiv.org/abs/2108.07258",target:"_blank",rel:"noopener noreferrer"}},[e._v("Bommasani et al., 2022"),t("OutboundLink")],1),e._v("). For example, if a widely-used foundation model has encoded harmful biases or unsafe behaviors, these might manifest across numerous applications, from content generation to automated decision-making. They also have fixed knowledge cutoffs based on their training data, creating potential safety issues when deployed in rapidly changing environments. This risk of correlated failures becomes particularly concerning when foundation models are deployed in critical systems. ("),t("a",{attrs:{href:"https://arxiv.org/abs/2109.13916",target:"_blank",rel:"noopener noreferrer"}},[e._v("Hendrycks et al., 2022"),t("OutboundLink")],1),e._v(").")]),e._v(" "),t("p",[t("strong",[e._v("What is emergence, and why does it matter?")]),e._v(" Foundation models can effectively leverage increases in data, computation, and model size to improve their capabilities. The models become qualitatively different as they scale, often developing new capabilities entirely absent in smaller versions ("),t("a",{attrs:{href:"https://arxiv.org/abs/2001.08361",target:"_blank",rel:"noopener noreferrer"}},[e._v("Kaplan et al., 2020"),t("OutboundLink")],1),e._v("). This property is called emergence - the development of capabilities that weren't explicitly trained for. This can be both a good thing and a bad thing.")]),e._v(" "),t("p",[e._v("We might encounter new, unexpected behaviors and risks that weren't present in smaller models. This emergence often happens discontinuously as models scale up, making it difficult to predict what capabilities might suddenly appear. Which means it also makes it difficult to predict which risks will emerge when. When combined with homogenization, this unpredictability becomes especially concerning - a single foundation model integrated into multiple critical systems could lead to correlated failures that span multiple safeguards or backup systems ("),t("a",{attrs:{href:"https://arxiv.org/abs/2109.13916",target:"_blank",rel:"noopener noreferrer"}},[e._v("Hendrycks et al., 2022"),t("OutboundLink")],1),e._v("). Therefore, this points to a need for a proactive approach to AI safety. Having safety measures in place before we begin scaling and training bigger models.")]),e._v(" "),t("p",[e._v("How do these limitations inform our approach to safety? We just briefly touched on some of the risks in this subsection to give you a basic intuition for why it is important to work on AI Safety. The entire next chapter is dedicated to a deep dive on different risks from AI models, and how they might come about.")]),e._v(" "),t("h1",{attrs:{id:"intelligence"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#intelligence"}},[e._v("#")]),e._v(" Intelligence")]),e._v(" "),t("h2",{attrs:{id:"case-studies"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#case-studies"}},[e._v("#")]),e._v(" Case Studies")]),e._v(" "),t("p",[e._v('!!! warning "The bulk of this subsection will deal with the theory and historical aspects of defining intelligence. If you are more interested in just the core practical aspects of how we measure artificial general intelligence (AGI), then you can safely skip to the next subsection - measurement."')]),e._v(" "),t("p",[t("strong",[e._v("Why do we need to define intelligence?")]),e._v(' In our previous section on foundation models, we explored how modern AI systems are becoming increasingly powerful. But before we can meaningfully discuss the risks and safety implications of these systems, we need to agree on what we mean when we talk about AGI. Some believe that "sparks" of AGI are already present in the latest language models ('),t("a",{attrs:{href:"https://arxiv.org/abs/2303.12712",target:"_blank",rel:"noopener noreferrer"}},[e._v("Bubeck et al., 2023"),t("OutboundLink")],1),e._v("), while others predict human-level AI within a decade ("),t("a",{attrs:{href:"https://arxiv.org/abs/2310.17688",target:"_blank",rel:"noopener noreferrer"}},[e._v("Bengio et al., 2024"),t("OutboundLink")],1),e._v("). Without a clear definition, how are we supposed to assess such claims or plan appropriate safety measures?")]),e._v(" "),t("p",[e._v("The core point is that if you can't define something, you can't measure it. If you can't measure it, you can't reliably track progress or identify potential risks. Think about an example from physics - saying something like \"it moved 5\" makes no sense without specifying the unit of measurement. Did it move 5 meters, 5 feet, or 5 royal cubits? Nobody knows. If we don't know how far or fast it moved, then can we enforce speed limits? Also, no. The same applies to intelligence, and subsequent risks and safety techniques. Just as physics needed standardized units like meters and watts to advance beyond qualitative descriptions, AI safety research needs rigorous definitions to move beyond vague analogies and anthropomorphisms.")]),e._v(" "),t("p",[e._v("**What makes defining intelligence so challenging? **If everyone agrees that we need a definition to measure progress and design safety measures, then why don‚Äôt we have a universally agreed upon definition? The problem is that the word intelligence is a term we use to describe multiple overlapping abilities - from problem-solving and learning to adaptation and abstract reasoning. Besides this, different academic disciplines view intelligence through different lenses. Psychologists emphasize measurable cognitive skills, computer scientists focus on task performance, and philosophers debate qualities like the relationship of intelligence to consciousness and self-awareness. So which approach is the most relevant to understanding and planning for AI Safety?")]),e._v(" "),t("p",[t("strong",[e._v("Case Study: Imitation based approach to intelligence.")]),e._v(" The Turing Test (or the imitation game) suggested that intelligence could be measured through a machine's ability to imitate human conversation ("),t("a",{attrs:{href:"https://academic.oup.com/mind/article/LIX/236/433/986238?login=false",target:"_blank",rel:"noopener noreferrer"}},[e._v("Turing, 1950"),t("OutboundLink")],1),e._v("). However, this behaviorist approach proved inadequate - modern language models can often pass Turing-style tests while lacking fundamental reasoning capabilities ("),t("a",{attrs:{href:"https://sciendo.com/issue/JAGI/11/2",target:"_blank",rel:"noopener noreferrer"}},[e._v("Rapaport, 2020"),t("OutboundLink")],1),e._v("). This is also still a process based approach, and was meant mainly as a philosophical thought experiment rather than a concrete operationalizable measure of intelligence.")]),e._v(" "),t("p",[t("strong",[e._v("Case Study: Consciousness based approaches to intelligence.")]),e._v(" One early view focused on machines that could truly understand and have cognitive states similar to humans ("),t("a",{attrs:{href:"https://psycnet.apa.org/record/1981-27235-001",target:"_blank",rel:"noopener noreferrer"}},[e._v("Searle, 1980"),t("OutboundLink")],1),e._v("). However, this definition proves problematic on multiple levels. First, consciousness remains poorly understood and difficult to measure. Second, we are unsure if intelligence and consciousness are necessarily linked - a system could potentially be highly intelligent without being conscious, or conscious without being particularly intelligent. A system doesn't need to be conscious to cause harm. Whether an AI system is conscious or not has no bearing on its ability to make high-impact decisions or take potentially dangerous actions.")]),e._v(" "),t("p",[t("strong",[e._v("Case Study: Brain analogy based approaches to intelligence.")]),e._v(" Another early approach defined AGI in terms of systems that rival or surpass the human brain in complexity and speed. This brain-centric definition is problematic for several reasons. While our brains may be the only example of general intelligence we have, modern AI has shown that matching human neural architecture isn't necessary for achieving intelligent behavior. From a safety perspective, focusing on brain-like architecture tells us little about what risks a system might pose - a system could be very unlike a brain in structure but still be capable of dangerous actions.")]),e._v(" "),t("p",[t("strong",[e._v("Case study: Process/Adaptability based approaches to intelligence.")]),e._v(' The process-based view sees intelligence as the efficiency of learning and adaptation, rather than accumulated capabilities. A few researchers adopt this view of intelligence. Under this view, intelligence is "'),t("em",[e._v("the capacity of a system to adapt to its environment while operating with insufficient knowledge and resources")]),e._v('" ('),t("a",{attrs:{href:"https://sciendo.com/issue/JAGI/11/2",target:"_blank",rel:"noopener noreferrer"}},[e._v("Wang, 2020"),t("OutboundLink")],1),e._v('). Alternatively, it is described as "'),t("em",[e._v("the efficiency with which a system can turn experience and priors into skills")]),e._v('" ('),t("a",{attrs:{href:"https://arxiv.org/abs/1911.01547",target:"_blank",rel:"noopener noreferrer"}},[e._v("Chollet, 2019"),t("OutboundLink")],1),e._v("). While this focus on meta-learning and adaptation captures something fundamental about intelligence, but from a safety perspective, what ultimately matters is what these systems can actually do - their concrete capabilities - rather than how they achieve these capabilities. This leads us to the final approach.")]),e._v(" "),t("p",[t("strong",[e._v("Case study 5: The capabilities approach to intelligence")]),e._v('. The motivating question behind this view is - If an AI system can perform dangerous tasks at human-level or beyond, does it really matter whether it achieved this through sophisticated learning processes, efficient memorization, with/without consciousness? If an AI system has capabilities that could pose risks - like sophisticated planning, manipulation, or deception - these risks exist regardless of whether the system acquired these capabilities through "true intelligence", "real understanding" or sophisticated pattern matching. The capabilities-based approach cuts through philosophical debates by asking concrete questions: What can the system actually do? How well can it do it? What range of tasks can it handle? This framework provides clear standards for progress and, crucially for safety work, clear ways to identify potential risks. The majority of AI labs use this capabilities-focused approach in how they frame their AGI goals. For example, AGI has been defined as "'),t("em",[e._v("highly autonomous systems that outperform humans at most economically valuable work")]),e._v('" ('),t("a",{attrs:{href:"https://openai.com/charter/",target:"_blank",rel:"noopener noreferrer"}},[e._v("OpenAI, 2014"),t("OutboundLink")],1),e._v('). Safety considerations are framed similarly in saying that the mission is to ensure "'),t("em",[e._v("transformative AI helps people and society")]),e._v('" ('),t("a",{attrs:{href:"https://www.anthropic.com/company",target:"_blank",rel:"noopener noreferrer"}},[e._v("Anthropic, 2024"),t("OutboundLink")],1),e._v(").")]),e._v(" "),t("p",[e._v('!!! quote "Capabilities vs Intelligence ('),t("a",{attrs:{href:"https://www.alignmentforum.org/posts/JtuTQgp9Wnd6R6F5s/when-discussing-ai-risks-talk-about-capabilities-not",target:"_blank",rel:"noopener noreferrer"}},[e._v("Krakovna, 2023"),t("OutboundLink")],1),e._v(')"')]),e._v(" "),t("tab",[t("p",[t("em",[e._v("When discussing AI risks, talk about capabilities, not intelligence... People often have different definitions of intelligence, or associate it with concepts like consciousness that are not relevant to AI risks, or dismiss the risks because intelligence is not well-defined.")])])]),e._v(" "),t("p",[e._v("Given these considerations, for the vast majority of this book, our primary focus will remain on the practical framework of capabilities for evaluation and safety assessment. This capabilities-focused approach is most relevant for immediate safety work, regulation, and deployment decisions. We acknowledge that research into consciousness, sentience, ethics surrounding digital minds and the fundamental nature of intelligence continues to be valuable but is less actionable for immediate safety work.")]),e._v(" "),t("p",[e._v('In our next subsection, we will explore how we can concretely define and measure capabilities within this framework. We\'ll see how moving beyond simple binary thresholds of "narrow" versus "general" AI helps us better understand the progression of AI capabilities and their associated risks.')]),e._v(" "),t("h2",{attrs:{id:"measuring"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#measuring"}},[e._v("#")]),e._v(" Measuring")]),e._v(" "),t("p",[t("strong",[e._v("Why do traditional definitions of AGI fall short?")]),e._v(" In the previous section, we explored how foundation models are becoming increasingly powerful and versatile. But before we can meaningfully discuss risks and safety implications, or make predictions about future progress, we need clear ways to measure and track AI capabilities. This section introduces frameworks for measuring progress toward artificial general intelligence (AGI) and understanding the relationship between capabilities, autonomy, and risk. For example, OpenAI's definition of AGI as \""),t("em",[e._v("systems that outperform humans at most economically valuable work")]),e._v('" ('),t("a",{attrs:{href:"https://openai.com/charter/",target:"_blank",rel:"noopener noreferrer"}},[e._v("OpenAI, 2014"),t("OutboundLink")],1),e._v('), or the commonly used definition "'),t("em",[e._v("Intelligence measures an agent‚Äôs ability to achieve goals in a wide range of environments.")]),e._v('" ('),t("a",{attrs:{href:"https://arxiv.org/abs/0706.3639",target:"_blank",rel:"noopener noreferrer"}},[e._v("Legg and Hutter, 2007"),t("OutboundLink")],1),e._v(") and many others are not specific enough to be operationalizable. Which humans? Which goals? Which tasks are economically valuable? What about systems that exceed human performance on some tasks but only for short durations?")]),e._v(" "),t("p",[t("strong",[e._v("Why do we need better measurement frameworks?")]),e._v(' Historically, discussions about AGI have often relied on binary thresholds - systems were categorized as either "narrow" or "general", "weak" or "strong", "sub-human" or "human-level." While these distinctions helped frame early discussions about AI, they become increasingly inadequate as AI systems grow more sophisticated. Just like we sidestepped debates around whether AIs display "true intelligence" or "real understanding" in favor of a more practical framework that focuses on capabilities, similarly we want to avoid debates around things like whether a system is "human-level" or not. It is much more pragmatic to be able to make statements like - it outperforms 75% of skilled adults on 30% of cognitive tasks.')]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/vlV_Image_18.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: This is the continuous outlook of AI measuring performance. All points on this axis can be called ANI (except for the origin).*")]),e._v(" "),t("p",[e._v('!!! note "Definition: Artificial Narrow Intelligence (ANI) ('),t("a",{attrs:{href:"https://www.ibm.com/topics/artificial-intelligence",target:"_blank",rel:"noopener noreferrer"}},[e._v("IBM, 2023"),t("OutboundLink")],1),e._v(')"')]),e._v(" "),t("tab",[t("p",[t("em",[e._v("Weak AI‚Äîalso called Narrow AI or Artificial Narrow Intelligence (ANI)‚Äîis AI trained and focused to perform specific tasks. Weak AI drives most of the AI that surrounds us today. ‚ÄòNarrow‚Äô might be a more accurate descriptor for this type of AI as it is anything but weak; it enables some very robust applications, such as Apple's Siri, Amazon's Alexa, IBM Watson, and autonomous vehicles.")])])]),e._v(" "),t("p",[t("strong",[e._v("Levels of artificial narrow intelligence (ANI)")]),e._v(". We should think about the performance of AI systems on a continuous spectrum. Traditional definitions of ANI correspond to high performance on a very small percentage of tasks. For example, chess engines like AlphaZero outperform 100% of humans, but only on roughly 0.01% of cognitive tasks. Similarly, specialized image recognition systems might outperform 95% of humans, but again only on a tiny fraction of possible tasks. According to the definition above, all these systems would be defined as ANI, but if we think about them in a continuous range of what percentage of skilled humans they can outperform we get a much more specific and granular picture.")]),e._v(" "),t("p",[t("strong",[e._v("How can we build a better measurement framework for AGI?")]),e._v(" We need to track AI progress along both - performance (how well can it do things?) and generality (how many different things can it do?). Just like we can describe a point on a map using latitude and longitude, we can characterize AGI systems by their combined level of performance and degree of generality, as measured by benchmarks and evaluations. This framework gives us a much more granular way to track progress. This precision helps us better understand both current capabilities and likely development trajectories.")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/075_Image_19.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: Table of performance x generality showing both levels of ANI, and levels of AGI.*")]),e._v(" "),t("p",[t("strong",[e._v("Where do current AI systems fit in this framework?")]),e._v(' Large language models like GPT-4 show an interesting pattern - they outperform roughly 50% of skilled adults on perhaps 15-20% of cognitive tasks (like basic writing and coding), while matching or slightly exceeding unskilled human performance on a broader range of tasks. This gives us a more precise way to track progress than simply debating whether such systems qualify as "AGI." LLMs like  GPT-4 are early forms of AGI ('),t("a",{attrs:{href:"https://arxiv.org/abs/2303.12712",target:"_blank",rel:"noopener noreferrer"}},[e._v("Bubeck, 2023"),t("OutboundLink")],1),e._v("), and over time we will achieve stronger AGI as both generality and performance increase. To understand how this continuous framework relates to traditional definitions, let's examine how key historical concepts map onto our performance-generality space.")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/jzt_Image_20.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: The two-dimensional view of performance x generality. The different curves are meant to represent the different paths we can take to ASI. Every single point on the path corresponds to a different level of AGI. The specific development trajectory is hard to forecast. This will be discussed in the section on forecasting and takeoff.*")]),e._v(" "),t("p",[e._v('!!! note "Definition: Transformative AI (TAI) ('),t("a",{attrs:{href:"https://www.openphilanthropy.org/research/some-background-on-our-views-regarding-advanced-artificial-intelligence/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Karnofsky, 2016"),t("OutboundLink")],1),e._v(')"')]),e._v(" "),t("tab",[t("p",[t("em",[e._v("Potential future AI that triggers a transition equivalent to, or more significant than, the agricultural or industrial revolution.")])])]),e._v(" "),t("p",[t("strong",[e._v("Transformative AI (TAI)")]),e._v(". Transformative AI represents a particularly interesting point in our framework because it isn't tied to specific performance or generality thresholds. Instead, it focuses on a range of impacts. For example, a system could be transformative by achieving moderate performance (outperforming 60% of humans) across a wide range of economically important tasks (50% of cognitive tasks), or by achieving exceptional performance (outperforming 99% of humans) on a smaller but critical set of tasks (20% of cognitive tasks).")]),e._v(" "),t("p",[e._v('!!! note "Definition: Human Level AI (TAI) ('),t("a",{attrs:{href:"https://www.openphilanthropy.org/research/some-background-on-our-views-regarding-advanced-artificial-intelligence/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Karnofsky, 2016"),t("OutboundLink")],1),e._v(')"')]),e._v(" "),t("tab",[t("p",[t("em",[e._v("Potential future AI that triggers a transition equivalent to, or more significant than, the agricultural or industrial revolution.")])])]),e._v(" "),t("p",[t("strong",[e._v("Human Level AI (HLAI)")]),e._v(". This term is sometimes used interchangeably with AGI, and refers to an AI system that equals human intelligence in essentially all economically valuable work. However, we only explain it here for reasons of completeness. Human-level is not well-defined which makes this definitions difficult to operationalize. If we map this onto the levels of AGI framework, then it roughly would correspond to outperforming 99% of skilled adults at most cognitve non physical tasks.")]),e._v(" "),t("p",[e._v('!!! quote "Definition: Artificial Superintelligence (ASI) ('),t("a",{attrs:{href:"https://psycnet.apa.org/record/2014-48585-000",target:"_blank",rel:"noopener noreferrer"}},[e._v("Bostrom, 2014"),t("OutboundLink")],1),e._v(')"')]),e._v(" "),t("tab",[t("p",[t("em",[e._v("Any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest.")])])]),e._v(" "),t("p",[t("strong",[e._v("Artificial Superintelligence (ASI)")]),e._v(". If systems achieve superhuman performance at all cognitive tasks, then this would be the strongest form of AGI, also called superintelligence.  In our framework, ASI represents the upper-right corner - systems that outperform 100% of humans on nearly 100% of cognitive tasks.")]),e._v(" "),t("p",[t("strong",[e._v("What is the relationship between levels of AGI and risk?")]),e._v(' Understanding AI systems through continuous performance and generality measures helps us better assess risk. Rather than waiting for systems to cross some "AGI threshold," we can identify specific combinations of performance and generality that warrant increased safety measures. For example:')]),e._v(" "),t("ul",[t("li",[t("p",[e._v("A system achieving 90% performance on 30% of tasks might require different safety protocols than one achieving 60% performance on 70% of tasks")])]),e._v(" "),t("li",[t("p",[e._v('Certain capability combinations might enable dangerous emergent behaviors even before reaching "human-level" on most tasks')])]),e._v(" "),t("li",[t("p",[e._v("The rate of improvement along either axis provides important signals about how quickly additional safety measures need to be developed")])])]),e._v(" "),t("p",[e._v("There are various other variables that we can add to make this picture even more precise. For example, just like we have levels of performance and generality, we can also have levels of autonomy with which these systems operate. As an example, at a low level of autonomy a human fully controls a task and uses AI to automate mundane sub-tasks, whereas at a higher level of autonomy we might see the AI take on a substantive role,or even co-equal work division. ("),t("a",{attrs:{href:"https://arxiv.org/abs/2311.02462v4",target:"_blank",rel:"noopener noreferrer"}},[e._v("Morris et al., 2024"),t("OutboundLink")],1),e._v(") Similarly, we have the variable of propensities which would measure what the AI model tends to do by default ("),t("a",{attrs:{href:"https://arxiv.org/abs/2305.15324",target:"_blank",rel:"noopener noreferrer"}},[e._v("Shevlane et al., 2023"),t("OutboundLink")],1),e._v("), and the variable of controllability which measures what percent of the time the AI model is able to subvert our current safety measures ("),t("a",{attrs:{href:"https://arxiv.org/abs/2312.06942",target:"_blank",rel:"noopener noreferrer"}},[e._v("Roger et al., 2023"),t("OutboundLink")],1),e._v("). Combining our definition of levels of AGI with variables like this gives us an extremely accurate picture of what the model is able to, and allows actionable technical safety and regulatory proposals.")]),e._v(" "),t("h1",{attrs:{id:"scaling"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#scaling"}},[e._v("#")]),e._v(" Scaling")]),e._v(" "),t("p",[e._v("In the previous section, we explored how we can measure AI capabilities along continuous dimensions of performance and generality. Now we'll examine one of the most important drivers behind improvements in these capabilities: scale.")]),e._v(" "),t("h2",{attrs:{id:"the-bitter-lesson"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#the-bitter-lesson"}},[e._v("#")]),e._v(" The Bitter Lesson")]),e._v(" "),t("p",[e._v("We assume that most of you probably went to university in an era where machine learning and AI roughly mean the same thing, or rather deep learning and AI mean the same thing. This hasn't always been true. Early in the history of artificial intelligence, researchers took very different approaches to creating intelligent systems. They believed that the key to artificial intelligence was carefully encoding human knowledge and expertise into computer programs. This led to things like expert systems filled with hand-crafted rules and chess engines programmed with sophisticated strategic principles. However, time and time again, researchers learned what we now call the bitter lesson.")]),e._v(" "),t("p",[e._v('!!! quote "The Bitter Lesson ('),t("a",{attrs:{href:"http://www.incompleteideas.net/IncIdeas/BitterLesson.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("Sutton, 2019"),t("OutboundLink")],1),e._v(')"')]),e._v(" "),t("tab",[t("p",[t("em",[e._v("The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin. [...] The bitter lesson is based on the historical observations that 1) AI researchers have often tried to build knowledge into their agents, 2) this always helps in the short term, and is personally satisfying to the researcher, but 3) in the long run it plateaus and even inhibits further progress, and 4) breakthrough progress eventually arrives by an opposing approach based on scaling computation by search and learning.")])])]),e._v(" "),t("p",[e._v('What makes this lesson bitter? The bitterness comes from discovering that decades of careful human engineering and insight were ultimately less important than simple algorithms plus computation. In chess, researchers who had spent years encoding grandmaster knowledge watched as "brute force" search-based approaches like Deep Blue defeated world champion Garry Kasparov. In computer vision, hand-crafted feature detectors were outperformed by convolutional neural networks that learned their own features from data. In speech recognition, systems based on human understanding of phonetics were surpassed by statistical approaches using hidden Markov models ('),t("a",{attrs:{href:"http://www.incompleteideas.net/IncIdeas/BitterLesson.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("Sutton, 2019"),t("OutboundLink")],1),e._v(").")]),e._v(" "),t("p",[t("strong",[e._v("Does the bitter lesson mean we don't need any human engineering?")]),e._v(" Human ingenuity playing a smaller role in improving AI is a subtle point that can be easily misunderstood. The transformer architecture for example might seem to contradict the bitter lesson because they rely on sophisticated architectural innovations. Human ingenuity is important, but the subtlety is in recognizing that there's a difference between two types of human engineering:")]),e._v(" "),t("ul",[t("li",[t("p",[t("strong",[e._v("Algorithm-level improvements:")]),e._v(" These make better use of existing compute, like: better optimizers (Adam), architecture innovations (transformers, attention mechanisms) or training approaches (better learning rate schedules).")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Domain-specific engineering improvements:")]),e._v(" These try to encode human knowledge, like: special architectures designed for specific problems, hand-crafted features or rules or task-specific inductive biases.")])])]),e._v(" "),t("p",[e._v("The bitter lesson isn't arguing against all human engineering - it's specifically cautioning against the second type. The transformer architecture exemplifies this pattern - it doesn't encode any specific knowledge about language, but rather provides a general mechanism for learning patterns that becomes increasingly powerful as we scale up compute and data.")]),e._v(" "),t("p",[t("strong",[e._v("What does the bitter lesson mean for AI Safety?")]),e._v(" In the previous section, we discussed measuring AI capabilities along continuous dimensions of performance and generality. If it is the case that these continue to advance primarily through scaling, then we may have more predictable trajectories. This creates both opportunities and risks because performance and generality will continue to rise as long as scale is all that we need, but it also means that forecasting AI development trajectories and preparing appropriate safety measures for predictable capability levels is possible.")]),e._v(" "),t("h1",{attrs:{id:"scaling-laws"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#scaling-laws"}},[e._v("#")]),e._v(" Scaling Laws")]),e._v(" "),t("p",[t("strong",[e._v("Why do AI labs care about scaling laws?")]),e._v(" Training large AI models is extremely expensive - potentially hundreds of millions of dollars for frontier models. Scaling laws help labs make crucial decisions about resource allocation: Should they spend more on GPUs or on acquiring training data? Should they train a larger model for less time or a smaller model for longer? For example, with a fixed compute budget, they might need to choose between training a 20-billion parameter model on 40% of their data or a 200-billion parameter model on just 4%. Getting these tradeoffs wrong can waste enormous resources. So it is important to be able to have a predictable relationship between how you invest your money and what level of capabilities you get at the end.")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/9Is_Image_21.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: Example of capabilities increasing with an increasing with one of variables in the scaling laws - parameter count. The same model architecture (Parti) was used to generate an image using an identical prompt, with the only difference between the models being the parameter size. There are noticeable leaps in quality, and somewhere between 3 billion and 20 billion parameters, the model acquires the ability to spell words correctly. ("),t("strong",[t("a",{attrs:{href:"https://arxiv.org/abs/2206.10789",target:"_blank",rel:"noopener noreferrer"}},[e._v("Yu et al., 2022"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[t("strong",[e._v("What are scaling laws?")]),e._v(" Scaling laws are mathematical relationships that describe how an AI system's performance changes as we vary key inputs like model size, dataset size, and computing power. These are empirical power-law relationships that have been observed to hold across many orders of magnitude. The key variables involved are:")]),e._v(" "),t("ul",[t("li",[t("p",[t("strong",[e._v("Compute (C)")]),e._v(': This represents the total processing power used during training, measured in floating-point operations (FLOPs). Think of this as the training "budget" - more compute means either training for longer, using more powerful hardware, or both. While having more GPUs helps increase compute capacity, compute ultimately refers to the total number of operations performed, not just hardware.')])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Parameters (N)")]),e._v(": These are the tunable numbers in the model that get adjusted during training - like knobs that the model can adjust to better fit the data. More parameters allow the model to learn more complex patterns but require more compute per training step. Current frontier models have hundreds of billions of parameters.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Dataset size (D)")]),e._v(": This measures how many examples the model trains on (typically measured in tokens for language models). The larger the dataset, the more information the model can read. Simultaneously, to read and learn from more data, the training runs also need to be generally longer, which in turn increases the total compute needed before the model can be considered ‚Äútrained‚Äù. While researchers initially focused on scaling parameters, we've learned that the relationship between compute, parameters, and data is crucial - many early models were actually trained with suboptimal ratios between these variables.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Loss (L)")]),e._v(": This measures how well the model performs on its training objective. This is what we are trying to minimize, and it tends to improve as we scale up these variables.")])])]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/nQO_Image_18.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: OpenAIs Scaling Laws showing impact of each variable on performance (loss) ("),t("strong",[t("a",{attrs:{href:"https://arxiv.org/abs/2001.08361",target:"_blank",rel:"noopener noreferrer"}},[e._v("Kaplan et al., 2020"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[t("strong",[e._v("OpenAI's initial scaling laws in 2020")]),e._v(". To determine the relationships between different variables that might contribute to scale, OpenAI conducted a series of experiments. For an intuitive idea of how they came up with the scaling laws, you can imagine that while training a model you can hold some variables fixed while varying others and see how loss changes. Eventually this allows you to see some patterns. As an example, dataset size can be kept constant, while parameter count and training time are varied, or parameter count is kept constant and data amounts are varied, etc‚Ä¶ So we can get a measurement of the relative contribution of each towards overall performance. If these relationships hold true across many different model architectures and tasks, then this suggests they capture something fundamental about deep learning systems. This is how the first generation of scaling laws came about from OpenAI. For example, by these laws if you have a 10x more compute, you should increase model size by about 5x and data size by only 2x.  ("),t("a",{attrs:{href:"https://arxiv.org/abs/2001.08361",target:"_blank",rel:"noopener noreferrer"}},[e._v("Kaplan et al., 2020"),t("OutboundLink")],1),e._v(")")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/o2j_Image_19.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: ("),t("strong",[t("a",{attrs:{href:"https://arxiv.org/abs/2001.08361",target:"_blank",rel:"noopener noreferrer"}},[e._v("Kaplan et al., 2020"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[t("strong",[e._v("DeepMind's scaling law update in 2022")]),e._v(". DeepMind found that most large language models were actually significantly overparameterized for the amount of data they were trained on. The Chinchilla scaling laws showed that for optimal performance, models should be trained on approximately 20 times more data tokens than they have parameters. This meant that many leading models could have achieved better performance with smaller sizes, but with more data. They were called chinchilla scaling laws because the laws were demonstrated using a model called Chinchilla. This was a 70B parameter model trained on more data, which outperformed much larger models like Gopher (280B parameters) despite using the same amount of compute. So by these laws, for optimal performance, you should increase model size and dataset size in roughly equal proportions - if you get 10x more compute, you should make your model ~3.1x bigger and your data ~3.1x bigger. ("),t("a",{attrs:{href:"https://arxiv.org/abs/2203.15556",target:"_blank",rel:"noopener noreferrer"}},[e._v("Hoffmann et al., 2022"),t("OutboundLink")],1),e._v(")")]),e._v(" "),t("p",[t("strong",[e._v("The Broken Neural Scaling Laws (BNSL) update in 2023")]),e._v('. Ongoing research showed that performance doesn\'t always improve smoothly - there can be sharp transitions, temporary plateaus, or even periods where performance gets worse before getting better. Examples of this include things like "Grokking", where models suddenly achieve strong generalization after many training steps, or deep double descent, where increasing model size initially hurts then helps performance. Rather than simple power laws, BNSL uses a more flexible functional form that can capture these complex behaviors. This allows for more accurate predictions of scaling behavior, particularly around discontinuities and transitions. Scaling laws are a good baseline, but according discontinuous jumps in capabilities and abrupt step changes are still possible. ('),t("a",{attrs:{href:"https://arxiv.org/abs/2210.14891",target:"_blank",rel:"noopener noreferrer"}},[e._v("Caballero et al., 2023"),t("OutboundLink")],1),e._v(")")]),e._v(" "),t("p",[t("strong",[e._v("How do training and inference scaling differ?")]),e._v(" Training scaling involves using more compute during initial model training by using larger models, training for longer, or using bigger datasets. Another way that we might not be accounting for using scaling laws, is called inference time scaling. This instead uses more compute at runtime through techniques like chain-of-thought prompting, repeated sampling, or tree search. For example, you can either train a very large model that generates high-quality outputs directly, or train a smaller model that achieves similar performance by using more computation to think through problems step by step at inference time.")]),e._v(" "),t("h1",{attrs:{id:"scaling-hypothesis"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#scaling-hypothesis"}},[e._v("#")]),e._v(" Scaling Hypothesis")]),e._v(" "),t("p",[t("strong",[e._v("What is the strong scaling hypothesis?")]),e._v(" The strong scaling hypothesis proposes that simply scaling up current foundation model architectures with more compute and data will be sufficient to reach transformative AI capabilities and potentially even ASI. This view suggests we already have all the fundamental components needed - it's just a matter of making them bigger, following established scaling laws. ("),t("a",{attrs:{href:"https://gwern.net/scaling-hypothesis",target:"_blank",rel:"noopener noreferrer"}},[e._v("Branwen, 2020"),t("OutboundLink")],1),e._v(") There is heated debate around this hypothesis and we can't possibly cover every argument. We can give you a slight overview in the next few paragraphs.")]),e._v(" "),t("p",[e._v("Proponents include OpenAI ("),t("a",{attrs:{href:"https://openai.com/blog/planning-for-agi-and-beyond",target:"_blank",rel:"noopener noreferrer"}},[e._v("OpenAI, 2023"),t("OutboundLink")],1),e._v("), Anthropic‚Äôs CEO Dario Amodei ("),t("a",{attrs:{href:"https://www.dwarkeshpatel.com/p/dario-amodei",target:"_blank",rel:"noopener noreferrer"}},[e._v("Amodei, 2023"),t("OutboundLink")],1),e._v("), Conjecture ("),t("a",{attrs:{href:"https://www.lesswrong.com/posts/PE22QJSww8mpwh7bt/agi-in-sight-our-look-at-the-game-board",target:"_blank",rel:"noopener noreferrer"}},[e._v("Conjecture, 2023"),t("OutboundLink")],1),e._v("), DeepMind‚Äôs safety team ("),t("a",{attrs:{href:"https://www.lesswrong.com/posts/GctJD5oCDRxCspEaZ/clarifying-ai-x-risk",target:"_blank",rel:"noopener noreferrer"}},[e._v("DeepMind, 2022"),t("OutboundLink")],1),e._v('), and others. According to the DeepMind team, there are "'),t("em",[e._v("not many more fundamental innovations needed for AGI. Scaled-up deep learning foundation models with RL from human feedback (RLHF) fine-tuning [should suffice]")]),e._v('" ('),t("a",{attrs:{href:"https://www.lesswrong.com/posts/GctJD5oCDRxCspEaZ/clarifying-ai-x-risk",target:"_blank",rel:"noopener noreferrer"}},[e._v("DeepMind, 2022"),t("OutboundLink")],1),e._v(").")]),e._v(" "),t("p",[t("strong",[e._v("What are the key arguments supporting the strong scaling hypothesis?")]),e._v(" The most compelling evidence for this view comes from empirical observations of progress in recent years. Researchers have been developing algorithms that follow the bitter lesson's principle for many years (focusing on general methods that leverage compute effectively). But even when researchers have developed sophisticated algorithms following the bitter lesson's principles, these improvements still only account for 35% of performance gains in language models in 2024, with the remaining 65% coming purely from increased scale in compute and data ("),t("a",{attrs:{href:"https://arxiv.org/abs/2403.05812",target:"_blank",rel:"noopener noreferrer"}},[e._v("Ho et al., 2024"),t("OutboundLink")],1),e._v("). Basically, even when our algorithmic improvements align perfectly with the bitter lesson, they're still far less important than raw scaling.")]),e._v(" "),t("p",[e._v("The emergence of unexpected capabilities provides another powerful argument for strong scaling. We've seen previous generations of foundation models demonstrate remarkable abilities that weren't explicitly trained for, like programming for example. This emergent behavior hints that it is not impossible for higher-order metacognitive abilities like causal reasoning to similarly emerge simply as a function of scale. We see that bigger models become increasingly sample efficient - they require fewer examples to learn new tasks. This improved efficiency with scale suggests that scaling up further could eventually lead to human-like few-shot learning capabilities, which is a precursor for TAI and ASI. Finally,  these models also appear to be capable of learning any task that can be expressed through their training modalities. Right now this is text for LLMs but there is a clear path forward to multimodal LMMs. Since text can express virtually any human-comprehensible task, scaling up language understanding might be sufficient for general intelligence.")]),e._v(" "),t("p",[e._v("**What are the key arguments against the strong scaling hypothesis? **Recent research has also identified several challenges to the strong scaling hypothesis. The most immediate is data availability - language models will likely exhaust high-quality public text data between 2026 and 2032 ("),t("a",{attrs:{href:"https://arxiv.org/abs/2211.04325",target:"_blank",rel:"noopener noreferrer"}},[e._v("Villalobos et al., 2024"),t("OutboundLink")],1),e._v("). While synthetic data might help address this limitation, it's unclear whether it can provide the same quality of learning signal as organic human-generated content. Alternatively, we still have a lot of multi-modal data left to train on (like YouTube videos) despite running out of text data.")]),e._v(" "),t("p",[e._v('A more fundamental challenge comes from the way these models work. LLMs are fundamentally "interpolative databases" (or stochastic parrots , or a variety of other similar terms). The point being that they just build up a vast collection of vector transformations through pre-training. While these transformations become increasingly sophisticated with scale, critics argue there\'s a fundamental difference between recombining existing ideas and true synthesis - deriving novel solutions from first principles. However, this is not an airtight case against strong scaling. This could simply be a limitation of current scale - a larger model trained on multimodal data might learn to handle any new novel situation simply as a recombination of previously memorized patterns. So, it is unclear if template recombination actually does have an upper bound.')]),e._v(" "),t("p",[t("strong",[e._v("What is the weak scaling hypothesis?")]),e._v(" Given these challenges, a weaker version of the scaling hypothesis has also been proposed. According to the weak scaling hypothesis even though scale will continue to be the primary driver of progress, we will also need targeted architectural and algorithmic improvements to overcome specific bottlenecks. These improvements wouldn't require fundamental breakthroughs, but rather incremental enhancements to better leverage scale. ("),t("a",{attrs:{href:"https://gwern.net/scaling-hypothesis",target:"_blank",rel:"noopener noreferrer"}},[e._v("Branwen, 2020"),t("OutboundLink")],1),e._v(") Similar to the strong scaling hypothesis, the weak one is also contentious and debated. We can provide a few of the results arguing both for and against this outlook.")]),e._v(" "),t("p",[e._v("LeCun's H-Jepa architecture ("),t("a",{attrs:{href:"https://openreview.net/pdf?id=BZ5a1r-kVsf",target:"_blank",rel:"noopener noreferrer"}},[e._v("LeCun, 2022"),t("OutboundLink")],1),e._v("), or Richard Sutton‚Äôs Alberta Plan ("),t("a",{attrs:{href:"https://arxiv.org/abs/2208.11173",target:"_blank",rel:"noopener noreferrer"}},[e._v("Sutton, 2022"),t("OutboundLink")],1),e._v(") are notable plans that might support the weak scaling hypothesis.")]),e._v(" "),t("p",[e._v('**What are the key arguments supporting the weak scaling hypothesis? **The arguments for strong scaling, like algorithmic improvements only contributing 35% of performance gains in language models can also double count for weak scaling. Since one third is still a non-trivial role to play in capabilities improvement. Some more empirical observations also support weak scaling. Like hardware support for lower-precision calculations, which provided order-of-magnitude performance improvements for machine learning workloads (Hobbhahn et al., 2023, "Trends in Machine Learning Hardware"). These kinds of targeted improvements don\'t change the fundamental scaling story but rather help us better leverage available resources. Similarly, the Chinchilla results showed that many models were poorly optimized along the different variables that contribute to capabilities. This suggests that there is still room for improvement through better scaling strategies rather than fundamental breakthroughs. ('),t("a",{attrs:{href:"https://arxiv.org/abs/2203.15556",target:"_blank",rel:"noopener noreferrer"}},[e._v("Hoffmann et al., 2022"),t("OutboundLink")],1),e._v(")")]),e._v(" "),t("p",[t("strong",[e._v("What if neither the weak nor the strong scaling hypothesis is true?")]),e._v(' Essentially, both the scaling laws (which only predict foundation model capabilities) and most debates around "scale is all you need" often miss crucial aspects of AI development that happen outside the scope of what scaling laws can predict. They don\'t account for improvements in AI "scaffolding" (like chain-of-thought prompting, tool use, or retrieval), or combinations of multiple models working together in novel ways. Debates around the scaling laws only tell us about the capabilities of a single foundation model trained in a standard way. For example, by the strong scaling hypothesis we can reach TAI by simply scaling up the same foundation model until meta cognitive abilities emerge. But even if scaling stops, halting capabilities progress on the core foundation model (in either a weak or a strong way), the external techniques that leverage the existing model can still continue advancing.')]),e._v(" "),t("p",[e._v('Think of foundation models like LLMs or LMMs as simply one transistor. Alone they might not be able to do much, but if we combine enough transistors we end up with all the capabilities of a supercomputer. Many researcher think that this is a crucial element where future capabilities will come from. It is also referred to as "unhobbling" ('),t("a",{attrs:{href:"https://situational-awareness.ai/from-gpt-4-to-agi/#Unhobbling",target:"_blank",rel:"noopener noreferrer"}},[e._v("Aschenbrenner, 2024"),t("OutboundLink")],1),e._v('), "schlep" ('),t("a",{attrs:{href:"https://www.planned-obsolescence.org/scale-schlep-and-systems/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Cotra, 2023"),t("OutboundLink")],1),e._v(") and various other terms, but it all of them in principle points to the same thing - raw scaling of single model performance is only one part of overall AI capability advancement. This is illustrated using the two images below.")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/au8_Image_24.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: Augmentation/Scaffolding stays constant, but if the scaling hypothesis, weak or strong, is true, then capabilities keep improving.*")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/x2u_Image_25.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: Scaling stays constant, but other elicitation techniques and scaffolding keep improving, then capabilities keep improving. Realistically, the future is probably going to see both improvement due to scaffolding and scale. So for now, there does not seem to be an upper limit on improving capabilities as long as either one of the two hold.*")]),e._v(" "),t("p",[e._v("We go deeper into the arguments and counterarguments for all views on scaling foundation models in the appendix.")]),e._v(" "),t("p",[e._v('!!! note "Argument: Against scaling - Memorization vs Synthesis"')]),e._v(" "),t("tab",[t("p",[e._v("This is a slightly deeper dive into trying to understand the argument of both sides if you are interested. It is slightly more technical, and can be safely skipped.")]),e._v(" "),t("p",[e._v("When we talk about LLMs as \"interpolative databases\", we're referring to how they store and manipulate vector programs - these shou;dnt be confused with traditional computer programs like python or C++. These templates, or vector programs are transformations in the model's embedding space. Early work on embeddings showed simple transformations (like king - man + woman = queen), but modern LLMs can store millions of much more complex transformations. But due to a function of scale, LLMs can now store arbitrarily complex vector functions ‚Äî so complex, in fact, that researchers found it more accurate to refer to them as vector programs rather than functions.")]),e._v(" "),t("p",[e._v('So what\'s happening in LLMs is that they build up a vast database of these vector programs through pre-training. When we say they\'re doing "template matching" or "memorization", what we really mean is that they\'re storing millions of these vector transformations that they can retrieve and combine with each prompt.')]),e._v(" "),t("p",[e._v("So the deciding question for/against strong (and even weak scaling) becomes - Is this type of template program combination enough to reach general intelligence. In other words can program synthesis be approximated using recombinations of enough templates (also called abstractions and many other words but the key idea is the same).")]),e._v(" "),t("p",[e._v('People who argue against this say that no matter how numerous or sophisticated, are fundamentally different from true program synthesis. True program synthesis would mean deriving a new solution from first principles - not just recombining existing transformations. There are some empirical observations to support this view. Like the Caesar cipher example: "LLMs can solve a Caesar cipher with key size 3 or 5, but fail with key size 13, because they\'ve memorized specific solutions rather than understanding the general algorithm" ('),t("a",{attrs:{href:"https://www.youtube.com/watch?v=nL9jEy99Nh0",target:"_blank",rel:"noopener noreferrer"}},[e._v("Chollet, 2024"),t("OutboundLink")],1),e._v('). Or alternatively, the "reversal curse" which shows that even SOTA language models in 2024 cannot do reverse causal inference - if they are trained on "A is B" they fail to learn "B is A" ('),t("a",{attrs:{href:"https://arxiv.org/abs/2309.12288",target:"_blank",rel:"noopener noreferrer"}},[e._v("Berglund et al., 2023"),t("OutboundLink")],1),e._v(")")]),e._v(" "),t("p",[e._v("But this does still not seem to completely invalidate scaling as of yet. If we scale up the size of the program database and cram more knowledge and patterns into it, we are going to be increasing its performance. ("),t("a",{attrs:{href:"https://www.dwarkeshpatel.com/p/francois-chollet",target:"_blank",rel:"noopener noreferrer"}},[e._v("Chollet, 2024"),t("OutboundLink")],1),e._v(") Both sides of the debate agree on this. So this suggests the real issue isn't whether template recombination has an obvious absolute upper bound, but whether it's the most efficient path to general intelligence. Program synthesis might achieve the same capabilities with far less compute and data by learning to derive solutions rather than memorizing patterns.")])]),e._v(" "),t("h1",{attrs:{id:"forecasting"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#forecasting"}},[e._v("#")]),e._v(" Forecasting")]),e._v(" "),t("p",[e._v("In previous sections, we explored how foundation models leverage computation through scaling laws and the bitter lesson. But how can we actually predict where AI capabilities are headed? This section introduces key forecasting methodologies that help us anticipate AI progress and prepare appropriate safety measures.")]),e._v(" "),t("p",[t("strong",[e._v("Why should we care about forecasting?")]),e._v(" Forecasting AI progress is critical for AI safety work. The timeline to transformative AI shapes everything from research priorities to governance frameworks ‚Äì if we expect transformative AI within 5 years versus 50 years, this dramatically changes which safety approaches are viable. For example, if we expect rapid progress, we might need to focus on safety measures that can be implemented quickly rather than long-term theoretical research. Additionally, understanding likely development trajectories helps us anticipate specific capabilities and prepare targeted safety measures before they emerge. This is especially critical given the potential for sudden capability jumps, especially in dangerous capabilities like malware generation or deception.")]),e._v(" "),t("p",[e._v('!!! question "Initial Forecast"')]),e._v(" "),t("tab",[t("p",[e._v("Before we explore more sophisticated methods, make an initial prediction: When do you think we'll see transformative AI? Keep this forecast in mind as we examine different forecasting approaches.")])]),e._v(" "),t("h2",{attrs:{id:"methodology"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#methodology"}},[e._v("#")]),e._v(" Methodology")]),e._v(" "),t("p",[t("strong",[e._v("How do we convert beliefs into probabilities and forecasts?")]),e._v(' We need some ways to actually convert beliefs like "I think AGI is likely this decade" into precise probability estimates. One way we can do this is by decomposition - breaking down complex beliefs into smaller, measurable components and analyzing relevant data. Rather than directly estimating the year in which transformative AI emerges, we can start by separately forecasting things like compute growth, algorithmic progress, and hardware limitations, and then combine these estimates ('),t("a",{attrs:{href:"https://forecasting-sp24.quarto.pub/forecasting-sp24/estimation.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("Zhang, 2024"),t("OutboundLink")],1),e._v("). This decomposition approach helps us ground predictions in observable trends rather than relying purely on intuitions. So, using this approach there are two main techniques we need to discuss - zeroth-order forecasting for establishing baselines, and first-order forecasting for understanding trajectories of change.")]),e._v(" "),t("p",[t("strong",[e._v("What are reference classes and why do they matter?")]),e._v(" When analyzing each component of our decomposed forecast, we need relevant historical examples to inform our predictions. This is where reference classes come in - they are categories of similar historical situations we can use to make predictions. For AI development, relevant reference classes might include things like previous technological revolutions (like the industrial or computer revolution), other optimization systems (like biological evolution or economies), or the impact of rapid scientific advances (like CRISPR or mRNA vaccines). The basic point is that they should be meaningfully analogous to what you're trying to predict, but they don't have to be from the same exact category.")]),e._v(" "),t("p",[t("strong",[e._v("What is zeroth-order forecasting?")]),e._v(" The simplest forecasting approach starts with recognizing that tomorrow often looks pretty close to today. Zeroth-order forecasting uses reference classes - looking at 3-5 similar historical examples and using their average as a baseline prediction. Rather than trying to identify trends or make complex projections, it assumes recent patterns will continue. ("),t("a",{attrs:{href:"https://forecasting-sp24.quarto.pub/forecasting-sp24/zeroth-first.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("Steinhardt, 2024"),t("OutboundLink")],1),e._v(') These examples can be from different reference classes. For a concrete example of using multiple reference classes in AI safety forecasting: Suppose we want to predict how quickly advanced AI systems might transition from "safe" to "potentially dangerous" capabilities. We could look at:')]),e._v(" "),t("ul",[t("li",[t("p",[e._v("How long it took language models to go from generating basic text to being able to plan multi-step deception (an AI-specific reference point)")])]),e._v(" "),t("li",[t("p",[e._v("How quickly nuclear technology transitioned from peaceful to military applications (a dual-use technology reference point)")])]),e._v(" "),t("li",[t("p",[e._v("How rapidly biological techniques like CRISPR went from lab discovery to widespread use requiring safety protocols (a biosafety reference point)")])])]),e._v(" "),t("p",[e._v("Looking at these examples collectively might suggest that dangerous capabilities often emerge within 2-5 years of the underlying technical breakthroughs, which could inform how urgently we need to develop safety measures. Major shifts in development patterns tend to be rare, making recent history a decent baseline predictor of the near future. This doesn't mean changes never happen ‚Äì but it does mean that deviating from recent patterns requires strong evidence.")]),e._v(" "),t("p",[t("strong",[e._v("What is first-order forecasting?")]),e._v(" While zeroth-order forecasting uses historical examples from various reference classes as direct predictors, first-order forecasting attempts to identify and project forward patterns in the direct historical data of AI development. In AI, we see some pretty consistent exponential growth patterns. The compute used in frontier models has grown by 4.2x annually since 2010, training datasets have expanded by approximately 2.9x per year, and hardware performance improves by roughly 1.35x every year through architectural advances ("),t("a",{attrs:{href:"https://epoch.ai/trends",target:"_blank",rel:"noopener noreferrer"}},[e._v("Epoch AI, 2023"),t("OutboundLink")],1),e._v("). First-order forecasting tries to identify these kinds of patterns and project them forward. This is the approach taken by most systematic AI forecasting work today, including Epoch AI's compute-centric framework and Ajeya Cotra's biological anchors. However, it's worth keeping in mind that even though these trends have been remarkably consistent, they can't continue indefinitely. Physical, thermodynamic, or economic constraints will eventually limit growth. The key question is: when do these limits become relevant? We will explore this in the next section on the compute centric framework.")]),e._v(" "),t("p",[t("strong",[e._v("How do we combine different forecasts?")]),e._v(" Multiple forecasting approaches often give us different predictions ‚Äì zeroth-order might suggest one timeline while trend extrapolation indicates another. Just like we can average out over the opinions of many experts, we can integrate these predictions to get a hopefully more accurate picture. One approach is to model each forecast as a probability distribution and combine them using mixture models ("),t("a",{attrs:{href:"https://forecasting-sp24.quarto.pub/forecasting-sp24/combining-forecasts-new.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("Steinhardt, 2024"),t("OutboundLink")],1),e._v("). For example, if zeroth-order forecasting suggests 3-4 years between major breakthroughs based on recent history, while trend extrapolation points to 1.5-2 years based on compute growth, a combined model might predict 2-3 years but with wider confidence intervals to account for uncertainty in both approaches.")]),e._v(" "),t("p",[e._v("**What about situations with limited data or limited reference classes? **While decomposition, reference classes and trend analysis form the backbone of AI forecasting, we sometimes face questions where direct data is limited or no clear reference classes exist. For instance, predicting the societal impact of advanced AI systems or forecasting novel capabilities that haven't been demonstrated before. In these cases, we often turn to expert judgment and superforecasters. An advantage of expert forecasting is the ability to integrate qualitative insights that might be missed by pure trend analysis. For example, experts might notice early warning signs of diminishing returns or identify emerging technical approaches that could accelerate progress. This balanced use of both data-driven methods and expert judgment is especially important for AI safety work. While we should ground our predictions in empirical trends whenever possible, we also need frameworks for reasoning about unprecedented developments and potential discontinuities in progress.")]),e._v(" "),t("p",[t("strong",[e._v("How far do empirical findings generalize?")]),e._v(" There's an ongoing debate about how much we can trust current trends to predict future AI development. Some researchers argue that empirical findings in AI generalize surprisingly far - that patterns we observe today will continue to hold even as systems become more capable ("),t("a",{attrs:{href:"https://www.lesswrong.com/posts/ekFMGpsfhfWQzMW2h/empirical-findings-generalize-surprisingly-far",target:"_blank",rel:"noopener noreferrer"}},[e._v("Steinhardt, 2022"),t("OutboundLink")],1),e._v("). However, our track record with forecasting suggests we should be cautious. When superforecasters predicted MATH dataset accuracy would improve from 44% to 57% by June 2022, actual performance reached 68% - a level they had rated extremely unlikely. Shortly after, GPT-4 achieved 86.4% accuracy. There are a couple of more examples of LLMs surprising most forecasters and experts on certain benchmarks. ("),t("a",{attrs:{href:"https://www.planned-obsolescence.org/language-models-surprised-us/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Cotra, 2023"),t("OutboundLink")],1),e._v(").")]),e._v(" "),t("p",[e._v("This pattern of underestimating progress suggests that while empirical trends provide valuable guidance, they may not capture all the dynamics of AI development. Prior to GPT-3, many experts believed tasks like complex reasoning would require specialized architectures. The emergence of these capabilities from scaling alone shows how systems can develop unexpected abilities simply through quantitative improvements. This has critical implications for both forecasting and governance - we need frameworks that can adapt to capabilities emerging faster or differently than current trends suggest.")]),e._v(" "),t("p",[e._v("**How does this help us predict transformative AI? **These forecasting fundamentals help us critically evaluate claims about AI timelines and takeoff scenarios. When we encounter predictions about discontinuous progress or smooth scaling, we can ask: What trends support this view? What reference classes are relevant? How have similar forecasts performed historically? This systematic approach helps us move beyond intuition to make more rigorous predictions about AI development trajectories.")]),e._v(" "),t("p",[e._v("<--")]),e._v(" "),t("p",[e._v("Trend Based Forecasting")]),e._v(" "),t("p",[e._v("This is an entire new subsection that will go here. It is still to be written.")]),e._v(" "),t("p",[e._v("--\x3e")]),e._v(" "),t("h2",{attrs:{id:"biology-inspired-forecasting"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#biology-inspired-forecasting"}},[e._v("#")]),e._v(" Biology Inspired Forecasting")]),e._v(" "),t("p",[t("strong",[e._v("What are Biological anchors?")]),e._v(" Biological anchors are a forecasting technique. To find a reference class, assume that the human brain is indicative of general intelligence. This means we can treat it as a proof of concept. Whatever ‚Äúamount of compute‚Äù it takes to train a human being, might be roughly the same amount it should take to train a TAI. The biological anchors approach estimates the compute required for AI to reach a level of intelligence comparable to humans, outlined through several steps:")]),e._v(" "),t("ul",[t("li",[t("p",[e._v("First, assess how much computation the human brain performs, translating this into a quantifiable measure similar to computer operations in FLOP/s.")])]),e._v(" "),t("li",[t("p",[e._v("Second, estimate the amount of computation needed to train a neural network to match the brain's inferential capacity, adjusting for future improvements in algorithmic efficiency.")])]),e._v(" "),t("li",[t("p",[e._v("Third, examine when it would be feasible to afford such vast computational resources, taking into account the decreasing cost of compute, economic growth, and increasing investment in AI.")])]),e._v(" "),t("li",[t("p",[e._v("Finally, by analyzing these factors, we can predict when it might be economically viable for AI companies to deploy the necessary resources for developing TAI.")])])]),e._v(" "),t("p",[e._v('Determining the exact computational equivalent for the human brain\'s training process is complex, leading to the proposal of six hypotheses, collectively referred to as "biological anchors" or "bioanchors." Each anchor has a different weighting contributing to the overall prediction.')]),e._v(" "),t("p",[e._v("Evolution Anchor: Total computational effort across all evolutionary history.")]),e._v(" "),t("p",[e._v("Lifetime Anchor: Brain's computational activity from birth to adulthood (0-32).")]),e._v(" "),t("p",[e._v("Neural Network and Genome Anchors: Various computational benchmarks based on the human brain and genome to gauge the scale of parameters needed for AI to achieve general intelligence.")]),e._v(" "),t("p",[t("strong",[e._v("Forecasting with Biological Anchors")]),e._v('. By integrating these anchors with projections of future compute accessibility, we can outline a potential timeline for TAI. This method aims to provide a "soft upper bound" on TAI\'s arrival rather than pinpointing an exact year, acknowledging the complexity and unpredictability of AI development. ('),t("a",{attrs:{href:"https://forum.effectivealtruism.org/posts/ajBYeiggAzu6Cgb3o/biological-anchors-is-about-bounding-not-pinpointing-ai",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(") The following image gives an overview of the methodology.")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/4IF_Image_20.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: The biological anchor model ("),t("strong",[t("a",{attrs:{href:"https://epochai.org/blog/grokking-bioanchors",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[e._v('=== "'),t("strong",[e._v("Evolution anchor")]),e._v('"')]),e._v(" "),t("tab",[t("p",[e._v('!!! warning "This is extra detail provided for those interested. It can be safely skipped."')]),e._v(" "),t("p",[e._v('This anchor quantifies the computational effort invested by evolution in shaping the human brain. It considers the vast amount of processing and learning that has taken place from the emergence of the first neurons to the development of the modern human brain. This method suggests that evolution has served as a form of "pre-training" for the human brain, enhancing its ability to adapt and survive. To estimate the computational power of this evolutionary "pre-training", the report considers the total amount of compute used by all animal brains over the course of evolution. This includes not just the brains of humans, but also those of our ancestors and other animals with nervous systems. The idea is that all of this brain activity represents a form of learning or adaptation that has contributed to the development of the modern human brain. While the exact calculations involved in this estimate are complex and subject to considerable uncertainty, the basic idea is to multiply the number of animals that have ever lived by the amount of compute each of their brains performed over their lifetimes. This gives an estimate of the total compute performed by all animal brains over the course of evolution.')]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/jAr_Image_21.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: The Evolution anchor ("),t("strong",[t("a",{attrs:{href:"https://epochai.org/blog/grokking-bioanchors",target:"_blank",rel:"noopener noreferrer"}},[e._v("Ho et al., 2022"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[e._v("Cotra accounts for these considerations and assumes that the ‚Äúaverage ancestor‚Äù performed as many FLOP/s as a nematode, and that there were on average ~1e21 ancestors at any time. This yields a "),t("strong",[e._v("median of ~1e41 FLOP")]),e._v(", which seems extraordinarily high compared to modern machine learning. As an example, Google‚Äôs PaLM model was trained with ~2.5e24 FLOP (17 orders of magnitude smaller). She gives this anchor a "),t("strong",[e._v("weight of 10%")]),e._v(". ("),t("a",{attrs:{href:"https://epochai.org/blog/grokking-bioanchors",target:"_blank",rel:"noopener noreferrer"}},[e._v("Ho et al., 2022"),t("OutboundLink")],1),e._v(")")])]),e._v(" "),t("p",[e._v('=== "'),t("strong",[e._v("Lifetime anchor")]),e._v('"')]),e._v(" "),t("tab",[t("p",[e._v('!!! warning "This is extra detail provided for those interested. It can be safely skipped."')]),e._v(" "),t("p",[e._v('This refers to the total computational activity the human brain performs over a human lifetime. This anchor is essentially a measure of the "training" a human brain undergoes from birth to adulthood and incorporates factors such as the number of neurons in the human brain, the amount of computation each neuron performs per year, and the number of years it takes for a human to reach adulthood. The human brain has an estimated 86 billion neurons. Each of these neurons performs a certain number of computations per second, which can be calculated as a certain number of operations per second in FLOP/s. When calculating the total amount of compute over a lifetime, these factors are multiplied together, along with the number of years a human typically lives.')]),e._v(" "),t("p",[e._v("For example, if we assume that a neuron is able to perform about 1000 operations per second, and there are about 31.5 million seconds in a year, then a single neuron would perform about 31.5 billion operations in a year. Now, if we multiply this by the estimated number of neurons in the human brain (86 billion), we get an estimate of the total brain-compute performed in one year. We can then multiply this by the number of years in a typical human lifespan to estimate the total brain-compute over a lifetime. Plugging in the numbers about brain FLOP/s seems to suggest that ~1e27 FLOP would be required to reach TAI. This seems low since examples from other technological domains suggest that the efficiency of things we build (on relevant metrics) is generally not great when compared to nature.")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/UP9_Image_22.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: The Lifelong anchor ("),t("strong",[t("a",{attrs:{href:"https://epochai.org/blog/grokking-bioanchors",target:"_blank",rel:"noopener noreferrer"}},[e._v("Ho et al., 2022"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[e._v("Cotra's report finds a median of ~1e28 FLOP and only gives the lifetime anchor a 5% weight, indicating that it's not the most influential factor in the overall model. The report acknowledges the inherent uncertainties and complexities involved in such a calculation and uses this anchor as one of several to provide a range of estimates for the compute required to achieve AI with human-level performance. ("),t("a",{attrs:{href:"https://epochai.org/blog/grokking-bioanchors",target:"_blank",rel:"noopener noreferrer"}},[e._v("Ho et al., 2022"),t("OutboundLink")],1),e._v(")")])]),e._v(" "),t("p",[e._v('=== "'),t("strong",[e._v("Neural network Anchors")]),e._v('"')]),e._v(" "),t("tab",[t("p",[e._v('!!! warning "This is extra detail provided for those interested. It can be safely skipped."')]),e._v(" "),t("p",[e._v("Each of the neural network anchors serves to provide a different perspective on the amount of compute that might be required to train a TAI. There are three Neural Network Anchors presented in the report: long (~1e37 FLOP), medium (~1e34 FLOP), and short horizon (~1e32 FLOP). These anchors hypothesize that the ratio of parameters to compute used by a TAI should be similar to the ratio observed in today's neural networks. Additionally, a TAI should perform approximately as many FLOPs per subjective second as a human brain. A ‚Äúsubjective second‚Äù is the time it takes a model to process as much data as a human can in one second ("),t("a",{attrs:{href:"https://epochai.org/blog/grokking-bioanchors",target:"_blank",rel:"noopener noreferrer"}},[e._v("Ho et al., 2022"),t("OutboundLink")],1),e._v("). As an example a typical human reads about 3-4 words per second for non-technical material, so ‚Äúone subjective second‚Äù for a language model would correspond to however much time that the model takes to process about ~3-4 words of data. ("),t("a",{attrs:{href:"https://www.alignmentforum.org/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines",target:"_blank",rel:"noopener noreferrer"}},[e._v("Cotra, 2020"),t("OutboundLink")],1),e._v(") Cotra determines the training data requirements based on a mix of machine learning theory and empirical considerations. She puts 15% weight on short horizons, 30% on medium horizons, and 20% on long horizons, for a total of 65% on the three anchors. ("),t("a",{attrs:{href:"https://epochai.org/blog/grokking-bioanchors",target:"_blank",rel:"noopener noreferrer"}},[e._v("Ho et al., 2022"),t("OutboundLink")],1),e._v(")")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/Day_Image_23.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: "),t("strong",[e._v("Neural network Anchors")]),e._v("("),t("strong",[t("a",{attrs:{href:"https://epochai.org/blog/grokking-bioanchors",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1)]),e._v(")*")])]),e._v(" "),t("p",[e._v('=== "'),t("strong",[e._v("Genome Anchor")]),e._v('"')]),e._v(" "),t("tab",[t("p",[e._v('!!! warning "This is extra detail provided for those interested. It can be safely skipped."')]),e._v(" "),t("p",[e._v("The genome anchor looks at the FLOP/subj sec of the human brain and expects TAI to require as many parameters as there are bytes in the human genome. This hypothesis implicitly assumes a training process that‚Äôs structurally analogous to evolution, and that TAI will have some critical cognitive ability that evolution optimized for. This differs from the evolution anchor in that it assumes we can search over possible architectures/algorithms a lot more efficiently than evolution, using gradients. Due to this structural similarity, and because feedback signals about the fitness of a particular genome configuration are generally sparse, this suggests that the anchor only really makes sense with long horizon lengths. ("),t("a",{attrs:{href:"https://epochai.org/blog/grokking-bioanchors",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(")")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/hu8_Image_24.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: The Genome Anchor ("),t("strong",[t("a",{attrs:{href:"https://epochai.org/blog/grokking-bioanchors",target:"_blank",rel:"noopener noreferrer"}},[e._v("Ho et al., 2022"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[e._v("At least at the time of writing (May 2022), machine learning architectures don‚Äôt look very much like the human genome, and we are yet to develop TAI ‚Äì thus Cotra updates against this hypothesis towards requiring more FLOP. Overall, she finds a median of ~1e33 FLOP and places 10% weight on this anchor. "),t("em",[e._v("("),t("strong",[t("a",{attrs:{href:"https://epochai.org/blog/grokking-bioanchors",target:"_blank",rel:"noopener noreferrer"}},[e._v("Ho et al., 2022"),t("OutboundLink")],1)]),e._v(")")])])]),e._v(" "),t("p",[e._v("**Affordability of compute. **The costs related to bio anchors are calculated by considering three different factors: Algorithmic progress, compute price estimates, and willingness to spend on machine learning. The report considers a doubling in algorithmic efficiency every ~2-3 years. As for prices, Cotra assumes cost decreases over time, halving every ~2.5 years, and further expects this to level off after 6 orders of magnitude. Cotra assumes that the willingness to spend on machine learning training runs should be capped at 1% of the GDP of the largest country, referencing previous case studies with megaprojects (e.g. the Manhattan Project), and should follow a doubling time of 2 years after 2025. The main uncertainty is whether or not existing trends are going to persist more than several years into the future. For instance, Epoch found that OpenAI‚Äôs AI and Compute investigation ("),t("a",{attrs:{href:"https://openai.com/blog/ai-and-compute/",target:"_blank",rel:"noopener noreferrer"}},[e._v("OpenAI, 2018"),t("OutboundLink")],1),e._v(") was too aggressive in its findings for compute growth. ("),t("a",{attrs:{href:"https://epochai.org/blog/grokking-bioanchors",target:"_blank",rel:"noopener noreferrer"}},[e._v("Ho et al., 2022"),t("OutboundLink")],1),e._v(") This suggests taking caution when interpreting the forecasts made by the Bio Anchors report.")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/F6B_Image_25.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: "),t("strong",[e._v("Affordability of compute "),t("strong",[e._v("(")]),t("a",{attrs:{href:"https://epochai.org/blog/grokking-bioanchors",target:"_blank",rel:"noopener noreferrer"}},[e._v("Ho et al., 2022"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[e._v("The following graph gives an overview of the findings. Overall, the graph takes a weighted average of the different ways that the trajectory could flow. This gives us an estimate of a >10% chance of transformative AI by 2036, a ~50% chance by 2055, and an ~80% chance by 2100. In 2022 a two-year update on the author‚Äôs (Ajeya Cotra) timelines was published. The updated timelines for TAI are ~15% probability by 2030, ~35% probability by 2036, a median of ~2040, and a ~60% probability by 2050. ("),t("a",{attrs:{href:"https://www.alignmentforum.org/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines",target:"_blank",rel:"noopener noreferrer"}},[e._v("Cotra, 2022"),t("OutboundLink")],1),e._v(")")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/1e4_Image_26.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[e._v("Figure: Results from the biological anchor model for different anchors ("),t("a",{attrs:{href:"https://forum.effectivealtruism.org/posts/vCaEnTbZ5KbypaGsm/forecasting-transformative-ai-the-biological-anchors-method",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(")")]),e._v(" "),t("p",[e._v("**Criticisms. **The Biological Anchors framework provides a unique perspective but it's also crucial to recognize its limitations and the broader debates it sparks within the AI research community. It is not universally accepted as the primary predictive tool among all ML scientists or alignment researchers.")]),e._v(" "),t("p",[e._v('Platt\'s Law is a generalized observation named after Charles Platt. It is used to highlight a historical pattern where the estimated arrival of AGI (Artificial General Intelligence) consistently seems to be "just 30 years away". Vernor Vinge referenced it in the body of his famous 1993 NASA speech, whose abstract begins, ‚Äú'),t("em",[e._v("Within thirty years, we will have the technological means to create superhuman intelligence. Shortly after, the human era will be ended.")]),e._v("‚Äù ("),t("a",{attrs:{href:"https://intelligence.org/2021/12/03/biology-inspired-agi-timelines-the-trick-that-never-works/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Yudkowsky, 2021"),t("OutboundLink")],1),e._v(') Yudkowsky notes that this law seems to fit remarkably well with the prediction made by the Biological Anchors report in 2020. As the statistical aphorism goes: "All models are wrong, but some are useful".')]),e._v(" "),t("p",[e._v("So to get a complete picture of how biological anchors were received, here are some of the criticisms of the Biological Anchors report:")]),e._v(" "),t("ul",[t("li",[t("p",[t("strong",[e._v("Challenges Beyond Compute")]),e._v(": While Biological Anchors highlight compute power as a critical factor for AI development, they may oversimplify the complexity of achieving transformative AI. Factors beyond sheer computing capacity, such as algorithm design, data availability, and the intricacies of learning environments, play pivotal roles. It‚Äôs overly simplistic to equate the future of AI solely with compute availability, as transformative AI development encompasses more nuanced challenges like algorithm innovation and data accessibility. ("),t("a",{attrs:{href:"https://www.cold-takes.com/biological-anchors-is-about-bounding-not-pinpointing-ai-timelines/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Karnofsky, 2021"),t("OutboundLink")],1),e._v(")")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Potential for Rapid Advancements")]),e._v(": Critics of the Biological Anchors method, such as Eliezer Yudkowsky, emphasize the unpredictability of AI progress and the potential for breakthroughs that could dramatically alter AI capabilities without adhering strictly to computational benchmarks derived from biology. These critiques highlight the importance of considering a range of factors and potential shifts in AI development paradigms that could accelerate progress beyond current forecasts. ("),t("a",{attrs:{href:"https://www.alignmentforum.org/posts/nNqXfnjiezYukiMJi/reply-to-eliezer-on-biological-anchors",target:"_blank",rel:"noopener noreferrer"}},[e._v("Karnofsky, 2021"),t("OutboundLink")],1),e._v(")")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Purpose and Misinterpretation")]),e._v(": The Biological Anchors approach is intended to provide boundary estimates for AI development timelines rather than precise predictions. Misinterpretations may arise from expecting the method to offer specific year forecasts, whereas its goal is to outline possible upper and lower bounds, acknowledging the significant uncertainties in AI development. ("),t("a",{attrs:{href:"https://www.cold-takes.com/biological-anchors-is-about-bounding-not-pinpointing-ai-timelines/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Karnofsky, 2021"),t("OutboundLink")],1),e._v(")")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Contemplating Paradigm Shifts")]),e._v(": The AI field's history suggests that major paradigm shifts and technological breakthroughs could substantially impact development timelines. While deep learning currently dominates AI advances, the possibility of new, transformative methodologies emerging remains open, challenging the assumption that current trends will linearly continue into the future.")])])]),e._v(" "),t("p",[e._v("This is not an exhaustive list of all the criticisms but it serves to highlight the complexity of forecasting AI's future.")]),e._v(" "),t("h1",{attrs:{id:"takeoff"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#takeoff"}},[e._v("#")]),e._v(" Takeoff")]),e._v(" "),t("p",[e._v("This section introduces the concept of AI takeoff. There is much debate about how AI development will play out in the future. The main questions to consider are:")]),e._v(" "),t("ul",[t("li",[t("p",[t("strong",[e._v("Speed")]),e._v(": How quickly do AI systems become more powerful?")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Continuity")]),e._v(": Does the speed progress in sudden jumps or is the speed gradual?")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Homogeneity")]),e._v(": How similar are the AI systems to each other?")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Polarity")]),e._v(": How many AI systems do we see vying for power in the world?")])])]),e._v(" "),t("p",[t("strong",[e._v("Takeoff vs. Timelines")]),e._v(". The difference between these two concepts is subtle, and forecasts in one domain affect the outlooks in the other. However, both takeoff and timelines offer some distinct insights: timelines provide a countdown to AI breakthroughs, while takeoff dynamics forecast the speed and breadth of their societal impact. Understanding both perspectives is key to preparing for the future of AI, guiding safety measures, policy development, and strategic research investment.")]),e._v(" "),t("h2",{attrs:{id:"speed"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#speed"}},[e._v("#")]),e._v(" Speed")]),e._v(" "),t("p",[t("strong",[e._v("Takeoff Speeds")]),e._v(". Slow and fast takeoff scenarios offer contrasting views on AI's future trajectory, emphasizing different challenges and preparation strategies. While slow takeoff allows for iterative safety measures and societal adaptation, fast takeoff underscores the urgency of robust, pre-established safety frameworks. Understanding both dynamics is crucial for informed AI governance, safety research, and policy development, ensuring readiness for a range of possible futures.")]),e._v(" "),t("p",[t("strong",[e._v("Slow takeoff")]),e._v('. This refers to a scenario where AI capabilities improve in a gradual and measured manner over a relatively longer temporal interval, which could be years or even decades. Note that the "speed" of the takeoff is not just about the raw increase in AI capabilities, but also about the societal and economic impact of those capabilities. Slow takeoff\'s advantage lies in the preparation time it affords, potentially allowing us to adapt, respond, and correct course if things go wrong. However, on the other hand, more time being exposed to a certain growth trajectory might also breed complacency. It might also be the case that most preparations undertaken before the onset of a slow takeoff would be rendered obsolete as better solutions gradually become visible which utilize new AI capabilities. In a slow takeoff scenario, humanity‚Äôs fate essentially depends on preparations put in place during the times when such growth is apparent. The terms "slow takeoff" and "soft takeoff" are often used interchangeably.')]),e._v(" "),t("p",[t("strong",[e._v("Fast takeoff")]),e._v(". A \"fast takeoff\" envisages AI's leap to superintelligence in a short timeframe, such as hours or days, limiting human capacity for response or control. This speed leaves little to no room for humans to react, adapt, or manage the situation. Fast takeoff scenarios highlight the critical importance of preemptive safety and alignment efforts, as the window for post-emergence intervention could be minimal. Given such a short time to react, local dynamics of the takeoff become relevant. The primary mechanism behind a fast takeoff is often considered to be 'recursive self-improvement' through an ‚Äúintelligence explosion‚Äù, where the AI system continually improves its own intelligence and capabilities. We talk about an intelligence explosion, and discontinuity in the next subsection. Terms like ‚Äúfast takeoff‚Äù, ‚Äúhard takeoff‚Äù and ‚ÄúFOOM‚Äù are often used interchangeably.")]),e._v(" "),t("p",[t("strong",[e._v("Speed vs. Continuity.")]),e._v(' We observe that AI systems are created in discrete "events", sometimes many months or years apart. As an example GPT-3 was released in June 2020, and GPT-4 was released in March 2023. Is there a sudden, extremely large jump between such releases? or could the capabilities have been predicted by extrapolating trends? Where speed might be a measure of how quickly the AI becomes superintelligent, continuity can be thought of as a measure of "surprise". Continuity measures the predictability and pattern of AI advancements, focusing on whether these advancements occur through gradual, steady improvements or unexpected breaks in previous technological trends. Even though we are presenting speed and continuity separately, the distinction between them is extremely nuanced. They are very related concepts, so much so that oftentimes most researchers use slow takeoff as analogous to continuous takeoff, and fast takeoff as analogous to discontinuous takeoff, making no distinction whatsoever.')]),e._v(" "),t("p",[t("strong",[e._v("Continuous takeoff")]),e._v(". Continuous takeoff depicts AI's progression as an extension of existing trends, where advancements are made incrementally. An example is the steady advancement of technology witnessed during the Industrial Revolution, where each decade produced slightly better tools and machinery. The term 'continuous' in this context borrows from mathematical terminology, where a function is considered continuous if it doesn't have any abrupt changes or discontinuities. Some people prefer the term 'gradual' or 'incremental' over 'continuous' as they find it more descriptive and mathematically accurate.")]),e._v(" "),t("p",[t("strong",[e._v("Discontinuous Takeoff")]),e._v(". The term 'discontinuous' in this context suggests that there are sudden jumps in capabilities, rather than being a smooth, gradual increase. This means that the AI system makes leaps that significantly exceed what would be expected based on past progress. In a discontinuous takeoff scenario, the AI's capabilities quickly surpass human control and understanding. The terms 'fast takeoff' and 'discontinuous takeoff' are often used interchangeably. However, the images below displaying different takeoff trajectories might help in clarifying the subtle differences between the concepts.")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/kIj_Image_27.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: A comparative illustration of combinations of slow and fast takeoff, alongside continuity. ("),t("strong",[t("a",{attrs:{href:"https://www.alignmentforum.org/posts/pGXR2ynhe5bBCCNqn/takeoff-speeds-and-discontinuities",target:"_blank",rel:"noopener noreferrer"}},[e._v("Martin & Eth, 2021"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("h2",{attrs:{id:"similarity"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#similarity"}},[e._v("#")]),e._v(" Similarity")]),e._v(" "),t("p",[e._v("**Homogenous takeoff. **This refers to the similarity among different AI systems in play during the development and deployment of advanced AI. ("),t("a",{attrs:{href:"https://www.lesswrong.com/posts/mKBfa8v4S9pNKSyKK/homogeneity-vs-heterogeneity-in-ai-takeoff-scenarios",target:"_blank",rel:"noopener noreferrer"}},[e._v("Hubinger, 2020"),t("OutboundLink")],1),e._v(") Future AI systems might share significant similarities in their design, capabilities, and alignment. This could be due to shared underlying foundation models, common APIs, or similar training methods. The uniformity of AI systems under a homogenous takeoff scenario might imply the potential for easy cooperation between AI entities. Additionally, if homogeneity arises due to fine-tuning prevailing as the paradigm; the alignment of the initial AI system becomes the crucial factor for the alignment of all subsequent AI systems. A single misaligned system could, therefore, have widespread consequences, underscoring the importance of rigorous alignment and safety measures in early AI development stages. Homogeneity could either mitigate risks or exacerbate them, depending on the alignment and governance frameworks established during the initial phases of AI development.")]),e._v(" "),t("p",[t("strong",[e._v("Heterogeneity")]),e._v(". A heterogeneous takeoff scenario represents diversity and variation in the development, safety levels, and deployment of AI systems. Several factors contribute to the heterogeneity in AI takeoff. First is the diversity in AI development approaches. Organizations and researchers might adopt different strategies and technologies, leading to varied outcomes in terms of AI capabilities and alignment. Moreover, competitive dynamics among AI projects could exacerbate this diversity, as teams race to achieve breakthroughs without necessarily aligning on methodologies or sharing crucial information. As an example, we might have a future where AI becomes a strategic national asset, and AI development is closely guarded. In this environment, the pursuit of AI capabilities becomes siloed, each company or country would then employ different development methodologies, potentially leading to a wide range of behaviors, functionalities, and safety levels.")]),e._v(" "),t("h2",{attrs:{id:"polarity"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#polarity"}},[e._v("#")]),e._v(" Polarity")]),e._v(" "),t("p",[e._v("Takeoff polarity in the context of AI development explores how power and control might be distributed during the critical phase when AI systems transition to higher levels of intelligence and capability. It measures whether we will see power centralized in the hands of a single entity or dispersed among various actors.")]),e._v(" "),t("p",[t("strong",[e._v("Unipolar Takeoff")]),e._v(". In a unipolar takeoff scenario, the landscape is characterized by the emergence of a single AI system or entity that achieves a significant lead over all others. This lead could be due to a breakthrough that allows for a sudden leap in capabilities, enabling this entity to outpace competitors rapidly, or due to something like an intelligence explosion. The concept hinges on the idea that early advantages in AI development‚Äîbe it through technological breakthroughs, resource accumulation, or strategic positioning‚Äîcould allow one project to dominate the field. This dominance could then be used to consolidate power, leveraging economic gains and network effects to maintain a hold over AI advancements and their applications.")]),e._v(" "),t("p",[t("strong",[e._v("Multipolar Takeoff")]),e._v(". Conversely, a multipolar takeoff scenario envisions a more balanced distribution of advanced AI systems across multiple entities. This diversity could stem from slower, more incremental progress in AI capabilities, allowing various projects to evolve in tandem without any single entity pulling decisively ahead. Factors contributing to a multipolar landscape include widespread sharing of technological advancements, collaborative efforts to ensure safety and alignment, and regulatory environments that promote competition and mitigate monopolistic outcomes. A unipolar scenario raises concerns about the concentration of power and the potential for misuse of advanced AI capabilities, while a multipolar world presents challenges in coordination among diverse entities or AI systems.")]),e._v(" "),t("p",[e._v("Factors Influencing Polarity. Several key elements influence whether takeoff polarity leans towards a unipolar or multipolar outcome:")]),e._v(" "),t("ul",[t("li",[t("p",[e._v("Speed of AI Development: A rapid takeoff might favor a unipolar outcome by giving a significant advantage to the fastest developer. In contrast, a slower takeoff could lead to a multipolar world where many entities reach advanced capabilities more or less simultaneously.")])]),e._v(" "),t("li",[t("p",[e._v("Collaboration vs. Competition: The degree of collaboration and openness in the AI research community can significantly affect takeoff polarity. High levels of collaboration and information sharing could support a multipolar outcome, while secretive or highly competitive environments might push towards unipolarity.")])]),e._v(" "),t("li",[t("p",[e._v("Regulatory and Economic Dynamics: Regulatory frameworks and economic incentives also play a crucial role. Policies that encourage diversity in AI development and mitigate against the accumulation of too much power in any single entity's hands could foster a multipolar takeoff.")])])]),e._v(" "),t("h2",{attrs:{id:"takeoff-arguments"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#takeoff-arguments"}},[e._v("#")]),e._v(" Takeoff Arguments")]),e._v(" "),t("p",[t("strong",[e._v("The Overhang Argument")]),e._v(". There might be situations where there are substantial advancements or availability in one aspect of the AI system, such as hardware or data, but the corresponding software or algorithms to fully utilize these resources haven't been developed yet. The term 'overhang' is used because these situations imply a kind of 'stored‚Äô or ‚Äòlatent‚Äô potential. Once the software or algorithms catch up to the hardware or data, there could be a sudden unleashing of this potential, leading to a rapid leap in AI capabilities. Overhangs provide one possible argument for why we might favor discontinuous or fast takeoffs. There are two types of overhangs commonly discussed:")]),e._v(" "),t("ul",[t("li",[t("p",[t("strong",[e._v("Hardware Overhang")]),e._v(": This refers to a situation where there is enough computing hardware to run many powerful AI systems, but the software to run such systems hasn't been developed yet. If such hardware could be repurposed for AI, this would mean that as soon as one powerful AI system exists, probably a large number of them would exist, which might amplify the impact of the arrival of human-level AI.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Data Overhang")]),e._v(": This would be a situation where there is an abundance of data available that could be used for training AI systems, but the AI algorithms capable of utilizing all that data effectively haven't been developed or deployed yet.")])])]),e._v(" "),t("p",[e._v("Overhangs are also used as a counter argument to why AI pauses do not affect takeoff. One counter argument to the overhang argument is that it relies on the assumption that during the time that we are pausing AI development, the rate of production of chips will remain constant. It could be argued that the companies manufacturing these chips will not make as many chips if data centers aren't buying them. However, this argument only works if the pause is for any appreciable length of time, otherwise the data centers might just stockpile the chips. It is also possible to make progress on improved chip design, without having to manufacture as many during the pause period. However, during the same pause period we could also make progress on AI Safety techniques. ("),t("a",{attrs:{href:"https://www.youtube.com/watch?v=Q3eRy4t2oPQ",target:"_blank",rel:"noopener noreferrer"}},[e._v("Elmore, 2024"),t("OutboundLink")],1),e._v(")")]),e._v(" "),t("p",[t("strong",[e._v("The Economic Growth Argument")]),e._v(". Historical patterns of economic growth, driven by human population increases, suggest a potential for slow and continuous AI takeoff. This argument says that as AIs augment the effective economic population, we might witness a gradual increase in economic growth, mirroring past expansions but at a potentially accelerated rate due to AI-enabled automation. Limitations in AI's ability to automate certain tasks, alongside societal and regulatory constraints (e.g. that medical or legal services can only be rendered by humans), could lead to a slower expansion of AI capabilities. Alternatively, growth might far exceed historical rates. Using a similar argument for a fast takeoff hinges on AI's potential to quickly automate human labor on a massive scale, leading to unprecedented economic acceleration.")]),e._v(" "),t("p",[t("strong",[e._v("Compute Centric Takeoff Argument")]),e._v(". This argument, similar to the Bio Anchors report, assumes that compute will be sufficient for transformative AI. Based on this assumption, Tom Davidson's 2023 report on compute-centric AI takeoff discusses feedback loops that may contribute to takeoff dynamics.")]),e._v(" "),t("ul",[t("li",[t("p",[t("strong",[e._v("Investment feedback loop")]),e._v(": There might be increasing investment in AI, as AIs play a larger and larger role in the economy. This increases the amount of compute available to train models, as well as potentially leading to the discovery of novel algorithms. All of this increases capabilities, which drives economic progress, and further incentivizes investment.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Automation feedback loop")]),e._v(": As AIs get more capable, they will be able to automate larger parts of the work of coming up with better AI algorithms, or helping in the design of better GPUs. Both of these will increase the capability of the AIs, which in turn allow them to automate more labor.")])])]),e._v(" "),t("p",[e._v("Depending on the strength and interplay of these feedback loops, they can create a self-fulfilling prophecy leading to either an accelerating fast takeoff if regulations don't curtail various aspects of such loops, or a slow takeoff if the loops are weaker or counterbalanced by other factors. The entire model is shown in the diagram below:")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/AbI_Image_28.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: A summary of What a Compute-Centric Framework Says About Takeoff Speeds ("),t("strong",[t("a",{attrs:{href:"https://www.openphilanthropy.org/research/what-a-compute-centric-framework-says-about-takeoff-speeds/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Davidson, 2024"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[e._v("**Automating Research Argument. **Researchers could potentially design the next generation of ML models more quickly by delegating some work to existing models, creating a feedback loop of ever-accelerating progress. The following argument is put forth by Ajeya Cotra:")]),e._v(" "),t("p",[e._v("Currently, human researchers collectively are responsible for almost all of the progress in AI research, but are starting to delegate a small fraction of the work to large language models. This makes it somewhat easier to design and train the next generation of models.")]),e._v(" "),t("p",[e._v("The next generation is able to handle harder tasks and more different types of tasks, so human researchers delegate more of their work to them. This makes it significantly easier to train the generation after that. Using models gives a much bigger boost than it did the last time around.")]),e._v(" "),t("p",[e._v("Each round of this process makes the whole field move faster and faster. In each round, human researchers delegate everything they can productively delegate to the current generation of models ‚Äî and the more powerful those models are, the more they contribute to research and thus the faster AI capabilities can improve. ("),t("a",{attrs:{href:"https://www.planned-obsolescence.org/ais-accelerating-ai-research/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Cotra, 2023"),t("OutboundLink")],1),e._v(")")]),e._v(" "),t("p",[e._v("So before we see a recursive explosion of intelligence, we see a steadily increasing amount of the full RnD process being delegated to AIs. At some point, instead of a significant majority of the research and design being done by AI assistants at superhuman speeds, it will become that - all of the research and design for AIs is done by AI assistants at superhuman speeds.")]),e._v(" "),t("p",[e._v("At this point there is a possibility that this might eventually lead to a full automated recursive intelligence explosion.")]),e._v(" "),t("p",[t("strong",[e._v("The Intelligence Explosion Argument")]),e._v(". This concept of the 'intelligence explosion' is also central to the conversation around discontinuous takeoff. It originates from I.J. Good's thesis, which posits that sufficiently advanced machine intelligence could build a smarter version of itself. This smarter version could in turn build an even smarter version of itself, and so on, creating a cycle that could lead to intelligence vastly exceeding human capability ("),t("a",{attrs:{href:"https://intelligence.org/files/IEM.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Yudkowsky, 2013"),t("OutboundLink")],1),e._v(").")]),e._v(" "),t("p",[e._v("In their 2012 report on the evidence for Intelligence Explosions, Muehlhauser and Salamon delve into the numerous advantages that machine intelligence holds over human intelligence, which facilitate rapid intelligence augmentation. ("),t("a",{attrs:{href:"https://intelligence.org/files/IE-EI.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Muehlhauser, 2012"),t("OutboundLink")],1),e._v(") These include:")]),e._v(" "),t("ul",[t("li",[t("p",[t("strong",[e._v("Computational Resources")]),e._v(": Human computational ability remains somewhat stationary, whereas machine computation possesses scalability.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Speed")]),e._v(': Humans communicate at a rate of two words per second, while GPT-4 can process 32k words in an instant. Once LLMs can write "better" than humans, their speed will most probably surpass us entirely.')])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Duplicability")]),e._v(": Machines exhibit effortless duplicability. Unlike humans, they do not need birth, education, or training. While humans predominantly improve individually, machines have the potential to grow collectively. Humans take 20 years to become competent from birth, whereas once we have one capable AI, we can duplicate it immediately. Once AIs reach the level of the best programmer, we can just duplicate this AI. The same goes for other jobs.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Editability")]),e._v(": Machines potentially allow more regulated variations. They exemplify the equivalent of direct brain enhancements via neurosurgery in opposition to laborious education or training requirements. Humans can also improve and learn new skills, but they don't have root access to their hardware: we are just starting to be able to understand the genome's \"spaghetti code,\" while AI could use code versioning tools to improve itself, being able to attempt risky experiments with backup options in case of failure. This allows for much more controlled variation.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Goal coordination")]),e._v(": Copied AIs possess the capability to share goals effortlessly, a feat challenging for humans.")])])]),e._v(" "),t("h1",{attrs:{id:"appendix-1-expert-opinions"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#appendix-1-expert-opinions"}},[e._v("#")]),e._v(" Appendix 1: Expert Opinions")]),e._v(" "),t("p",[e._v('???+ note "Expert Opinions - Video Introduction"')]),e._v(" "),t("tab",[t("iframe",{staticStyle:{width:"100%","aspect-ratio":"16 / 9"},attrs:{frameborder:"0",allowfullscreen:"",src:"https://www.youtube.com/embed/NqmUBZQhOYw"}}),e._v(" "),t("p",[e._v('!!! warning "This video is optional and not necessary to understand the text."')])]),e._v(" "),t("h2",{attrs:{id:"surveys"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#surveys"}},[e._v("#")]),e._v(" Surveys")]),e._v(" "),t("p",[e._v("According to a recent survey conducted by AI Impact ("),t("a",{attrs:{href:"https://aiimpacts.org/wp-content/uploads/2023/04/Thousands_of_AI_authors_on_the_future_of_AI.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("AI Impacts, 2022"),t("OutboundLink")],1),e._v("): "),t("em",[e._v("‚Äú"),t("strong",[t("strong",[e._v("Expected time to human-level performance dropped 1‚Äì5 decades since the 2022 survey")])]),e._v(". As always, our questions about ‚Äòhigh-level machine intelligence‚Äô (HLMI) and ‚Äòfull automation of labor‚Äô (FAOL) got very different answers, and individuals disagreed a lot (shown as thin lines below), but the aggregate forecasts for both sets of questions dropped sharply. For context, between 2016 and 2022 surveys, the forecast for HLMI had only shifted about a year.‚Äù")])]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/01G_Image_29.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: 2024 Survey of AI Experts ("),t("strong",[t("a",{attrs:{href:"https://blog.aiimpacts.org/p/2023-ai-survey-of-2778-six-things",target:"_blank",rel:"noopener noreferrer"}},[e._v("AI Impacts, 2022"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[e._v('It is also possible to compare the predictions of the same study in 2022 to the current results. It is interesting to note that the community has generally underestimated the speed of progress over the year 2023 and has adjusted its predictions downward. Some predictions are quite surprising. For example, tasks like "Write High School Essay" and "Transcribe Speech" are arguably already automated with ChatGPT and Whisper, respectively. However, it appears that researchers are not aware of these results. Additionally, it is surprising that the forecast for when we are able to build an ‚ÄúAI researcher‚Äù has longer timelines than when we are able to build ‚ÄúHigh-level machine intelligence (all human tasks)‚Äù.')]),e._v(" "),t("p",[e._v("The median of the 2024 expert survey predicts human-level machine intelligence (HLMI) in 2049.")]),e._v(" "),t("h2",{attrs:{id:"expert-quotes"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#expert-quotes"}},[e._v("#")]),e._v(" Expert Quotes")]),e._v(" "),t("p",[e._v("Here are some quotes from experts regarding transformative AI:")]),e._v(" "),t("p",[e._v('!!! quote "Geoffrey Hinton"')]),e._v(" "),t("tab",[t("p",[e._v('"Until quite recently, I thought it was going to be like 20 to 50 years before we have general purpose AI," Hinton said. "And now I think it may be 20 years or less." ('),t("a",{attrs:{href:"https://www.cbsnews.com/news/godfather-of-artificial-intelligence-weighs-in-on-the-past-and-potential-of-artificial-intelligence/",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(")")])]),e._v(" "),t("p",[e._v('!!! quote "Yoshua Bengio"')]),e._v(" "),t("tab",[t("p",[e._v('Leading expert in AI, Yoshua Bengio: "...it started to dawn on me that my previous estimates of when human-level AI would be reached needed to be radically changed. Instead of decades to centuries, I now see it as 5 to 20 years with 90%." ('),t("a",{attrs:{href:"https://yoshuabengio.org/2023/08/12/personal-and-psychological-dimensions-of-ai-researchers-confronting-ai-catastrophic-risks/",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(")")])]),e._v(" "),t("p",[e._v('!!! quote "Yann LeCun"')]),e._v(" "),t("tab",[t("p",[e._v('‚ÄúBy "not any time soon", I mean "clearly not in the next 5 years", contrary to a number of folks in the AI industry.‚Äù ('),t("a",{attrs:{href:"https://www.cnbc.com/2023/12/03/meta-ai-chief-yann-lecun-skeptical-about-agi-quantum-computing.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(")")])]),e._v(" "),t("p",[e._v('!!! quote "Ilya Sutskever"')]),e._v(" "),t("tab",[t("p",[e._v("\"You're gonna see dramatically more intelligent systems in 10 or 15 years from now, and I think it's highly likely that those systems will have a completely astronomical impact on society\" ("),t("a",{attrs:{href:"https://old.reddit.com/r/singularity/comments/kxgg1b/openais_chief_scientist_ilya_sutskever_comments/",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(")")])]),e._v(" "),t("p",[e._v('!!! quote "Demis Hassabis"')]),e._v(" "),t("tab",[t("p",[e._v("‚ÄúWe could only be a few years, maybe a decade away‚Äù ("),t("a",{attrs:{href:"https://www.wsj.com/video/events/the-race-for-true-ai-at-google/7953FE4B-AE84-4AFA-9722-AA215EB357EE.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(")")])]),e._v(" "),t("p",[e._v("Note that Hinton, Bengio, and Sutskever are the 3 most cited researchers in the field of AI. And that Hinton, Bengio, and LeCun are the recipients of the Turing Award in Deep Learning. Some users on reddit have put together a comprehensive list of publicly stated AI timelines forecasts from famous researchers and industry leaders.")]),e._v(" "),t("h2",{attrs:{id:"prediction-markets"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#prediction-markets"}},[e._v("#")]),e._v(" Prediction Markets")]),e._v(" "),t("p",[e._v("Prediction markets are like betting systems where people can buy and sell shares based on their predictions of future events. For instance, if there‚Äôs a prediction market for a presidential election, you can buy shares for the candidate you think will win. If many people believe Candidate A will win, the price of shares for Candidate A goes up, indicating a higher probability of winning.")]),e._v(" "),t("p",[e._v("These markets are helpful because they gather the knowledge and opinions of many people, often leading to accurate predictions. For example, a company might use a prediction market to forecast whether a new product will succeed. Employees can buy shares if they believe the product will do well. If the majority think it will succeed, the share price goes up, giving the company a good indication of the product‚Äôs potential success.")]),e._v(" "),t("p",[e._v("By allowing participants to profit from accurate predictions, these markets encourage the sharing of valuable information and provide real-time updates on the likelihood of various outcomes. The argument is that either prediction markets are more accurate than experts, or experts should be able to make a lot of money from these markets and, in doing so, correct the markets. So the incentive for profit leads to the most accurate predictions. Examples of prediction markets include "),t("a",{attrs:{href:"https://manifold.markets/home",target:"_blank",rel:"noopener noreferrer"}},[e._v("manifold"),t("OutboundLink")],1),e._v(", or metaculus.")]),e._v(" "),t("p",[e._v("When using prediction markets to estimate the reproducibility of scientific research it was found that  they outperformed expert surveys ("),t("a",{attrs:{href:"https://www.pnas.org/doi/10.1073/pnas.1516179112",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v("). So if a lot of experts participate, prediction markets might be one of our best probabilistic forecasting tools, better even than surveys or experts.")]),e._v(" "),t("p",[e._v("The live charts below show the results of the prediction markets from Metaculus for - ‚ÄúWhen will the first weakly general AI system be devised, tested, and publicly announced?‚Äù At the time of writing, weakly general systems are expected in 2027, and general systems in 2032.")]),e._v(" "),t("iframe",{staticStyle:{width:"100%","aspect-ratio":"16 / 9"},attrs:{src:"https://www.metaculus.com/questions/question_embed/3479/?theme=light",frameborder:"0"}}),e._v(" "),t("iframe",{staticStyle:{width:"100%","aspect-ratio":"16 / 9"},attrs:{src:"https://www.metaculus.com/questions/question_embed/5121/?theme=light",frameborder:"0"}}),e._v(" "),t("h1",{attrs:{id:"appendix-2-discussions-on-llms"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#appendix-2-discussions-on-llms"}},[e._v("#")]),e._v(" Appendix 2: Discussions on LLMs")]),e._v(" "),t("p",[e._v("Current LLMs, although trained on abundant data, are still far from perfect.")]),e._v(" "),t("p",[e._v("Will these problems persist in future iterations, or will they disappear? This section examines the main criticisms of those models and tries to determine if they are valid even for future LLMs.")]),e._v(" "),t("p",[e._v("This kind of qualitative assessment is important to know whether LLMs represent the most likely route to AGI or not.")]),e._v(" "),t("h2",{attrs:{id:"empirically-insufficiency"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#empirically-insufficiency"}},[e._v("#")]),e._v(" Empirically insufficiency?")]),e._v(" "),t("p",[t("strong",[e._v("Can LLMs be "),t("strong",[t("strong",[e._v("c")])]),e._v("reative")]),e._v("**? **The creativity of LLMs is often debated, but there are clear indications that AI, in principle, is capable of creative processes in various ways:")]),e._v(" "),t("ul",[t("li",[t("p",[t("strong",[e._v("Autonomous Scientific Research")]),e._v(': Recent advancements have shown that LLMs can indeed make novel discoveries. For instance, a study by DeepMind demonstrated that an LLM "'),t("em",[e._v("discovered new solutions for the cap set problem, a long-standing open problem in mathematics")]),e._v('" ('),t("a",{attrs:{href:"https://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(") which was a favorite open problem of Terence Tao. This indicates that AI can not only understand existing knowledge but also contribute new insights in complex fields like mathematics.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Autonomous Discovery")]),e._v(": AI has the capability to rediscover human strategies and openings independently. AlphaGo, for example, rediscovered human Go strategies and openings through self-play ("),t("a",{attrs:{href:"https://arxiv.org/pdf/2111.09259.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v("), without any human data input. This demonstrates an AI's ability to independently learn and innovate within established domains.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Creative Optimization")]),e._v(": AI can optimize in surprisingly creative ways. The phenomena of specification gaming, where AI finds unintended solutions to problems, illustrate this. Although this unpredictability poses its challenges, it also shows that AI systems can come up with novel, creative solutions that might not be immediately obvious or intuitive to human problem solvers. DeepMind's blog post on Specification Gaming illustrates this point vividly. ("),t("a",{attrs:{href:"https://deepmind.google/discover/blog/specification-gaming-the-flip-side-of-ai-ingenuity/",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(")")])])]),e._v(" "),t("p",[t("strong",[e._v("Aren‚Äôt LLMs just too slow at learning things?")]),e._v(" Arguments against transformer based language models often state that they are too sample inefficient, and that LLMs are extremely slow to learn new concepts when compared to humans. To increase performance in new tasks or situations, it‚Äôs often argued that LLMs require training on vast amounts of data ‚Äî millions of times more than a human would need. However, there's a growing trend towards data efficiency, and an increasing belief that this can be significantly improved in future models.")]),e._v(" "),t("p",[e._v("EfficientZero is a reinforcement learning agent that surpasses median human performance on a set of 26 Atari games after just two hours of real-time experience per game. ("),t("a",{attrs:{href:"https://github.com/YeWR/EfficientZero",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(") This is a considerable improvement over previous algorithms, showcasing the potential leaps in data efficiency. The promise here is not just more efficient learning but also the potential for rapid adaptation and proficiency in new tasks, akin to a child's learning speed. EfficientZero is not an LLM, but it shows that deep learning can sometimes be made efficient.")]),e._v(" "),t("p",[e._v('Scaling laws indicate that larger AIs tend to be more data efficient, requiring less data to reach the same level of performance as their smaller counterparts. Papers  such as "Language Models are Few-Shot Learners" ('),t("a",{attrs:{href:"https://arxiv.org/abs/2005.14165",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(") and the evidence that larger models seem to take less data to reach the same level of performance ("),t("a",{attrs:{href:"https://arxiv.org/abs/2001.08361",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v("), suggest that as models scale, they become more proficient with fewer examples. This trend points towards a future where AI might be able to rapidly adapt and learn from limited data, challenging the notion that AIs are inherently slow learners compared to humans.")]),e._v(" "),t("p",[e._v("**Are LLMs robust to distributional shifts? **While it is true that AI has not yet achieved maximal robustness, for example being able to perform perfectly after a change in distribution, there has been considerable progress:")]),e._v(" "),t("ul",[t("li",[t("p",[t("strong",[e._v("Robustness correlates with capabilities")]),e._v(": Robustness is closely linked to the capabilities of AI models when AIs are trained on difficult tasks. For instance, there is a significant improvement in robustness and transfer learning from GPT-2 to GPT-4. In computer vision, recent models like Segment Anything ("),t("a",{attrs:{href:"https://arxiv.org/abs/2304.02643",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(") are far more robust and capable of transfer learning than their less capable predecessors. This progression isn't due to any mysterious factors but rather a result of scaling and improving upon existing architectures.")])]),e._v(" "),t("li",[t("p",[e._v("**Robustness is a continuum, and perfect robustness may be not necessary: **Robustness in AI should not be viewed as a binary concept, but rather as existing on a continuum. This continuum is evident in the way AI models, like those in image classification, often surpass human performance in both capability and robustness ("),t("a",{attrs:{href:"https://aiimpacts.org/time-for-ai-to-cross-the-human-performance-range-in-imagenet-image-classification/",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v("). However, it's important to recognize that no system is completely immune to challenges such as adversarial attacks. This is exemplified by advanced AIs like Katago in Go, which, despite being vulnerable to such attacks, still achieves a superhuman level of play. However, the quest for perfect robustness may not be essential to create capable transformative AI, as even systems with certain vulnerabilities can achieve superhuman levels of competence. However, while robustness may not be necessary to create capable AI, the creation of safe, aligned AI will have to solve the problem of misgeneralizing goals.")])])]),e._v(" "),t("h2",{attrs:{id:"shallow-understanding"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#shallow-understanding"}},[e._v("#")]),e._v(" Shallow Understanding?")]),e._v(" "),t("p",[t("strong",[e._v("Stochastic Parrots: Do AIs only memorize information without truly compressing it?")])]),e._v(" "),t("p",[e._v('Fran√ßois Chollet, a prominent artificial intelligence researcher currently working at Google said: *‚ÄúUnfortunately, too few people understand the distinction between memorization and understanding. It\'s not some lofty question like "does the system have an internal world model?", it\'s a very pragmatic behavior distinction: "is the system capable of broad generalization, or is it limited to local generalization?‚Äù. *Fran√ßois Chollet then listed papers aiming to show that LLMs do not really understand. ('),t("a",{attrs:{href:"https://x.com/fchollet/status/1736079054313574578?s=20",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(") There is a small informal commentary on this list "),t("a",{attrs:{href:"https://x.com/CRSegerie/status/1736466297175560268?s=20",target:"_blank",rel:"noopener noreferrer"}},[e._v("at this link"),t("OutboundLink")],1),e._v(".")]),e._v(" "),t("p",[e._v('There are two archetypal ways to represent information in an LLM: either memorize point by point, like a look-up table, or compress the information by only memorizing higher-level features, which we can then call ‚Äúthe world model‚Äù. This is explained in the very important paper "Superposition, Memorization, and Double Descent" ('),t("a",{attrs:{href:"https://transformer-circuits.pub/2023/toy-double-descent/index.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v("): it turns out that to store points, initially the model learns the position of all the points (pure memorization), then, if we increase the number of points, the model starts to compress this knowledge, and the model is now capable of generalization (and implements a simple model of the data).")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/XcP_Image_30.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: From Superposition, Memorization, and Double Descent ("),t("strong",[t("a",{attrs:{href:"https://transformer-circuits.pub/2023/toy-double-descent/index.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[e._v("AI is capable of compressing information, often in a relevant manner. For example, when examining the representations of words representing colors in LLMs like ‚Äúred‚Äù and ‚Äúblue‚Äù, the structure formed by all the embeddings of those colors creates the correct color circle (This uses a nonlinear projection such as a T-distributed stochastic neighbor embedding (T-SNE) to project from high-dimensional space to the 2D plane). Other examples of world models are presented in a paper called ‚ÄúEight Things to Know about Large Language Models‚Äù ("),t("a",{attrs:{href:"https://arxiv.org/abs/2304.00612",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(").")]),e._v(" "),t("p",[e._v('Of course, there are other domains where AI resembles more of a look-up table, but it is a spectrum, and each case should be examined individually. For instance, for "factual association," the paper ‚ÄúLocating and Editing Factual Associations in GPT‚Äù shows that the underlying data structure for GPT-2 is more of a look-up table ('),t("a",{attrs:{href:"https://arxiv.org/abs/2202.05262",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v("), but the paper ‚ÄúEmergent Linear Representations in World Models of Self-Supervised Sequence Models‚Äù demonstrates that a small GPT is capable of learning a compressed world model of OthelloGpt. ("),t("a",{attrs:{href:"https://arxiv.org/abs/2309.00941",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(") There are more examples in the section dedicated to world models in the paper ‚ÄúEight Things to Know about Large Language Models‚Äù ("),t("a",{attrs:{href:"https://arxiv.org/abs/2304.00612",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(").")]),e._v(" "),t("p",[e._v('It‚Äôs clear that LLMs are compressing their representations at least a bit. Many examples of impressive capabilities are presented in the work "The Stochastic Parrot Hypothesis is debatable for the last generation of LLMs", which shows that it cannot be purely a memorization. ('),t("a",{attrs:{href:"https://www.lesswrong.com/posts/HxRjHq3QG8vcYy4yy/the-stochastic-parrot-hypothesis-is-debatable-for-the-last",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(")")]),e._v(" "),t("p",[t("strong",[e._v("Will LLMs Inevitably Hallucinate?")])]),e._v(" "),t("p",[e._v('LLMs are prone to "hallucinate," a term used to describe the generation of content that is nonsensical or factually incorrect in response to certain prompts. This issue, highlighted in studies such as "On Faithfulness and Factuality in Abstractive Summarization" by Maynez et al. ('),t("a",{attrs:{href:"https://arxiv.org/abs/2005.00661",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(') and "TruthfulQA: Measuring How Models Mimic Human Falsehoods" by Lin et al. ('),t("a",{attrs:{href:"https://arxiv.org/abs/2109.07958",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v("), poses a significant challenge. However, it's important to see that these challenges are anticipated due to the training setup and can be mitigated:")]),e._v(" "),t("ul",[t("li",[t("p",[t("strong",[e._v("Inherent Bias in Source Texts")]),e._v(": One of the fundamental reasons LLMs may produce untrue content is training data, which may not always be entirely factual or unbiased. In essence, LLMs are reflecting the diverse and sometimes contradictory nature of their training data. In this context, LLMs are constantly 'hallucinating', but occasionally, these hallucinations align with our perception of reality.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Strategies to Enhance Factual Accuracy")]),e._v(": The tendency of LLMs to generate hallucinations can be significantly diminished using various techniques. See the box below for a breakdown of those.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Larger models can be more truthful than smaller ones.")]),e._v(" This is the case with TruthfulQA. OpenAI reports that GPT-4 is 40% more accurate and factually consistent than its predecessor.")])])]),e._v(" "),t("p",[e._v('??? note "Many techniques can be used to increase the truthfulness of LLMs"')]),e._v(" "),t("tab",[t("ul",[t("li",[t("p",[t("strong",[e._v("Fine-tuning LLMs for Factuality")]),e._v("**üòó* In this paper ("),t("a",{attrs:{href:"https://arxiv.org/abs/2311.08401",target:"_blank",rel:"noopener noreferrer"}},[e._v("link"),t("OutboundLink")],1),e._v("), the authors recommend fine-tuning methods using Direct Preference Optimization (DPO) to decrease the rate of hallucinations. By applying such techniques, a 7B Llama 2 model saw a 58% reduction in factual error rate compared to its original model.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Retrieval Augmented Generation (RAG)")]),e._v(". This method works by incorporating a process of looking up real-world information (retrieval, like a Google search) and then using that information to guide the AI's responses (generation, based on the document retrieved). By doing so, the AI is better anchored in factual reality, reducing the chances of producing unrealistic or incorrect content. Essentially, it's like giving the AI a reference library to check facts against while it learns and responds, ensuring its output is more grounded in reality. This approach is particularly useful in the context of in-context learning, where the AI learns from the information and context provided in each interaction.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Prompting techniques")]),e._v(" in AI have evolved to include sophisticated methods like")])])]),e._v(" "),t("tab",[t("ul",[t("li",[t("p",[t("strong",[e._v("Consistency checks")]),e._v(" ("),t("a",{attrs:{href:"https://arxiv.org/abs/2306.09983",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v("), that involve comparing the output from multiple instances of the model on the same prompt, identifying and resolving any disagreements in the responses. This method enhances the accuracy and credibility of the information provided. For instance, if different iterations of the model produce conflicting answers, this discrepancy can be used to refine and improve the model's understanding.")])]),e._v(" "),t("li",[t("p",[e._v('**Reflexion. **The Reflexion technique ("Reflexion: Language Agents with Verbal Reinforcement Learning"): It‚Äôs possible to simply ask the LLM to take a step back, to question whether what it has done is correct or not, and to consider ways to improve the previous answer, and this enhances a lot the capabilities of GPT-4, and this technique is emergent and does not work well with previous models. ('),t("a",{attrs:{href:"https://arxiv.org/abs/2303.11366",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(").")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Verification chains")]),e._v(", like "),t("strong",[e._v("selection inference")]),e._v(" ("),t("a",{attrs:{href:"https://arxiv.org/abs/2205.09712",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v("). Chain-of-Thought has access to the whole context, so each reasoning step is not necessarily causally connected to the last. But selection inference enforces a structure where each reasoning step necessarily follows from the last, and therefore the whole reasoning chain is causal. This process involves the AI model examining its own reasoning or the steps it took to arrive at a conclusion. By doing so, it can verify the logic and consistency of its responses, ensuring they are well-founded and trustworthy.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Allowing the AI to express degrees of confidence")]),e._v(' in its answers, acknowledging uncertainty when appropriate. For instance, instead of a definitive "Yes" or "No," the model might respond with "I am not sure," reflecting a more nuanced understanding akin to human reasoning. This approach is evident in advanced models like Gopher ('),t("a",{attrs:{href:"https://arxiv.org/pdf/2112.11446.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v("), which contrasts with earlier models such as WebGPT which may not exhibit the same level of nuanced responses.")])])])]),e._v(" "),t("ul",[t("li",[t("p",[t("strong",[e._v("Process-based training")]),e._v(" ensures that the systems are accustomed to detailing their thoughts in much greater detail and not being able to skip too many reasoning steps. For example, see OpenAI‚Äôs Improving Mathematical Reasoning with process supervision ("),t("a",{attrs:{href:"https://openai.com/research/improving-mathematical-reasoning-with-process-supervision",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(").")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Training for metacognition")]),e._v(': Models can be trained to give the probability of what they assert, a form of metacognition. For instance, the paper "Language Models (Mostly) Know What They Know" ('),t("a",{attrs:{href:"https://arxiv.org/abs/2207.05221",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(') demonstrates that AIs can be Bayesian calibrated about their knowledge. This implies that they can have a rudimentary form of self-awareness, recognizing the likelihood of their own accuracy. Informally, this means it is possible to query a chatbot with "Are you sure about what you are telling me?" and receive a relatively reliable response. This can serve as training against hallucinations.')])])]),e._v(" "),t("p",[e._v("It's worth noting that these techniques enable substantial problem mitigation for the current LLMs, but they don‚Äôt solve all the problems that we encounter with AI that are potentially deceptive, as we will see in the chapter on goal misgeneralization.")])],1),e._v(" "),t("h2",{attrs:{id:"structural-inadequacy"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#structural-inadequacy"}},[e._v("#")]),e._v(" Structural inadequacy?")]),e._v(" "),t("p",[e._v("**Are LLMs m"),t("strong",[t("strong",[e._v("issing System 2?")])]),e._v(" **System 1 and System 2 are terms popularized by economist Daniel Kahneman in his book \"Thinking, Fast and Slow,\" describing the two different ways our brains form thoughts and make decisions. System 1 is fast, automatic, and intuitive; it's the part of our thinking that handles everyday decisions and judgments without much effort or conscious deliberation. For instance, when you recognize a face or understand simple sentences, you're typically using System 1. On the other hand, System 2 is slower, more deliberative, and more logical. It takes over when you're solving a complex problem, making a conscious choice, or focusing on a difficult task. It requires more energy and is more controlled, handling tasks such as planning for the future, checking the validity of a complex argument, or any activity that requires deep focus. Together, these systems interact and influence how we think, make judgments, and decide, highlighting the complexity of human thought and behavior.")]),e._v(" "),t("p",[e._v("A key concern is whether LLMs are able to emulate System 2 processes, which involve slower, more deliberate, and logical thinking. Some theoretical arguments about the depth limit in transformers show that they are provably incapable of internally dividing large integers ("),t("a",{attrs:{href:"https://arxiv.org/abs/2207.02098",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v("). However, this is not what we observe in practice: GPT-4 is capable of detailing some calculations step-by-step and obtaining the expected result through a chain of thought or via the usage of tools like a code interpreter.")]),e._v(" "),t("p",[t("strong",[e._v("Emerging Metacognition")]),e._v(". Emerging functions in LLMs, like the Reflexion technique ("),t("a",{attrs:{href:"https://arxiv.org/abs/2303.11366",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v("), allow these models to retrospectively analyze and improve their answers. It is possible to ask the LLM to take a step back, question the correctness of its previous actions, and consider ways to improve the previous answer. This greatly enhances the capabilities of GPT-4, enhancing its capabilities and aligning them more closely with human System 2 operations. Note that this technique is emergent and does not work well with previous models.")]),e._v(" "),t("p",[e._v("These results suggest a blurring of the lines between these two systems. System 2 processes may be essentially an assembly of multiple System 1 processes, appearing slower due to involving more steps and interactions with slower forms of memory. This perspective is paralleled in how language models operate, with each step in a System 1 process akin to a constant time execution step in models like GPT. Although these models struggle with intentionally orchestrating these steps to solve complex problems, breaking down tasks into smaller steps (Least-to-most prompting) or prompting them for incremental reasoning (Chain-of-Thought (CoT) prompting) significantly improves their performance.")]),e._v(" "),t("p",[t("strong",[e._v("Are LLMs m****issing an internal "),t("strong",[t("strong",[e._v("w")])]),e._v("orld "),t("strong",[t("strong",[e._v("m")])]),e._v("odel?")])]),e._v(" "),t("p",[e._v('The notion of a "world model" in AI need not be confined to explicit encoding within an architecture. Contrary to approaches like H-JEPA ('),t("a",{attrs:{href:"https://openreview.net/pdf?id=BZ5a1r-kVsf",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v("), which advocate for an explicit world model to enhance AI training, there's growing evidence that a world model can be effectively implicit. This concept is particularly evident in reinforcement learning (RL), where the distinction between model-based and model-free RL can be somewhat misleading. Even in model-free RL, algorithms often implicitly encode a form of a world model that is crucial for optimal performance.")]),e._v(" "),t("ul",[t("li",[t("p",[t("strong",[e._v("Time and geographical coordinates:")]),e._v(" Research on Llama-2 models reveals how these models can represent spatial and temporal information ("),t("a",{attrs:{href:"https://arxiv.org/abs/2310.02207",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v("). LLMs like Llama-2 models encode approximate real-world coordinates and historical timelines of cities. Key findings include the gradual emergence of geographical representations across model layers, the linearity of these representations, and the models' robustness to different prompts. Significantly, the study shows that the models are not just passively processing this information but actively learning the global geometry of space and time.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Board representation:")]),e._v(" In the paper ‚ÄúEmergent Linear Representations in World Models of Self-Supervised Sequence Models‚Äù ("),t("a",{attrs:{href:"https://arxiv.org/abs/2309.00941",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v('), the author presents significant findings on the nature of representations in AI models. The paper delves into how the Othello-GPT model, trained to predict legal moves in the game of Othello, develops an emergent world representation of the game board! Contrary to previous beliefs that this representation was non-linear, he demonstrates that it is, in fact, linear. He discovers that the model represents board states not in terms of black or white pieces, but as "my color" or "their color," aligning with the model\'s perspective of playing both sides. This work sheds light on the potential of AI models to develop complex, yet linear, world representations through simple objectives like next-token prediction.')])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Other examples")]),e._v(" are presented in the paper: ‚ÄúEight Things to know about LLMs‚Äù.  ("),t("a",{attrs:{href:"https://arxiv.org/abs/2304.00612",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(")")])])]),e._v(" "),t("p",[e._v("**Can LLMs learn continuously, and have long term memory? **Continual learning and the effective management of long-term memory represent significant challenges in the field of AI in general.")]),e._v(" "),t("p",[t("strong",[e._v("Catastrophic Forgetting")]),e._v(". A crucial obstacle in this area is catastrophic forgetting, a phenomenon where a neural network, upon learning new information, tends to entirely forget previously learned information. This issue is an important focus of ongoing research, aiming to develop AI systems that can retain and build upon their knowledge over time. For example, suppose we train an AI on an Atari game. At the end of the second training, the AI has most likely forgotten how to play the first game. This is an example of catastrophic forgetting.")]),e._v(" "),t("p",[e._v("But now suppose we train a large AI on many ATARI games, simultaneously, and even add some Internet text and some robotic tasks. This can just work. For example, the AI GATO is an example of such a training process and exemplifies what we call the "),t("strong",[e._v("blessing of scale")]),e._v(", which is that what is impossible in small regimes can become possible in large regimes.")]),e._v(" "),t("p",[e._v("Other techniques are being developed to solve long-term memory, for example, "),t("strong",[e._v("Scaffolding-based approaches")]),e._v(" have also been employed for achieving long-term memory and continual learning in AI. Scaffolding in AI refers to the use of hard-coded wrappers explicitly programmed structures by humans that involve a for loop to query continuously the model:")]),e._v(" "),t("ul",[t("li",[t("p",[t("strong",[e._v("LangChain")]),e._v(" addresses these challenges by creating extensive memory banks. LangChain is a Python library that allows LLM to retrieve and utilize information from large datasets, essentially providing a way for AI to access a vast repository of knowledge and use this information to construct more informed responses. However, this approach may not be the most elegant due to its reliance on external data sources and complex retrieval mechanisms. A potentially more seamless and integrated solution could involve utilizing the neural network's weights as dynamic memory, constantly evolving and updating based on the tasks performed by the network.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Voyager:")]),e._v(' A remarkable example of a scaffolding-based long-term memory is the AI Voyager, an AI system developed under the "AutoGPT" paradigm. This system is notable for its ability to engage in continuous learning within a 3D game environment like Minecraft. In a single game session, AI Voyager demonstrates the capacity to learn basic controls, achieve initial goals such as resource acquisition, and eventually advance to more complex behaviors, including combat with enemies and crafting tools for gathering sophisticated resources. This demonstrates a significant stride in LLM\'s ability to learn continually and manage long-term memory within dynamic environments.')])])]),e._v(" "),t("p",[e._v("It should be noted that scaffold-based long-term memory is not considered an elegant solution, and purists would prefer to use the system's own weights as long-term memory.")]),e._v(" "),t("p",[t("strong",[e._v("Planning")])]),e._v(" "),t("p",[e._v("Planning is an area that AIs currently struggle with, but there is significant progress. Some paradigms, such as those based on scaffolding, enable task decomposition and breaking down objectives into smaller, more achievable sub-objectives.")]),e._v(" "),t("p",[e._v("Furthermore, the paper ‚Äú"),t("em",[e._v("Voyager: An Open-Ended Embodied Agent with Large Language Models")]),e._v("‚Äù demonstrates that it is possible to use GPT-4 for planning in Natural language in Minecraft.  ("),t("a",{attrs:{href:"https://arxiv.org/abs/2305.16291",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(")")]),e._v(" "),t("h2",{attrs:{id:"differences-with-the-brain"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#differences-with-the-brain"}},[e._v("#")]),e._v(" Differences with the brain")]),e._v(" "),t("p",[e._v("It appears that there are several points of convergence between the LLMs and the linguistic cortex:")]),e._v(" "),t("ul",[t("li",[t("p",[t("strong",[e._v("Behavioral similarities.")]),e._v(" From ("),t("a",{attrs:{href:"https://www.lesswrong.com/posts/3nMpdmt8LrzxQnkGp/ai-timelines-via-cumulative-optimization-power-less-long#fn-uL4CtAHDBwDHrweh8-4",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v("), it's highlighted that LLMs show a close comparison to human linguistic abilities and the linguistic cortex. These models have excelled in mastering syntax and a significant portion of semantics in human language. Of course, today, they still lag in aspects such as long-term memory, coherence, and general reasoning - faculties that in humans depend on various brain regions like the hippocampus and prefrontal cortex, but we explained in the last sections that those problems may be solvable.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Convergence in internal Representations")]),e._v(': LLMs have a representation that converges with scale toward the brain representation. This is supported by the study, "Brains and algorithms partially converge in natural language processing." ('),t("a",{attrs:{href:"https://scholar.google.com/scholar?cluster=7281145279140743388&hl=en&as_sdt=0,5",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(') Additional insights can be found in the works "The Brain as a Universal Learning Machine" ('),t("a",{attrs:{href:"https://www.lesswrong.com/posts/9Yc7Pp7szcjPgPsjf/the-brain-as-a-universal-learning-machine",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(') and "Brain Efficiency: Much More than You Wanted to Know." ('),t("a",{attrs:{href:"https://www.lesswrong.com/posts/xwBuoE9p8GE7RAuhd/brain-efficiency-much-more-than-you-wanted-to-know",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(') At comparable learning stages, LLMs and the linguistic cortex develop similar or equivalent feature representations. In some evaluations, advanced LLMs have been able to predict 100% of the explainable neural variance, as detailed by Schrimpf, Martin, et al. in "The neural architecture of language: Integrative modeling converges on predictive processing." ('),t("a",{attrs:{href:"https://www.pnas.org/content/118/45/e2105646118",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(")")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Scale is also important in primates.")]),e._v(" The principal architectural difference between human and other primate brains seems to be the number of neurons rather than anything else, as demonstrated in various studies. ("),t("a",{attrs:{href:"https://www.pnas.org/doi/10.1073/pnas.1201895109",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(") ("),t("a",{attrs:{href:"https://onlinelibrary.wiley.com/doi/10.1002/ajpa.24712",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(") ("),t("a",{attrs:{href:"https://royalsocietypublishing.org/doi/10.1098/rspb.2020.2987",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(").")])])]),e._v(" "),t("h2",{attrs:{id:"further-reasons-to-continue-scaling-llms"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#further-reasons-to-continue-scaling-llms"}},[e._v("#")]),e._v(" Further reasons to continue scaling LLMs")]),e._v(" "),t("p",[e._v("Following are some reasons to believe that labs will continue to scale LLMs.")]),e._v(" "),t("p",[e._v("**Scaling Laws on LLM implies further qualitative improvements. **The scaling laws might not initially appear impressive. However, linking these quantitative measures can translate to a qualitative improvement in algorithm quality. An algorithm that achieves near-perfect loss, though, is one that necessarily comprehends all subtleties, and displays enormous adaptability. The fact that the scaling laws are not bending is very significant and means that we can make the model a qualitatively better reasoner.")]),e._v(" "),t("p",[t("strong",[e._v("From simple correlations to understanding.")]),e._v(" During a training run, GPTs go from basic correlations to deeper and deeper understanding. Initially, the model merely establishes connections between successive words. Gradually, it develops an understanding of grammar and semantics, creating links between sentences and subsequently between paragraphs. Eventually, GPT masters the nuances of writing style[^footnote_scaling_law].")]),e._v(" "),t("p",[e._v('[^footnote_scaling_law]: See also "The Scaling Hypothesis," to delve into this progression in a fascinating story.')]),e._v(" "),t("p",[e._v('??? question "Exercise: Scaling Laws on LLM implies further qualitative improvements."')]),e._v(" "),t("tab",[t("p",[e._v('Let\'s calculate the difference in loss, measured in bits, between two model outputs: "Janelle ate some ice cream because he likes sweet things like ice cream." and "Janelle ate some ice cream because she likes sweet things like ice cream.‚Äù The sentence contains approximately twenty tokens. If the model vacillates between "He" or "She," choosing randomly (50/50 odds), it incurs a loss of 2 bits on the pronoun token when incorrect. The loss for other tokens remains the same in both models. However, since the model is only incorrect half the time, a factor of 1/2 should be applied. This results in a difference of (1/2) * (2/20) = 1/20, or 0.05 bits. Thus, a model within 0.05 bits of the minimal theoretical loss should be capable of understanding even more nuanced concepts than the one discussed above.')])]),e._v(" "),t("p",[t("strong",[e._v("Text completion is probably an AI-complete test")]),e._v(" ("),t("a",{attrs:{href:"https://en.wikipedia.org/wiki/AI-complete",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(").")]),e._v(" "),t("p",[e._v("**Current LLMs have only as many parameters as small mammals have synapses, no wonder they are still imperfect. **Models like GPT-4, though very big compared to other models, should be noted for their relatively modest scale compared to the size of a human brain. To illustrate, the largest GPT-3 model has a similar number of parameters to the synapses of a hedgehog.  We don't really know how many parameters GPT-4 has, but if it is the same size as PALM, which has 512 B parameters, then GPT-4 has only as many parameters as a chinchilla has synapses. In contrast, the human neocortex contains about 140 trillion synapses, which is over 200 times more synapses than a chinchilla. For a more in-depth discussion on this comparison, see the related discussion "),t("a",{attrs:{href:"https://www.lesswrong.com/posts/YKfNZAmiLdepDngwi/gpt-175bee",target:"_blank",rel:"noopener noreferrer"}},[e._v("here"),t("OutboundLink")],1),e._v(". For a discussion of the number of parameters necessary to emulate a synapse, see the discussion on biological anchors.")]),e._v(" "),t("p",[t("strong",[e._v("GPT-4 is still orders of magnitude cheaper than other big science projects.")]),e._v(": Despite the high costs associated with training large models, the significant leaps in AI capabilities provided by scaling justify these costs. For example, GPT-4 is expensive compared to other ML models. It is said to cost 50M in training (source). But the Manhattan Project cost 25B, which is 500 times more without accounting for inflation, and achieving Human-level intelligence, may be more economically important than achieving the nuclear bomb.")]),e._v(" "),t("p",[e._v("Collectively, these points support the idea that AGI it is plausible that AGI can be achieved by only scaling current algorithms.")]),e._v(" "),t("h1",{attrs:{id:"appendix-3-trends"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#appendix-3-trends"}},[e._v("#")]),e._v(" Appendix 3: Trends")]),e._v(" "),t("p",[e._v("Generally, the three main components recognized as the main variables of advancement in deep learning are: computational power available, algorithmic improvements, and the availability of data. These three variables are also sometimes called the inputs to the AI production function, or the AI triad. ("),t("a",{attrs:{href:"https://cset.georgetown.edu/publication/the-ai-triad-and-what-it-means-for-national-security-strategy/",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(")")]),e._v(" "),t("p",[e._v("We can anticipate that models will continue to scale in the near future. Increased scale combined with the increasingly general-purpose nature of foundation models could potentially lead to a sustained growth in general-purpose AI capabilities.")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/yZw_Image_31.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v('*: Monitoring the trends in ML quantitatively. Epoch (2023), "Key trends and figures in Machine Learning". Published online at epochai.org. ('),t("strong",[t("a",{attrs:{href:"https://epochai.org/trends",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[e._v('=== "'),t("strong",[e._v("Compute")]),e._v('"')]),e._v(" "),t("tab",[t("p",[e._v('!!! warning "This is extra detail provided for those interested. It can be safely skipped."')]),e._v(" "),t("p",[t("a",{attrs:{href:"https://arxiv.org/abs/2202.05924",target:"_blank",rel:"noopener noreferrer"}},[e._v("[2202.05924] Compute Trends Across Three Eras of Machine Learning"),t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("The first thing to look at is the trends in the overall amount of training compute required when we train our model. Training compute grew by 1.58 times/year up until the Deep Learning revolution around 2010, after which growth rates increased to 4.2 times/year. We also find a new trend of ‚Äúlarge-scale‚Äù models that emerged in 2016, trained with 2-3 OOMs more compute than other systems in the same period.")]),e._v(" "),t("p",[e._v("Hardware advancements are paralleling these trends in training compute and data. GPUs are seeing a yearly 1.35 times increase in floating-point operations per second (FLOP/s). However, memory constraints are emerging as potential bottlenecks, with DRAM capacity and bandwidth improving at a slower rate. Investment trends reflect these technological advancements")]),e._v(" "),t("p",[e._v("In 2010, before the deep learning revolution, DeepMind co-founder Shane Legg predicted human-level AI by 2028 using compute-based estimates ("),t("a",{attrs:{href:"http://www.vetta.org/2010/12/goodbye-2010/",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v("). OpenAI co-founder Ilya Sutskever, whose AlexNet paper sparked the deep learning revolution, was also an early proponent of the idea that scaling up deep learning would be transformative.")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/fhc_Image_32.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v('*: Epoch (2023), "'),t("strong",[t("a",{attrs:{href:"https://epochai.org/trends",target:"_blank",rel:"noopener noreferrer"}},[e._v("Key trends and figures in Machine Learning"),t("OutboundLink")],1)]),e._v('"*')])]),e._v(" "),t("p",[e._v('=== "'),t("strong",[e._v("Parameters")]),e._v('"')]),e._v(" "),t("tab",[t("p",[e._v('!!! warning "This is extra detail provided for those interested. It can be safely skipped."')]),e._v(" "),t("p",[t("a",{attrs:{href:"https://arxiv.org/abs/2207.02852",target:"_blank",rel:"noopener noreferrer"}},[e._v("[2207.02852] Machine Learning Model Sizes and the Parameter Gap"),t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("In this section, let's look at the trends in model parameters. The following graph shows how even though parameter counts have always been increasing, in the new 2018+ era, we have really entered a different phase of growth. Overall, between the 1950s and 2018, models have grown at a rate of 0.1 orders of magnitude per year (OOM/year). This means that in the 68 years between 1950 and 2018 models grew by a total of 7 orders of magnitude. However, post-2018, in just the last 5 years models have increased by yet another 4 orders of magnitude (not accounting for however many parameters GPT-4 has because we don't know).")]),e._v(" "),t("p",[e._v("The following table and graph illustrate the trend change in machine learning models' parameter growth. Note the increase to half a trillion parameters with constant training data.")]),e._v(" "),t("table",[t("thead",[t("tr",[t("th",[e._v("model")]),e._v(" "),t("th",[e._v("year")]),e._v(" "),t("th",[e._v("size (#parameters)")]),e._v(" "),t("th",[e._v("data (#training tokens)")])])]),e._v(" "),t("tbody",[t("tr",[t("td",[e._v("LaMDA")]),e._v(" "),t("td",[e._v("2021")]),e._v(" "),t("td",[e._v("137 billion")]),e._v(" "),t("td",[e._v("168 billion")])]),e._v(" "),t("tr",[t("td",[e._v("GPT-3")]),e._v(" "),t("td",[e._v("2020")]),e._v(" "),t("td",[e._v("174 billion")]),e._v(" "),t("td",[e._v("300 billion")])]),e._v(" "),t("tr",[t("td",[e._v("Jurassic")]),e._v(" "),t("td",[e._v("2021")]),e._v(" "),t("td",[e._v("178 billion")]),e._v(" "),t("td",[e._v("300 billion")])]),e._v(" "),t("tr",[t("td",[e._v("Gopher")]),e._v(" "),t("td",[e._v("2021")]),e._v(" "),t("td",[e._v("280 billion")]),e._v(" "),t("td",[e._v("300 billion")])]),e._v(" "),t("tr",[t("td",[e._v("MT-NLG 530B")]),e._v(" "),t("td",[e._v("2022")]),e._v(" "),t("td",[e._v("530 billion")]),e._v(" "),t("td",[e._v("270 billion")])])])]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/iPe_Image_33.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: Villalobos, Pablo et. al. (Jul 2022) ‚Äú"),t("strong",[t("a",{attrs:{href:"https://arxiv.org/abs/2207.02852",target:"_blank",rel:"noopener noreferrer"}},[e._v("Machine Learning Model Sizes and the Parameter Gap"),t("OutboundLink")],1)]),e._v("‚Äù*")])]),e._v(" "),t("p",[e._v('=== "'),t("strong",[e._v("Data")]),e._v('"')]),e._v(" "),t("tab",[t("p",[e._v('!!! warning "This is extra detail provided for those interested. It can be safely skipped."')]),e._v(" "),t("p",[t("a",{attrs:{href:"https://arxiv.org/abs/2211.04325",target:"_blank",rel:"noopener noreferrer"}},[e._v("[2211.04325] Will we run out of data? Limits of LLM scaling based on human-generated data"),t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("We are using ever-increasing amounts of data to train our models. The paradigm of training foundation models to fine-tune later is accelerating this trend. If we want a generalist base model then we need to provide it with ‚Äògeneral data‚Äô which is code for all the data we can get our hands on. You have probably heard that models like ChatGPT and PaLM are trained on data from the internet. The internet is the biggest repository of data that humans have. Additionally, as we observed from the Chinchilla papers scaling laws, it is possible that data to train our models is the actual bottleneck, and not compute or parameter count. So the natural question is how much data is left on the internet for us to keep training our models? and how much more data do we humans generate every year?")]),e._v(" "),t("p",[t("strong",[e._v("How much data do we generate?")])]),e._v(" "),t("p",[e._v("The total amount of data generated every single day is on the order of ~463EB (Source: "),t("a",{attrs:{href:"https://www.weforum.org/agenda/2019/04/how-much-data-is-generated-each-day-cf4bddf29f/",target:"_blank",rel:"noopener noreferrer"}},[e._v("World Economic Forum"),t("OutboundLink")],1),e._v("). But in this post, we will assume that models are not training on ‚Äòall the data generated‚Äô (yet), rather they will continue to only train on open-source internet text and image data. The available stock of text and image data grew by 0.14 OOM/year between 1990 and 2018 but has since slowed to 0.03 OOM/year.")]),e._v(" "),t("p",[t("strong",[e._v("How much data is left?")])]),e._v(" "),t("p",[e._v("The median projection for when the training dataset of notable ML models exhausts the stock of professionally edited texts on the internet is 2024. The median projection for the year in which ML models use up all the text on the internet is 2040. Overall, projections by Epochai predict that we will have exhausted high-quality language data before 2026, low-quality language data somewhere between 2030 and 2050, and vision data between 2030 and 2060. This might be an indicator of slowing down ML progress after the next couple of decades. These conclusions from Epochai, like all the other conclusions in this entire leveraging computation section, rely on the unrealistic assumptions that current trends in ML data usage and production will continue and that there will be no major innovations in data efficiency, i.e. we are assuming that the amount of capabilities gained per training datapoint will not change from current standards.")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/sZd_Image_34.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v('*: ML data consumption and data production trends for low-quality text, high-quality text, and images. - Source: Epoch (2023), "'),t("strong",[t("a",{attrs:{href:"https://epochai.org/trends",target:"_blank",rel:"noopener noreferrer"}},[e._v("Key trends and figures in Machine Learning"),t("OutboundLink")],1)]),e._v('"*')]),e._v(" "),t("p",[e._v('Even if we run out of Data, many solutions are proposed, from using synthetic data, for example, filtering and preprocessing the data with GPT-3.5 to create a new cleaner dataset, an approach used in the paper "Textbooks are all you need" with models like Phi 1.5B that demonstrate excellent performance for their size through the use of high-quality filtered data, to the use of more efficient trainings, or being more efficient by training on more epochs.')])]),e._v(" "),t("p",[e._v('=== "'),t("strong",[e._v("Hardware")]),e._v('"')]),e._v(" "),t("tab",[t("p",[e._v('!!! warning "This is extra detail provided for those interested. It can be safely skipped."')])]),e._v(" "),t("p",[e._v('=== "'),t("strong",[e._v("Algorithms")]),e._v('"')]),e._v(" "),t("tab",[t("p",[e._v('!!! warning "This is extra detail provided for those interested. It can be safely skipped."')]),e._v(" "),t("p",[t("a",{attrs:{href:"https://arxiv.org/abs/2403.05812",target:"_blank",rel:"noopener noreferrer"}},[e._v("[2403.05812] Algorithmic progress in language models"),t("OutboundLink")],1)]),e._v(" "),t("p",[t("strong",[e._v("Algorithmic advancements")]),e._v(" also play a role. For instance, between 2012 and 2021, the computational power required to match the performance of AlexNet has been reduced by a factor of 40, which corresponds to a threefold yearly reduction in the compute required for achieving the same performance on image classification tasks like ImageNet. Improving the architecture also counts as algorithmic advancement. A particularly influential architecture is that of Transformers, central to many recent innovations, especially in chatbots and autoregressive learning. Their ability to be trained in parallel over every token of the context window fully exploits the power of modern GPUs, and this is thought to be one of the main reasons why they work so well compared to their predecessor, even if this point is controversial.")]),e._v(" "),t("p",[e._v('??? question "Does the algorithmic architecture really matter?"')]),e._v(" "),t("tab",[t("p",[e._v("This is a complicated question, but some evidence suggests that once an architecture is expressive and scalable enough, the architecture matters less than we might have thought:")]),e._v(" "),t("p",[e._v("In a paper titled ‚Äò"),t("a",{attrs:{href:"https://arxiv.org/abs/2310.16764",target:"_blank",rel:"noopener noreferrer"}},[e._v("ConvNets Match Vision Transformers at Scale"),t("OutboundLink")],1),e._v(",' Google researchers found that Visual Transformers (ViT) can achieve the same results as CNNs simply by using more compute. They took a special CNN architecture and trained it on a massive dataset of four billion images. The resulting model matched the accuracy of existing ViT systems that used similar training compute.")]),e._v(" "),t("p",[e._v("Variational Auto Encoders (long an also-ran to"),t("a",{attrs:{href:"https://en.wikipedia.org/wiki/Generative_adversarial_network",target:"_blank",rel:"noopener noreferrer"}},[e._v(" GANs"),t("OutboundLink")],1),e._v(" or autoregressive models in terms of image generation) catch up if you make them very deep ("),t("a",{attrs:{href:"https://arxiv.org/abs/2011.10650#openai",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(", "),t("a",{attrs:{href:"https://arxiv.org/abs/2007.03898#nvidia",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(").")]),e._v(" "),t("p",[e._v("Progress in late 2023, such as the mamba architecture ("),t("a",{attrs:{href:"https://arxiv.org/abs/2312.00752",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v("), appears to be an improvement on the transformer. It can be seen as an algorithmic advancement that reduces the amount of training computation needed to achieve the same performance.")]),e._v(" "),t("p",[e._v("The connections and normalizations in the transformer, which were thought to be important, can be taken out if the weights are set up correctly. This can also make the transformer design simpler (Note however that this architecture is slower to converge than the others). ("),t("a",{attrs:{href:"https://openreview.net/forum?id=NPrsUQgMjKK",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(")")]),e._v(" "),t("p",[e._v("On the other side of the argument, certain attention architectures are significantly more scalable when dealing with long context windows, and no feasible amount of training could compensate for this in more basic transformer models. Architectures specifically designed to handle long sequences, like Sparse Transformers ("),t("a",{attrs:{href:"https://arxiv.org/abs/1904.10509",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v(") or Longformer ("),t("a",{attrs:{href:"https://arxiv.org/abs/2004.05150",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1),e._v("), can outperform standard transformers by a considerable margin for this usage. In computer vision, architectures like CNNs are inherently structured to recognize spatial hierarchies in images, making them more efficient for these tasks than architectures not specialized in handling spatial data when the amount of data is limited, and the ‚Äúprior‚Äù encoded in the architecture makes the model learn faster.")])])],1),e._v(" "),t("p",[e._v('=== "'),t("strong",[e._v("Costs")]),e._v('"')]),e._v(" "),t("tab",[t("p",[e._v('!!! warning "This is extra detail provided for those interested. It can be safely skipped."')]),e._v(" "),t("p",[e._v('Summary of the report by Ben Cottier (2023), "'),t("a",{attrs:{href:"https://epochai.org/blog/trends-in-the-dollar-training-cost-of-machine-learning-systems",target:"_blank",rel:"noopener noreferrer"}},[e._v("Trends in the Dollar Training Cost of Machine Learning Systems"),t("OutboundLink")],1),e._v("‚Äù published on EpochAI.")]),e._v(" "),t("p",[e._v("Understanding the dollar cost of ML training runs is crucial for several reasons. Firstly, it reflects the real economic expense of developing machine learning systems, which is essential for forecasting the future of AI development and identifying which actors can afford to pursue large-scale AI projects. Secondly, by combining cost estimates with performance metrics, we can track the efficiency and capabilities of ML systems over time, offering insights into how these systems are improving and where inefficiencies may lie. Lastly, these insights help determine the sustainability of current spending trends and guide future investments in AI research and development, ensuring resources are allocated effectively to foster innovation while managing economic impact.")]),e._v(" "),t("p",[e._v("Moore‚Äôs Law, which predicts the doubling of transistor density and thus computational power approximately every two years, has historically led to decreased costs of compute power. However, the report finds that spending on ML training has grown much faster than the cost reductions suggested by Moore‚Äôs Law. This means that while hardware has become cheaper, the overall expense of training ML systems has escalated due to increasing demand for computational resources. This divergence underscores the rapid pace of advancements in ML and the significant investments required to keep up with the escalating computational demands.")]),e._v(" "),t("p",[e._v("To measure the cost of ML training runs, the report employs two primary methods. The first method uses historical trends in the price-performance of GPUs to estimate costs. This approach leverages general trends in hardware advancements and cost reductions over time. The second method bases estimates on the specific hardware used to train the ML systems, such as NVIDIA GPUs, providing a more detailed and accurate picture of the costs associated with particular technologies. Both methods involve calculating the hardware cost‚Äîthe portion of the up-front hardware cost used for training‚Äîand the energy cost, which accounts for the electricity required to power the hardware during training. These calculations provide a comprehensive view of the economic burden of training ML models.")]),e._v(" "),t("p",[e._v("Measuring the cost of development extends beyond the final training run of an ML system to encompass a range of factors. This includes research and development costs, which cover the expenditures on preliminary experiments and model refinements that lead up to the final product. It also involves personnel costs, including salaries and benefits for researchers, engineers, and support staff. Infrastructure costs, such as investments in data centers, cooling systems, and networking equipment, are also significant. Additionally, software and tools, including licenses and cloud services, contribute to the overall cost. Energy costs throughout the development lifecycle, not just during the final training run, and opportunity costs‚Äîpotential revenue lost from not pursuing other projects‚Äîare also crucial components. Understanding these broader costs provides a more comprehensive view of the economic impact of developing advanced ML systems, informing strategic decisions about resource allocation.")]),e._v(" "),t("p",[e._v("The findings suggest that the cost of ML training runs will continue to grow, but the rate of growth might slow down in the future. The report estimates that the cost of ML training has grown by approximately 2.8 times per year for all systems. For large-scale systems, the growth rate is slower, at around 1.6 times per year. This substantial year-on-year increase in training costs highlights the need for significant efficiency improvements in both hardware and training methodologies to manage future expenses effectively.")]),e._v(" "),t("p",[e._v("The report forecasts that if current trends continue, the cost for the most expensive training runs could exceed significant economic thresholds, such as 1% of the US GDP, within the next few decades. This implies that without efficiency improvements, the economic burden of developing state-of-the-art ML systems will increase substantially. Consequently, understanding and managing these costs is essential for ensuring the sustainable growth of AI capabilities and maintaining a balanced approach to AI investment and development.")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/L5n_Image_41.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: estimated cost of compute in US dollars for the final training run of ML systems. ("),t("strong",[t("a",{attrs:{href:"https://epochai.org/blog/trends-in-the-dollar-training-cost-of-machine-learning-systems",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1)]),e._v(")*")])]),e._v(" "),t("Citations")],1)}),[],!1,null,null,null);t.default=o.exports}}]);