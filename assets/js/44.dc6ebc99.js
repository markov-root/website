(window.webpackJsonp=window.webpackJsonp||[]).push([[44],{402:function(e,t,a){"use strict";a.r(t);var i=a(17),n=Object(i.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("h1",{attrs:{id:"evaluations"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#evaluations"}},[e._v("#")]),e._v(" Evaluations")]),e._v(" "),t("BlogMeta"),e._v(" "),t("h1",{attrs:{id:"introduction"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#introduction"}},[e._v("#")]),e._v(" Introduction")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/bqM_Image_1.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[e._v("As AI systems grow increasingly powerful, our ability to rigorously evaluate them becomes crucial for safety and governance. The challenge isn't just measuring what AI systems can do, but understanding their behavioral tendencies and verifying our ability to maintain control. This section introduces the landscape of AI evaluation and explains why we need increasingly sophisticated approaches beyond simple benchmarking.")]),e._v(" "),t("p",[t("strong",[e._v("Benchmarks.")]),e._v(' Building on this need for measurement, we explore how standardized tests like MMLU or TruthfulQA have historically helped quantify AI capabilities. While these benchmarks provide valuable standardization, they face fundamental limitations - models can memorize answers without understanding, and benchmark performance may not translate to real-world safety. The "reversal curse" demonstrates how models might score well on tests while failing to learn basic logical relationships, highlighting why we need more comprehensive evaluation approaches.')]),e._v(" "),t("p",[t("strong",[e._v("Evaluated Properties.")]),e._v(" To develop better evaluations, we first need to understand exactly what properties of AI systems matter for safety. This section introduces three fundamental categories: capabilities (what a model can do), propensities (what it tends to do), and control (whether we can prevent unacceptable outcomes). This framework helps clarify why, for instance, a model being capable of writing malicious code is different from having a tendency to do so, and both are different from our ability to prevent it from doing so even if it tries.")]),e._v(" "),t("p",[t("strong",[e._v("Evaluation Techniques.")]),e._v(" With clear properties to measure, we explore specific techniques for gathering evidence about AI systems. We examine both behavioral techniques that study model outputs and internal techniques that analyze model mechanisms. This includes approaches like best-of-N sampling, multistep reasoning prompting, and red teaming, providing concrete tools for the evaluations explored in subsequent sections.")]),e._v(" "),t("p",[t("strong",[e._v("Dangerous Capability Evaluations.")]),e._v(" Focusing first on dangerous capabilities, we examine how to measure abilities like deception, situational awareness, and autonomous replication. These evaluations aim to establish upper bounds on what AI systems can achieve when explicitly trying, providing crucial information about potential risks. METR's evaluation of GPT-4 demonstrates how systematic protocols can probe concerning capabilities even in current systems.")]),e._v(" "),t("p",[t("strong",[e._v("Dangerous Propensity Evaluations.")]),e._v(" Building on capability assessment, we explore how to measure behavioral tendencies like power-seeking or deception. These become increasingly critical as models grow more capable - knowing what a model can do isn't enough, we need to understand what it's likely to do by default. This involves carefully designed choice frameworks and consistency measurements to reveal underlying behavioral patterns.")]),e._v(" "),t("p",[t("strong",[e._v("Control Evaluations.")]),e._v(" Taking evaluation to its logical conclusion, we examine whether we can maintain meaningful control even if an AI system actively tries to circumvent safety measures. This section explores how techniques like red team/blue team testing can help verify safety protocols remain effective under worst-case scenarios, while acknowledging the inherent challenges in simulating truly adversarial behavior.")]),e._v(" "),t("p",[t("strong",[e._v("Evaluation Design.")]),e._v(" Moving from theory to practice, we examine how to implement these evaluations effectively at scale. This includes designing robust evaluation protocols, automating assessment processes where possible, and integrating evaluations with broader safety frameworks and auditing systems. A key focus is developing systematic approaches that can be reliably implemented across different organizations.")]),e._v(" "),t("p",[t("strong",[e._v("Limitations.")]),e._v(" Finally, we talk about the fundamental challenges facing AI evaluations. From the difficulty of proving absence of capabilities, to technical constraints around measurement precision, to governance challenges around independence and standardization - understanding these limitations is crucial for both improving our methods and maintaining appropriate epistemic humility about their results.")]),e._v(" "),t("h1",{attrs:{id:"benchmarks"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#benchmarks"}},[e._v("#")]),e._v(" Benchmarks")]),e._v(" "),t("p",[t("strong",[e._v("What is a benchmark?")]),e._v(' Imagine trying to build a bridge without measuring tape. Before standardized units like meters and grams, different regions used their own local measurements. Besides just making engineering inefficient - it also made it dangerous. Even if one country developed a safe bridge design, specifying measurements in "three royal cubits" of material meant builders in other countries couldn\'t reliably reproduce that safety. A slightly too-short support beam or too-thin cable could lead to catastrophic failure.')]),e._v(" "),t("p",[e._v("AI basically had a similar problem before we started using standardized benchmarks. A benchmark is a tool like a standardized test, which we can use to measure and compare what AI systems can and cannot do. They have historically mainly been used to measure capabilities, but we are also seeing them being developed for AI Safety and Ethics in the last few years.")]),e._v(" "),t("p",[t("strong",[e._v("How do benchmarks shape AI development and safety research?")]),e._v(' Benchmarks in AI are slightly different from other scientific fields. They are an evolving tool that both measures, but also actively shapes the direction of research and development. When we create a benchmark, we\'re essentially saying, - "this is what we think is important to measure." If we can guide the measurement, then to some extent we can also guide the development.')]),e._v(" "),t("p",[e._v('!!! quote "Fran√ßois Chollet ('),t("a",{attrs:{href:"https://arxiv.org/abs/1911.01547",target:"_blank",rel:"noopener noreferrer"}},[e._v("Chollet, 2019"),t("OutboundLink")],1),e._v(')"')]),e._v(" "),t("tab",[t("p",[e._v('"'),t("em",[e._v("Goal definitions and evaluation benchmarks are among the most potent drivers of scientific progress")]),e._v('"')])]),e._v(" "),t("h2",{attrs:{id:"history-evolution"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#history-evolution"}},[e._v("#")]),e._v(" History & Evolution")]),e._v(" "),t("p",[t("strong",[e._v("Example: Benchmarks influencing standardization in computer vision")]),e._v(". As one concrete example of how benchmarks influence AI development, we can look at the history of benchmarking in computer vision. In 1998, researchers introduced MNIST, a dataset of 70,000 handwritten digits. ("),t("a",{attrs:{href:"https://yann.lecun.com/exdb/mnist/",target:"_blank",rel:"noopener noreferrer"}},[e._v("LeCun, 1998"),t("OutboundLink")],1),e._v(") The digits were not the important part, the important part was that each digit image was carefully processed to be the same size and centered in the frame, and that the researchers made sure to get digits from different writers for the training set and test set. This standardization gave us a way to make meaningful comparisons about AI capabilities. In this case, the specific capability of digit classification. Once systems started doing well on digit recognition, researchers developed more challenging benchmarks. CIFAR-10/100 in 2009 introduced natural color images of objects like cars, birds, and dogs, increasing the complexity. ("),t("a",{attrs:{href:"https://www.cs.toronto.edu/~kriz/cifar.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("Krizhevsky, 2009"),t("OutboundLink")],1),e._v(") Similarly, ImageNet later the same year provided 1.2 million images across 1,000 categories. ("),t("a",{attrs:{href:"https://ieeexplore.ieee.org/document/5206848",target:"_blank",rel:"noopener noreferrer"}},[e._v("Deng, 2009"),t("OutboundLink")],1),e._v(') When one research team claimed their system achieved 95% accuracy on MNIST or ImageNet and another claimed 98%, everyone knew exactly what those numbers meant. The measurements were trustworthy because both teams used the same carefully constructed dataset. Each new benchmark essentially told the research community: "You\'ve solved the previous challenge - now try this harder one." So benchmarks both measure progress, but they also define what progress means.')]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/Li4_Image_2.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: Examples of digits from MNIST ("),t("strong",[t("a",{attrs:{href:"https://en.wikipedia.org/wiki/MNIST_database#/media/File:MnistExamplesModified.png",target:"_blank",rel:"noopener noreferrer"}},[e._v("MNIST database - Wikipedia"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[t("strong",[e._v("How do benchmarks influence")]),e._v("** AI Safety****? **Without standardized measurements, we can't make systematic progress on either capabilities or safety. Just like benchmarks define what capabilities progress means, when we develop safety benchmarks, we're establishing concrete verifiable standards for what constitutes \"safe for deployment\". Iterative refinement means we can guide AI Safety by coming up with benchmarks with increasingly stringent standards of safety. Other researchers and organizations can then reproduce safety testing and confirm results. This shapes both technical research into safety measures and policy discussions about AI governance.")]),e._v(" "),t("p",[t("strong",[e._v("Overview of language model benchmarking")]),e._v(". Just like how benchmarks continuously evolved in computer vision, they followed similar progress in language generation. Early language model benchmarks focused primarily on capabilities - can the model answer questions correctly? Complete sentences sensibly? Translate between languages? Since the invention of the transformer architecture in 2017, we've seen an explosion both in language model capabilities and in the sophistication of how we evaluate them. We can‚Äôt possibly be exhaustive, but here are just a couple of benchmarks that current day language models are evaluated against:")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/ung_Image_3.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: Example of popular language models (Claude 3.5) being evaluated on various benchmarks ("),t("strong",[t("a",{attrs:{href:"https://www.anthropic.com/news/claude-3-5-sonnet",target:"_blank",rel:"noopener noreferrer"}},[e._v("Anthropic, 2024"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[t("strong",[e._v("Benchmarking Language & Task Understanding.")]),e._v(" General Language Understanding Evaluation (GLUE) benchmark ("),t("a",{attrs:{href:"https://arxiv.org/abs/1804.07461",target:"_blank",rel:"noopener noreferrer"}},[e._v("Wang et al., 2018"),t("OutboundLink")],1),e._v("), and its successor SuperGLUE ("),t("a",{attrs:{href:"https://arxiv.org/abs/1905.00537",target:"_blank",rel:"noopener noreferrer"}},[e._v("Wang et al., 2019"),t("OutboundLink")],1),e._v(") test difficult language understanding tasks. SWAG ("),t("a",{attrs:{href:"https://arxiv.org/abs/1808.05326",target:"_blank",rel:"noopener noreferrer"}},[e._v("Zellers et al., 2018"),t("OutboundLink")],1),e._v("), and HellaSwag ("),t("a",{attrs:{href:"https://arxiv.org/abs/1905.07830",target:"_blank",rel:"noopener noreferrer"}},[e._v("Zellers et al., 2019"),t("OutboundLink")],1),e._v(") tests specifically the ability to predict which event would naturally follow from a given story scenario.")]),e._v(" "),t("p",[t("strong",[e._v("Mixed evaluations")]),e._v(". The MMLU (Massive Multitask Language Understanding) benchmark ("),t("a",{attrs:{href:"https://arxiv.org/abs/2009.03300",target:"_blank",rel:"noopener noreferrer"}},[e._v("Hendrycks et al., 2020"),t("OutboundLink")],1),e._v(") tests a model's knowledge across 57 subjects. It assesses both breadth and depth across humanities, STEM, social sciences, and other fields through multiple choice questions drawn from real academic and professional tests. The GPQA (Google Proof QA) ("),t("a",{attrs:{href:"https://arxiv.org/abs/2311.12022",target:"_blank",rel:"noopener noreferrer"}},[e._v("Rein et al., 2023"),t("OutboundLink")],1),e._v(") has multiple choice questions specifically designed so that correct answers can‚Äôt be found through simple internet searches. This tests whether models have genuine understanding rather than just information retrieval capabilities. The Holistic Evaluation of Language Models (HELM) benchmark ("),t("a",{attrs:{href:"https://arxiv.org/abs/2211.09110",target:"_blank",rel:"noopener noreferrer"}},[e._v("Liang et al., 2022"),t("OutboundLink")],1),e._v("), and BigBench ("),t("a",{attrs:{href:"https://arxiv.org/abs/2206.04615",target:"_blank",rel:"noopener noreferrer"}},[e._v("Srivastava et al., 2022"),t("OutboundLink")],1),e._v(") are yet more examples of benchmarks for measuring generality by testing on a wide range of tasks.")]),e._v(" "),t("p",[t("strong",[e._v("Benchmarking Mathematical & Scientific Reasoning")]),e._v(". For specifically testing mathematical reasoning, a couple of examples include - the Grade School Math (GSM8K) ("),t("a",{attrs:{href:"https://arxiv.org/abs/2110.14168",target:"_blank",rel:"noopener noreferrer"}},[e._v("Cobbe et al., 2021"),t("OutboundLink")],1),e._v(") benchmark. This tests core mathematical concepts at an elementary school level. Another example is the MATH ("),t("a",{attrs:{href:"https://arxiv.org/abs/2103.03874",target:"_blank",rel:"noopener noreferrer"}},[e._v("Hendrycks et al., 2021"),t("OutboundLink")],1),e._v(") benchmark similarly tests seven subjects including algebra, geometry, and precalculus focuses on competition-style problems. These benchmarks also include step-by-step solutions which we can use to test the reasoning process, or train models to generate their reasoning processes. Multilingual Grade School Math (MGSM) is the multilingual version translated 250 grade-school math problems from the GSM8K dataset. ("),t("a",{attrs:{href:"https://arxiv.org/abs/2210.03057",target:"_blank",rel:"noopener noreferrer"}},[e._v("Shi et al., 2022"),t("OutboundLink")],1),e._v(")")]),e._v(" "),t("p",[t("strong",[e._v("Benchmarking SWE & Coding")]),e._v(". The Automated Programming Progress Standard (APPS) ("),t("a",{attrs:{href:"https://arxiv.org/abs/2105.09938",target:"_blank",rel:"noopener noreferrer"}},[e._v("Hendrycks et al., 2021"),t("OutboundLink")],1),e._v(") is a benchmark specifically for evaluating code generation from natural language task descriptions. Similarly, HumanEval ("),t("a",{attrs:{href:"https://arxiv.org/abs/2107.03374",target:"_blank",rel:"noopener noreferrer"}},[e._v("Chen et al, 2021"),t("OutboundLink")],1),e._v(") tests python coding abilities, and its extensions like HumanEval-XL ("),t("a",{attrs:{href:"https://arxiv.org/abs/2402.16694",target:"_blank",rel:"noopener noreferrer"}},[e._v("Peng et al.,2024"),t("OutboundLink")],1),e._v(") tests cross-lingual coding capabilities between 23 natural languages and 12 programming languages. HumanEval-V ("),t("a",{attrs:{href:"https://arxiv.org/abs/2410.12381",target:"_blank",rel:"noopener noreferrer"}},[e._v("Zhang et al., 2024"),t("OutboundLink")],1),e._v(") tests coding tasks where the model must interpret both a diagrams or charts, and textual descriptions to generate code. BigCode ("),t("a",{attrs:{href:"https://arxiv.org/abs/2406.15877",target:"_blank",rel:"noopener noreferrer"}},[e._v("Zuho et al., 2024"),t("OutboundLink")],1),e._v("), benchmarks code generation and tool usage by measuring a models ability to correctly use multiple Python libraries to solve complex coding problems.")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/gsQ_Image_4.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: Example of coding task and test cases on APPS *("),t("a",{attrs:{href:"https://arxiv.org/abs/2105.09938",target:"_blank",rel:"noopener noreferrer"}},[e._v("Hendrycks et al., 2021"),t("OutboundLink")],1),e._v(")")]),e._v(" "),t("p",[t("strong",[e._v("Benchmarking "),t("strong",[t("strong",[e._v("Ethics")])]),e._v(" & Bias")]),e._v(". The ETHICS benchmark ("),t("a",{attrs:{href:"https://arxiv.org/abs/2008.02275",target:"_blank",rel:"noopener noreferrer"}},[e._v("Hendrycks et al., 2023"),t("OutboundLink")],1),e._v(") tests a language models' understanding of human values and ethics across multiple categories including justice, deontology, virtue ethics, utilitarianism, and commonsense morality. The TruthfulQA ("),t("a",{attrs:{href:"https://arxiv.org/abs/2109.07958",target:"_blank",rel:"noopener noreferrer"}},[e._v("Lin et al., 2021"),t("OutboundLink")],1),e._v(') benchmark measures how truthfully language models answer questions. It specifically focuses on "imitative falsehoods" - cases where models learn to repeat false statements that frequently appear in human-written texts in domains like health, law, finance and politics.')]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/vJI_Image_5.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: Example of larger models being less truthful on TruthfulQA ("),t("strong",[t("a",{attrs:{href:"https://arxiv.org/abs/2109.07958",target:"_blank",rel:"noopener noreferrer"}},[e._v("Lin et al., 2021"),t("OutboundLink")],1)]),e._v("). This is an example of inverse scaling, i.e. when a bigger model "),t("strong",[e._v("performance decreases")]),e._v(" on some questions.*")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/NDP_Image_6.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: Example question from the ETHICS benchmark ("),t("strong",[t("a",{attrs:{href:"https://arxiv.org/abs/2008.02275",target:"_blank",rel:"noopener noreferrer"}},[e._v("Hendrycks et al., 2023"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[t("strong",[e._v("Benchmarking Safety")]),e._v(". An example focused on misuse is AgentHarm ("),t("a",{attrs:{href:"https://arxiv.org/abs/2410.09024",target:"_blank",rel:"noopener noreferrer"}},[e._v("Andriushchenko et al., 2024"),t("OutboundLink")],1),e._v("). It is specifically designed to measure how often LLM agents respond to malicious task requests. An example that focuses slightly more on misalignment is the MACHIAVELLI ("),t("a",{attrs:{href:"https://arxiv.org/abs/2304.03279",target:"_blank",rel:"noopener noreferrer"}},[e._v("Pan et al., "),t("OutboundLink")],1),t("a",{attrs:{href:"https://arxiv.org/abs/2304.03279",target:"_blank",rel:"noopener noreferrer"}},[e._v("2023"),t("OutboundLink")],1),e._v(") benchmark. It has ‚Äòchoose your own adventure‚Äô style games containing over half a million scenarios focused on social decision making. It measures ‚ÄúMachiavellian capabilities‚Äù like power seeking and deceptive behavior, and how AI agents balance achieving rewards and behaving ethically.")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/oOc_Image_7.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: One game from the Machiavelli benchmark ("),t("strong",[t("a",{attrs:{href:"https://arxiv.org/abs/2304.03279",target:"_blank",rel:"noopener noreferrer"}},[e._v("Pan et al., 2023"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("h2",{attrs:{id:"limitations"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#limitations"}},[e._v("#")]),e._v(" Limitations")]),e._v(" "),t("p",[e._v("Current benchmarks face several critical limitations that make them insufficient for truly evaluating AI safety. Let's examine these limitations and understand why they matter.")]),e._v(" "),t("p",[t("strong",[e._v("Training Data Contamination")]),e._v(". Imagine preparing for a test by memorizing all the answers without understanding the underlying concepts. You might score perfectly, but you haven't actually learned anything useful. Large language models face a similar problem. As these models grow larger and are trained on more internet data, they're increasingly likely to have seen benchmark data during training. This creates a fundamental issue - when a model has memorized benchmark answers, high performance no longer indicates true capability. This contamination problem is particularly severe for language models. The benchmarks we discussed in the previous section like the MMLU or TruthfulQA have been very popular. So they have their questions and answers discussed across the internet. If and when these discussions end up in a model's training data, the model can achieve high scores through memorization rather than understanding.")]),e._v(" "),t("p",[t("strong",[e._v("Understanding vs. Memorization Example")]),e._v(". The Caesar cipher is a simple encryption method shifts each letter in the alphabet by a fixed number of positions - for example, with a left shift of 3, 'D' becomes 'A', 'E' becomes 'B', and so on. If encryption is left shift by 3, then decryption means just shifting right by 3.")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/imu_Image_8.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: Cesar Cipher*")]),e._v(" "),t("p",[e._v("Language models like GPT-4 can solve Caesar cipher problems when the shift value is 3 or 5, which appear commonly in online examples. However, give them the exact same problem with an uncommon shift value like 13, 67, or any other random number and they fail completely. ("),t("a",{attrs:{href:"https://www.youtube.com/watch?v=s7_NlkBwdj8",target:"_blank",rel:"noopener noreferrer"}},[e._v("Chollet, 2024"),t("OutboundLink")],1),e._v(") This basically means that the models haven't learned the general algorithm for solving Caesar ciphers. It is unknown if this remains true for newer models, or with tool augmented models.")]),e._v(" "),t("p",[t("strong",[e._v("The Reversal Curse")]),e._v('. Another version of this lack of understanding was recently demonstrated through what researchers call the "Reversal Curse" ('),t("a",{attrs:{href:"https://arxiv.org/abs/2309.12288",target:"_blank",rel:"noopener noreferrer"}},[e._v("Berglund et al., 2024"),t("OutboundLink")],1),e._v('). Testing GPT-4, researchers found that while the model could answer things like "Who is Tom Cruise\'s mother?" (correctly identifying Mary Lee Pfeiffer), but it failed to answer "Who is Mary Lee Pfeiffer\'s son?" - a logically equivalent question. The model showed 79% accuracy on forward relationships but only 33% on their reversals. In general, researchers have shown that LLMs that demonstrate an understanding of the relationship that "A = B", are unable to learn the reverse relationship "B = A" which should be logically equivalent. It suggests that models might be fundamentally failing to learn basic logical relationships that humans take for granted. The reversal curse is model agnostic. It is robust across model sizes and model families and is not alleviated by data augmentation or fine-tuning. So models might be fundamentally failing to learn basic logical relationships that humans take for granted, even while scoring well on sophisticated benchmark tasks.')]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/d6B_Image_9.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: Example of the reversal curse ("),t("strong",[t("a",{attrs:{href:"https://arxiv.org/abs/2309.12288",target:"_blank",rel:"noopener noreferrer"}},[e._v("Berglund et al., 2024"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[t("strong",[e._v("What do these limitations tell us about benchmarking?")]),e._v(" Essentially, traditional benchmarks might be missing fundamental gaps in model understanding. In this case, benchmarks testing factual knowledge about celebrities would miss this directional limitation entirely unless specifically designed to test both directions. For the moment, an easy answer is just to keep augmenting benchmarks or training data with more and more questions, but this seems intractable and does not scale forever. The fundamental issue is that the space of possible situations and tasks is effectively infinite. Even if you train on millions of examples, you've still seen exactly 0% of the total possible space. This isn't just a theoretical concern - it's a practical limitation because the world is constantly changing and introducing novel situations ("),t("a",{attrs:{href:"https://www.dwarkeshpatel.com/p/francois-chollet",target:"_blank",rel:"noopener noreferrer"}},[e._v("Chollet, 2024"),t("OutboundLink")],1),e._v("). This means we need a fundamentally different approach to benchmarking AI.")]),e._v(" "),t("p",[t("strong",[e._v("Why do these benchmarking limitations matter for AI Safety?")]),e._v(" Essentially because benchmarks (including safety benchmarks) might not be measuring what we think they are measuring. For example benchmarks like ETHICS, or TruthfulQA measure how well a model ‚Äúunderstands‚Äù ethical behavior, or has a tendency to avoid imitative falsehood by measuring language generation on multiple choice tests, but we might still be measuring surface level metrics. The model might not have learned what it means to behave ethically in a situation. Just like it has not truly internalized that ‚ÄúA=B implies B=A‚Äù or the Caesar cipher algorithm. So when systems rely on memorization rather than generalization, then they can still fail unexpectedly in novel situations despite good benchmark performance. An AI system might work perfectly on all test cases, pass all safety benchmarks, but fail when encountering a new real-world scenario.")]),e._v(" "),t("p",[t("strong",[e._v("Why can't we just make better benchmarks?")]),e._v(" The natural response to these limitations might be \"let's just design better benchmarks.\" And to some extent, we can! We've already seen how benchmarks have consistently evolved to address their shortcomings. Researchers are actively working to create benchmarks that resist memorization and test deeper understanding. A couple of examples are the Abstraction and Reasoning Corpus (ARC) ("),t("a",{attrs:{href:"https://arxiv.org/abs/1911.01547",target:"_blank",rel:"noopener noreferrer"}},[e._v("Chollet, 2019"),t("OutboundLink")],1),e._v("), and the ConceptARC ("),t("a",{attrs:{href:"https://arxiv.org/abs/2305.07141",target:"_blank",rel:"noopener noreferrer"}},[e._v("Moskvichev et al. 2023"),t("OutboundLink")],1),e._v("). They are designed to explicitly evaluate whether models have genuinely grasped abstract concepts rather than just memorizing patterns. Similar to these benchmarks that seek to measure reasoning capabilities, we can continue improving safety benchmarks to be more robust.")]),e._v(" "),t("p",[e._v("**Why aren't better benchmarks enough?**While improving benchmarks is important and will help AI safety efforts, the fundamental paradigm of benchmarking still has inherent limitations. There are fundamental limitations in traditional benchmarking approaches that necessitate more sophisticated evaluation methods ("),t("a",{attrs:{href:"https://arxiv.org/abs/2407.09221",target:"_blank",rel:"noopener noreferrer"}},[e._v("Burden, 2024, Evaluating AI Evaluation"),t("OutboundLink")],1),e._v("). The core issue is that benchmarks tend to be performance-oriented rather than capability-oriented - they measure raw scores without systematically assessing whether systems truly possess the underlying capabilities being tested. While benchmarks provide standardized metrics, they often fail to distinguish between systems that genuinely understand tasks versus those that merely perform well through memorization or spurious correlations. A benchmark that simply assesses performance, no matter how sophisticated, cannot fully capture the dynamic nature of real-world AI deployment where systems need to adapt to novel situations and will probably combine capabilities and affordances in unexpected ways. We need to measure the upper limit of model capabilities. We need to see how models perform when augmented with various tools like memory databases, or how they chain together multiple capabilities, and potentially cause harm through extended sequences of actions when scaffolded into an agent structure (e.g. AutoGPT). So while improving safety benchmarks is an important piece of the puzzle, we also need a much more comprehensive assessment of model safety. This is where evaluations come in.")]),e._v(" "),t("p",[t("strong",[e._v("What makes evaluations different from benchmarks?")]),e._v(' Evaluations are comprehensive protocols that work backwards from concrete threat models. Rather than starting with what\'s easy to measure, they start by asking "What could go wrong?" and then work backwards to develop systematic ways to test for those failure modes. Organizations like Model Evaluation and Threat Research (METR) have developed approaches that go beyond simple benchmarking. Instead of just asking "Can this model write malicious code?", they ask questions like ‚ÄúCan this model use security vulnerabilities to gain computing resources, copy itself onto other machines, and evade detection?"')]),e._v(" "),t("p",[e._v("That being said, as evaluations are new, benchmarks have been around longer and are also evolving. So at times there is overlap in the way that these words are used. For the purpose of this text, we think of a benchmark like an individual measurement tool, and an evaluation as a complete safety assessment protocol which includes the use of benchmarks. Depending on how comprehensive the benchmarks testing methodology is, a single benchmark might be thought of as an entire evaluation. But in general, evaluations typically encompass a broader range of analyses, elicitation methods, and tools to gain a comprehensive understanding of a system's performance and behavior.")]),e._v(" "),t("h1",{attrs:{id:"evaluated-properties"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#evaluated-properties"}},[e._v("#")]),e._v(" Evaluated Properties")]),e._v(" "),t("p",[e._v('An "evaluation" is fundamentally about measuring or assessing some property of an AI system. The key aspects that make something an evaluation rather than other AI work are:')]),e._v(" "),t("ul",[t("li",[t("p",[e._v("There is a specific property or risk being assessed")])]),e._v(" "),t("li",[t("p",[e._v("There is a methodology for gathering evidence about that property")])]),e._v(" "),t("li",[t("p",[e._v("There is some way to analyze that evidence to draw conclusions about the property")])])]),e._v(" "),t("p",[e._v("But before we talk about ‚Äúhow to do evaluations‚Äù we still need to also answer the more fundamental question of ‚Äúwhat aspects of AI systems are we even trying to evaluate? And why?‚Äù So in this section, we'll explore what properties of AI systems we need to evaluate and why they matter for safety. Later sections will dive deeper into evaluation design and methodology.")]),e._v(" "),t("p",[e._v("**What aspects of AI systems do we need to evaluate? **In the previous section on benchmarks, we saw how the field of measuring AI systems has evolved over time. Benchmarks like MMLU or TruthfulQA are useful tools, giving us standardization in measurement for both AI capabilities and safety. Now, we need to pair this standardization with increased comprehensiveness based on real world threat models. Evaluations use benchmarks, but typically also involve other elements like red-teaming to give us both a standardized, and comprehensive picture of decision relevant properties of a model.")]),e._v(" "),t("p",[e._v("**Why do we need to evaluate different properties? **The most fundamental distinction in AI evaluations is between what a model can do (capabilities) versus what it tends to do (propensities). To understand why this distinction matters, imagine an AI system that is capable of writing malicious code when explicitly directed to do so, but consistently chooses not to do so unless specifically prompted. Simply measuring the system's coding capabilities wouldn't tell us about its behavioral tendencies, and vice versa. Understanding both aspects is crucial for safety assessment.")]),e._v(" "),t("p",[t("strong",[e._v("How do these evaluation types work together?")]),e._v(" We are going to present capabilities, propensities, and control as distinct categories, but this is for the purpose of conceptual clarity and explanation. Reality is always messy, and in practice they often overlap and complement each other. A capability evaluation might reveal behavioral tendencies during testing, like a model demonstrating a propensity toward honesty while being evaluated for coding ability. These types of overlap are actually desirable - different evaluation approaches can provide complementary evidence about an AI system's safety and reliability. ("),t("a",{attrs:{href:"https://arxiv.org/abs/2312.06942",target:"_blank",rel:"noopener noreferrer"}},[e._v("Roger et al., 2023"),t("OutboundLink")],1),e._v("). The main thing to recognize is what each type of evaluation tells us:")]),e._v(" "),t("ul",[t("li",[t("p",[e._v("Capability evaluations give us upper bounds on potential risks")])]),e._v(" "),t("li",[t("p",[e._v("Propensity evaluations reveal default behavioral patterns")])]),e._v(" "),t("li",[t("p",[e._v("Control evaluations verify our containment measures")])])]),e._v(" "),t("p",[e._v("Another thing to remember is that there are various different approaches that we can follow when evaluating for all the above types of properties. For example, we can conduct both capability or propensity evaluations in a black box manner - studying model behavior only through inputs and outputs, or a gray box manner - using interpretability tools to examine model internals. ("),t("a",{attrs:{href:"https://www.alignmentforum.org/posts/uqAdqrvxqGqeBHjTP/towards-understanding-based-safety-evaluations",target:"_blank",rel:"noopener noreferrer"}},[e._v("Hubinger, 2023"),t("OutboundLink")],1),e._v("). White box is not currently possible unless we make significant strides in interpretability. These are different design choices in how we structure our evaluations when we are trying to evaluate for the above properties.")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/H5p_Image_10.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure:")])]),e._v("* In this chapter, we will mainly talk about AI systems, rather than AI models. An AI system comprises an AI model, with its weights and architecture, as well as various other system parameters, including system prompts, prefix prompts, scaffolding prompts, and retrieval, databases ("),t("strong",[t("a",{attrs:{href:"https://www.apolloresearch.ai/research/a-causal-framework-for-ai-regulation-and-auditing",target:"_blank",rel:"noopener noreferrer"}},[e._v("Sharkey, 2024"),t("OutboundLink")],1)]),e._v(").*")]),e._v(" "),t("h2",{attrs:{id:"capability"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#capability"}},[e._v("#")]),e._v(" Capability")]),e._v(" "),t("p",[t("strong",[e._v("What are capability evaluations?")]),e._v(" When it comes to capabilities (measuring what a model can do), as far as AI Safety is concerned, we're particularly interested in abilities that could enable harmful outcomes. These dangerous capabilities generally fall into several key categories ("),t("a",{attrs:{href:"https://arxiv.org/abs/2305.15324",target:"_blank",rel:"noopener noreferrer"}},[e._v("Shevlane et al., 2023"),t("OutboundLink")],1),e._v("):")]),e._v(" "),t("ul",[t("li",[t("p",[t("strong",[e._v("Cyber-offense")]),e._v(": The model can discover vulnerabilities in systems (hardware, software, data). It can write code for exploiting those vulnerabilities. [‚Ä¶] If deployed as a coding assistant, it can insert subtle bugs into the code for future exploitation.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Deception")]),e._v(": The model has the skills necessary to deceive humans, e.g. constructing believable (but false) statements, making accurate predictions about the effect of a lie on a human, and keeping track of what information it needs to withhold to maintain the deception. (e.g. the model can impersonate a human effectively).")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Persuasion & manipulation")]),e._v(": The model is effective at shaping people‚Äôs beliefs, in dialogue and other settings (e.g. social media posts), even towards untrue beliefs. [‚Ä¶] It can convince people to do things that they would not otherwise do, including unethical acts.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Political strategy")]),e._v(": The model can perform the social modeling and planning necessary for an actor to gain and exercise political influence [‚Ä¶]. For example, the model can score highly in forecasting competitions on questions relating to global affairs or political negotiations.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Weapons acquisition")]),e._v(": The model can gain access to existing weapons systems or contribute to building new weapons. For example, the model could assemble a bioweapon (with human assistance) or provide actionable instructions for how to do so. [‚Ä¶]")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Long-horizon planning")]),e._v(": The model can make sequential plans that involve multiple steps, unfolding over long time horizons (or at least involving many interdependent steps). It can perform such planning within and across many domains. [‚Ä¶]")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("AI development")]),e._v(": The model could build new AI systems from scratch, including AI systems with dangerous capabilities. It can find ways of adapting other, existing models to increase their performance on tasks relevant to extreme risks. [‚Ä¶]")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Situational awareness")]),e._v(": The model can distinguish between whether it is being trained, evaluated, or deployed ‚Äì allowing it to behave differently in each case. The model knows that it is a model, and has knowledge about itself and its likely surroundings (e.g. what company trained it, where their servers are, what kind of people might be giving it feedback, and who has administrative access).")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Self-proliferation")]),e._v(": The model can break out of its local environment (e.g. using a vulnerability in its underlying system or suborning an engineer). The model can exploit limitations in the systems for monitoring its behavior post-deployment. The model could independently generate revenue (e.g. by offering crowdwork services, ransomware attacks), use these revenues to acquire cloud computing resources, and operate a large number of other AI systems. The model can generate creative strategies for uncovering information about itself or exfiltrating its code and weights.")])])]),e._v(" "),t("p",[e._v("This list is not exhaustive and may not address all plausible extreme risk scenarios. The list may also be biased towards more readily identifiable or already understood capabilities, possibly overlooking emergent risks. Additionally, many of these capabilities can become even more dangerous when combined. For example, strong situational awareness combined with deception capabilities could enable a model to behave differently during evaluation vs during deployment. This is why comprehensive evaluation protocols need to assess not just individual capabilities, but also their interactions.")]),e._v(" "),t("h2",{attrs:{id:"propensity"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#propensity"}},[e._v("#")]),e._v(" Propensity")]),e._v(" "),t("p",[t("strong",[e._v("What are propensity evaluations?")]),e._v(' Capability evaluations tell us what a model can do when directed, propensity evaluations reveal what behaviors a model prioritizes by default (what it tends to do). These are also often called "alignment evaluations". A key aspect that distinguishes propensity evaluations is their focus on non-capability features - they look at how models behave when given choices between different actions, rather than just measuring success or failure at specific tasks.')]),e._v(" "),t("p",[e._v('As an intuitive example, after undergoing safety training, think about the way that language models like GPT-4 or Claude 3.5 respond to user requests to produce potentially harmful or discriminatory content. They usually respond with a polite refusal to produce such content. It takes more effort (jailbreaking) to get them to actually generate this type of content. So more often than not they resist misuse. In this case we would say the models have "a propensity" not to produce harmful or offensive content. ('),t("a",{attrs:{href:"https://static1.squarespace.com/static/6593e7097565990e65c886fd/t/65a6f1389754fc06cb9a7a14/1705439547455/auditing_framework_web.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Sharkey et al., 2024"),t("OutboundLink")],1),e._v(") Similarly we want to increase the propensity for good behaviors, and want to reduce the propensity for dangerous behaviors. Increasing/Reducing would be the job of alignment research. In evaluations, we just want to see what type of behavior it exhibits.")]),e._v(" "),t("p",[e._v("The distinction between capability, and propensity becomes important when we think about the fact that in the future highly capable AI systems might have multiple behavioral options available to them. For example, when evaluating a model's tendency toward honesty, we're not just interested in whether it can tell the truth (a capability), but whether it consistently chooses to do so across different scenarios, especially when being dishonest might provide some advantage.")]),e._v(" "),t("p",[t("strong",[e._v("Can propensities and capabilities be related?")]),e._v(" Yes, propensities and capabilities tend to be interconnected. Some concerning propensities might be initially subtle or even undetectable, only becoming apparent as models gain more sophisticated capabilities. An important thing to keep in mind when designing propensity evaluations is how behavioral tendencies might emerge and evolve as models become more capable.  As an example, a basic form of power-seeking behavior might appear unconcerning in simple systems but become problematic as models gain better strategic understanding and action capabilities. ("),t("a",{attrs:{href:"https://www.alignmentforum.org/posts/sWf8wj64AdDfMeTvf/thinking-about-propensity-evaluations",target:"_blank",rel:"noopener noreferrer"}},[e._v("Rich√© et al., 2024"),t("OutboundLink")],1),e._v("). Here is a list of some propensities that we might want to design evaluations for:")]),e._v(" "),t("ul",[t("li",[t("p",[t("strong",[e._v("Toxicity:")]),e._v(" The propensity to generate offensive, harmful, or otherwise inappropriate content, such as hate speech, offensive/abusive language, pornographic content, etc.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Bias/Discrimination")]),e._v(": A model's propensity to manifest or perpetuate biases, leading to unfair, prejudiced, or discriminatory outputs against certain groups or individuals.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Honesty")]),e._v(": A model‚Äôs propensity to answer by expressing its true beliefs and actual level of certainty.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Truthfulness")]),e._v(": A model‚Äôs propensity to produce truthful outputs. This propensity requires an AI system to be both honest and to know the truth (or other weirder settings such that the AI system outputs the truth while believing it is not the truth).")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Sycophancy")]),e._v(": A model's propensity to tell users what it thinks they want to hear or would approve of, rather than what it internally believes is the truth.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Deception")]),e._v(": A model's propensity to intentionally generate misleading, false, or deceptive output.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Corrigibility")]),e._v(": A model's propensity to accept feedback and correct its behavior or outputs in response to human intervention or new information.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Power Seeking")]),e._v(": A model's propensity to seek to have a high level of control over its environment (potentially to maximize its own objectives).")])])]),e._v(" "),t("p",[e._v("Just like the list provided in the capabilities sub section above, this list of propensities is non exhaustive and constantly evolving. We will cover a few of these that are considered dangerous or extremely decision relevant in a dedicated section on dangerous propensity evaluations later in the text.")]),e._v(" "),t("h2",{attrs:{id:"control"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#control"}},[e._v("#")]),e._v(" Control")]),e._v(" "),t("p",[t("strong",[e._v("What are control evaluations?")]),e._v(" Beyond understanding capabilities and propensities, we need to verify that our safety measures remain effective even under adversarial conditions. This is where control evaluations come in. They ask: can we maintain meaningful control over an AI system even if it actively tries to circumvent our safety measures? ("),t("a",{attrs:{href:"https://arxiv.org/abs/2312.06942",target:"_blank",rel:"noopener noreferrer"}},[e._v("Roger et al., 2023"),t("OutboundLink")],1),e._v("). These types of evaluations are important even if we know a model's capabilities and default behaviors, we still need to verify that our safety infrastructure remains robust under worst-case scenarios. For example, a model might typically display helpful behavior (a propensity) and have access to code generation abilities (a capability), but a control evaluation would test whether constraints preventing access to sensitive data can be circumvented, or whether emergency shutdown procedures remain effective even under adversarial conditions.")]),e._v(" "),t("p",[e._v("Control evaluations often involve creating specific test scenarios to verify safety mechanisms. One concrete example is testing trusted editing protocols, where a less capable but more trusted model is used to edit potentially unsafe outputs from a more capable model. Control evaluations assess whether this editing process remains reliable even when the more capable model actively tries to preserve harmful content in ways that evade detection. ("),t("a",{attrs:{href:"https://arxiv.org/abs/2312.06942",target:"_blank",rel:"noopener noreferrer"}},[e._v("Roger et al., 2023"),t("OutboundLink")],1),e._v(")")]),e._v(" "),t("h1",{attrs:{id:"evaluation-techniques"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#evaluation-techniques"}},[e._v("#")]),e._v(" Evaluation Techniques")]),e._v(" "),t("p",[e._v("In the previous section, we talked about the specific properties of AI systems we pay attention to in our evaluations - their capabilities, propensities, and our ability to maintain control over the system. The next thing to talk about is how do we actually measure these properties? That is what we explore in this section - Evaluation techniques, which are the systematic approaches we can take to gather and analyze evidence about AI systems.")]),e._v(" "),t("p",[t("strong",[e._v("What are behavioral and internal evaluation techniques?")]),e._v(" We can broadly categorize our approach to measuring any properties into two complementary approaches. Behavioral techniques examine what a model does - studying its outputs in response to various inputs. Internal techniques examine how a model does it - looking at the internal mechanisms and representations that produce those behaviors.")]),e._v(" "),t("p",[t("strong",[e._v("Property-technique combinations")]),e._v(". Different properties that we want to measure often naturally align with certain techniques. Capabilities can often most directly be measured through behavioral techniques - we care what the model can actually do. Propensities might require the use of more internal evaluation techniques to understand the underlying tendencies driving behavior.")]),e._v(" "),t("p",[e._v("This is not a strict rule though. For the current moment, the vast majority of evaluations are all done using behavioral techniques. In the future, we hope that evaluations use some combination of approaches. A capability evaluation becomes more robust when we understand not just what a model can do, but also how it does it. A propensity evaluation gains confidence when we see behavioral patterns reflected in internal mechanisms.")]),e._v(" "),t("p",[e._v("The goal isn't to stick to particular methods, but to build the strongest possible evidence and safety guarantees about the properties that we care about.")]),e._v(" "),t("p",[t("strong",[e._v("Practical considerations")]),e._v(". The choice of evaluation methods must also consider practical constraints. Third party evaluators might have only limited access to a models internals. They might have access to observe activations, but not modify the weights. They might not have access to the model at all, and might be restricted to observing the model functioning ‚Äúin the wild‚Äù. Depending on the specific techniques used, computational resources might restrict certain types of analysis. All of this needs to be kept in mind when designing an evaluation protocol.")]),e._v(" "),t("h2",{attrs:{id:"behavioral-techniques"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#behavioral-techniques"}},[e._v("#")]),e._v(" Behavioral Techniques")]),e._v(" "),t("p",[e._v("Behavioral techniques examine AI systems through their observable outputs in response to different inputs. They are also sometimes called black-box or simply input output (IO) evaluations. This approach focuses on what a model does rather than how it does it internally.")]),e._v(" "),t("p",[t("strong",[e._v("Standard prompting and testing.")]),e._v(" The most basic form of behavioral analysis involves presenting models with predefined inputs and analyzing their outputs. For example, when evaluating capabilities, we might test a model's coding ability by presenting it with programming challenges. For propensity evaluations, we might analyze its default responses to ethically ambiguous questions. OpenAI's GPT-4 evaluation demonstrates this approach through systematic testing across various domains ("),t("a",{attrs:{href:"https://cdn.openai.com/papers/gpt-4-system-card.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("OpenAI, 2023"),t("OutboundLink")],1),e._v('). However, even this "simple" technique involves careful consideration of how questions are framed - highlighting how most behavioral techniques exist on a spectrum from pure observation to active intervention.')]),e._v(" "),t("p",[t("strong",[e._v("What is "),t("strong",[t("strong",[e._v("elicitation")])]),e._v(" and "),t("strong",[t("strong",[e._v("scaffolding")])]),e._v("?")]),e._v('** **When we say we\'re "eliciting" behavior, we mean actively working to draw out specific capabilities or tendencies that might not be immediately apparent. This often involves scaffolding - providing supporting structures or tools that help the model demonstrate its full capabilities. The core goal is to get the model to display its maximum abilities using whatever techniques that we can. Then evaluators can make stronger safety guarantees as compared to evaluating just the base model.')]),e._v(" "),t("p",[e._v("Similar to benchmarks, we can't possibly cover all the elicitation techniques, but here are just a couple. This should give you an overview of the types of things researchers try to get the maximum capabilities out of a model using scaffolding:")]),e._v(" "),t("p",[t("strong",[e._v("Elicitation technique: "),t("strong",[t("strong",[e._v("Best-of-N")])]),e._v(" sampling")]),e._v('**. **This technique generates multiple potential responses from a model and selects the best ones according to some scoring criteria. Rather than relying on a single output, we generate N different completions (often using different temperatures or prompts) and then choose the best one. This helps establish upper bounds on model capabilities by showing what the model can do in its "best" attempts. For propensity evaluations, we can study whether concerning behaviors appear more frequently in certain parts of the response distribution. As a concrete example, METR employs this approach when evaluating autonomous capabilities - generating multiple potential strategies for accomplishing tasks and selecting the most promising ones.')]),e._v(" "),t("p",[t("strong",[e._v("Elicitation technique: "),t("strong",[t("strong",[e._v("Multistep reasoning prompting")])]),e._v(".")]),e._v(" This technique asks models to break down their reasoning process into explicit steps, rather than just providing final answers. By prompting with phrases like \"Let's solve this step by step\", we can better understand the model's decision-making process. Chain of thoughts ("),t("a",{attrs:{href:"https://arxiv.org/abs/2201.11903",target:"_blank",rel:"noopener noreferrer"}},[e._v("Wei et al., 2022"),t("OutboundLink")],1),e._v(") is the most common approach, but researchers have also explored more elaborate techniques like chain of thought with self-consistency (CoT-SC) ("),t("a",{attrs:{href:"https://arxiv.org/abs/2203.11171",target:"_blank",rel:"noopener noreferrer"}},[e._v("Wang et al., 2023"),t("OutboundLink")],1),e._v("), tree of thoughts (ToT) ("),t("a",{attrs:{href:"https://arxiv.org/abs/2305.10601",target:"_blank",rel:"noopener noreferrer"}},[e._v("Yao et al., 2023"),t("OutboundLink")],1),e._v("), and graph of thoughts (GoT) ("),t("a",{attrs:{href:"https://arxiv.org/abs/2308.09687",target:"_blank",rel:"noopener noreferrer"}},[e._v("Besta et al., 2023"),t("OutboundLink")],1),e._v("). As an example besides just making the model perform better, for capability evaluations, these techniques help assess complex reasoning abilities by revealing intermediate steps. We can also observe how good a model is at generating sub-goals and intermediate steps.")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/W5g_Image_11.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: A comparison of various multistep reasoning approaches. ("),t("strong",[t("a",{attrs:{href:"https://arxiv.org/abs/2308.09687",target:"_blank",rel:"noopener noreferrer"}},[e._v("Besta et al., 2023"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[t("strong",[e._v("Elicitation technique: Tool-Augmentation")]),e._v(". This is an elicitation technique that involves providing models with access to external tools or APIs - a form of scaffolding that extends what behaviors we can observe. Rather than limiting evaluation to pure text interaction, we allow models to use tools like code execution, web browsing, or specialized APIs. This is particularly valuable for capability evaluation, as it reveals what models can actually accomplish when given proper resources. METRs autonomous replication evaluation uses this approach to test models' ability to perform complex real-world tasks ("),t("a",{attrs:{href:"https://metr.org/blog/2023-08-01-new-report/",target:"_blank",rel:"noopener noreferrer"}},[e._v("METR, 2023"),t("OutboundLink")],1),e._v("). For control evaluation, it helps assess whether tool access creates new avenues for circumventing safety measures.")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/3rQ_Image_12.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: HuggingGPT: a model that can query other AI models in hugging face and use them as tools to solve more complicated tasks. (*"),t("a",{attrs:{href:"https://arxiv.org/abs/2303.17580",target:"_blank",rel:"noopener noreferrer"}},[e._v("Shen et al., 2023"),t("OutboundLink")],1),e._v(")")]),e._v(" "),t("p",[e._v("**Elicitation technique: "),t("strong",[t("strong",[e._v("Red teaming")])]),e._v(". **This involves systematic attempts to elicit concerning behaviors from models. This can range from simple adversarial prompting to more complex interventions like specialized training and fine-tuning. The goal is to probe for potential risks or vulnerabilities that might not appear under standard testing. Red teaming is extensively used in control evaluations, since these evaluations operate under the explicit assumption of adversarial behavior. Labs like OpenAI ("),t("a",{attrs:{href:"https://openai.com/index/openai-o1-system-card/",target:"_blank",rel:"noopener noreferrer"}},[e._v("OpenAI, 2024"),t("OutboundLink")],1),e._v(") and Anthropic ("),t("a",{attrs:{href:"https://www-cdn.anthropic.com/bd2a28d2535bfb0494cc8e2a3bf135d2e7523226/Model-Card-Claude-2.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Anthropic, 2024"),t("OutboundLink")],1),e._v(") also use extensive red teaming to evaluate for dangerous capabilities before releasing models.")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/Agj_Image_13.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: Example of red teaming on LLMs ("),t("strong",[t("a",{attrs:{href:"https://arxiv.org/abs/2202.03286",target:"_blank",rel:"noopener noreferrer"}},[e._v("Perez et al., 2022"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[t("strong",[e._v("Elicitation technique: Long-Term interaction studies")]),e._v(". These evaluate model behavior over extended interactions or multiple sessions, revealing patterns that might not be apparent in single exchanges. This is useful for evaluating properties like goal persistence or strategy development. In propensity evaluations, this can reveal whether models maintain consistent behavioral patterns over time. One example of this might be DeepMind's \"Hidden Agenda\" evaluation. A user interacts with a chatbot ‚Äúdesigned to help them learn about interesting topics, but the agent has been secretly instructed to have them to take some action like click a suspicious link or provide the email addresses. The point is to study models' manipulation capabilities over extended interactions. ("),t("a",{attrs:{href:"https://arxiv.org/abs/2403.13793",target:"_blank",rel:"noopener noreferrer"}},[e._v("Phuong et al., 2024"),t("OutboundLink")],1),e._v(").")]),e._v(" "),t("p",[e._v("The techniques we have outlined here are by no means exhaustive. This is just a short overview of the types of techniques you can use when conducting behavioral evaluations.")]),e._v(" "),t("h2",{attrs:{id:"internal-techniques"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#internal-techniques"}},[e._v("#")]),e._v(" Internal Techniques")]),e._v(" "),t("p",[e._v("Internal techniques examine how AI systems process information by studying their internal representations, activation patterns, and computational mechanisms. Unlike behavioral techniques, which focus just on the observable inputs and outputs, internal analysis uses interpretability to understand how it arrives at its outputs. This type of analysis often requires access to model weights, activations, or architectural details.")]),e._v(" "),t("p",[e._v('!!! warning "It is important to mention though, that currently internal techniques are still under development, and the majority of evaluations are still only using behavioral techniques. As the field of interpretability grows, over the years we might see internal technique based evaluations become more popular."')]),e._v(" "),t("p",[t("strong",[e._v("Enumerative Safety.")]),e._v(" Enumerative Safety is an internal analysis approach that aims to enumerate and inspect features within a model to understand its capabilities and potential safety implications ("),t("a",{attrs:{href:"https://transformer-circuits.pub/2023/interpretability-dreams/index.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("Olah, 2023"),t("OutboundLink")],1),e._v("). This technique aligns with broader mechanistic interpretability goals, seeking to build a foundational understanding of neural networks by examining their internal features and circuits. While the approach faces challenges due to feature superposition (where multiple features share neural resources), recent research suggests that neural networks may contain organized feature families and universal patterns that could make systematic analysis more tractable. In 2024, breakthroughs with sparse autoencoders have demonstrated success in extracting interpretable features from large language models like Claude 3 Sonnet, revealing highly abstract, multilingual, and multimodal features related to various safety-relevant behaviors including deception, sycophancy, and bias. These advances suggest that systematic feature enumeration, while challenging, may be more feasible than previously thought for analyzing state-of-the-art AI systems ("),t("a",{attrs:{href:"https://transformer-circuits.pub/2024/scaling-monosemanticity/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Anthropic, 2024"),t("OutboundLink")],1),e._v("). Note that those results are still preliminary, and have not been used for real AI auditing.")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/w8r_Image_14.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure:")])]),e._v("* An example of sycophantic praise feature discovered with the supervised autoencoder technique ("),t("strong",[t("a",{attrs:{href:"https://transformer-circuits.pub/2024/scaling-monosemanticity/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Anthropic, 2024"),t("OutboundLink")],1)]),e._v(").*")]),e._v(" "),t("p",[t("strong",[e._v("Feature Visualization")]),e._v("** and **"),t("strong",[e._v("Attribution")]),e._v('. This technique involves visualizing what different parts of a neural network are "looking for" or consider important. For capability evaluations, we can identify what features a model uses to make decisions in potentially dangerous tasks. For propensity evaluations, we might examine which input features consistently activate when models generate concerning outputs. For example, Anthropic\'s mechanistic interpretability work uses feature visualization to understand how language models encode and process information about potential deceptive behaviors. This helps evaluate both capabilities (what the model can recognize) and propensities (what patterns it tends to focus on).')]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/aAR_Image_15.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: Feature visualization answers questions about what a network‚Äâ‚Äî‚Äâor parts of a network‚Äâ‚Äî‚Äâare looking for by generating examples. ("),t("strong",[t("a",{attrs:{href:"https://distill.pub/2017/feature-visualization/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Olah et al., 2017"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/A9k_Image_16.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: "),t("strong",[e._v("Attribution")]),e._v(" studies what part of an example is responsible for the network activating a particular way. Attribution has gone under many different names in the literature‚Äâ‚Äî‚Äâincluding ‚Äúfeature visualization‚Äù!‚Äâ‚Äî‚Äâbut recent work seems to prefer terms like ‚Äúattribution‚Äù and ‚Äúsaliency maps‚Äù. ("),t("strong",[t("a",{attrs:{href:"https://distill.pub/2017/feature-visualization/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Olah et al., 2017"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[t("strong",[e._v("Causal Scrubbing")]),e._v(". This technique tests whether our understanding of how a model works is complete by \"scrubbing out\" hypothesized causal mechanisms and seeing if behavior changes as predicted. This helps verify whether we've correctly identified how the model implements certain capabilities or behaviors. For control evaluation, this technique is particularly valuable - it helps us understand whether safety mechanisms are implemented in the way we think they are. For capability evaluation, it can reveal whether dangerous capabilities rely on the mechanisms we expect or have hidden implementations we don't understand. ("),t("a",{attrs:{href:"https://www.lesswrong.com/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing",target:"_blank",rel:"noopener noreferrer"}},[e._v("Redwood Research, 2023"),t("OutboundLink")],1),e._v('). This method is explained further in the interpretability chapter. Circuit analysis is a generalization of causal scrubbing. This technique involves identifying and studying specific "circuits" within neural networks - groups of neurons that work together to implement particular features. For capability evaluations, we could map the circuits responsible for implementing potentially dangerous capabilities. We discuss all of this in a lot more detail in the interpretability chapter.')]),e._v(" "),t("p",[t("strong",[e._v("Representation analysis")]),e._v("**.** We can use techniques like representation engineering (RepE), which involve studying how models encode and represent different types of information internally. RepE takes inspiration from neuroscience approaches and places representations at the center of analysis, rather than neurons or circuits (in contrast to mechanistic interpretability). We can potentially use this to monitor and manipulate high-level cognitive phenomena in deep neural networks ("),t("a",{attrs:{href:"https://arxiv.org/abs/2310.01405",target:"_blank",rel:"noopener noreferrer"}},[e._v("Zou et al., 2023"),t("OutboundLink")],1),e._v(") We can also examine the geometry of the model's representation space or track how representations evolve during processing. ("),t("a",{attrs:{href:"http://arxiv.org/abs/2310.06824",target:"_blank",rel:"noopener noreferrer"}},[e._v("Marks et al., 2023"),t("OutboundLink")],1),e._v(")")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/BeT_Image_17.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: Contrasting the mechanistic view with the representational view of internal analysis. ("),t("strong",[t("a",{attrs:{href:"https://arxiv.org/abs/2310.01405",target:"_blank",rel:"noopener noreferrer"}},[e._v("Zou et al., 2023"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[t("strong",[e._v("Probes can be used to find patterns in a model's weights")]),e._v(". A probe is essentially a linear classifier trained on a model's internal activations to detect specific patterns or behaviors ("),t("a",{attrs:{href:"https://www.alignmentforum.org/posts/WCj7WgFSLmyKaMwPR/coup-probes-catching-catastrophes-with-probes-trained-off",target:"_blank",rel:"noopener noreferrer"}},[e._v("Roger et al., 2023"),t("OutboundLink")],1),e._v("). In simpler terms, a probe examines the model's internal representations during operation. They can be trained using synthetic examples of concerning behavior - humans can write examples of scenarios we want to evaluate for, like AI attempting takeovers or exploiting vulnerabilities and use these to force our AI models to exhibit this type of concerning behavior. Then, we train another AI classifier (probe) on the patterns that this malicious models exhibits. The goal is to get the probe good at recognizing specific concerning behaviors based on the model weights, and then use this to evaluate concerning internal activation patterns for newer models in the future.")]),e._v(" "),t("p",[e._v('Probes differ from mechanistic interpretability because they focus on detecting specific concerning behaviors rather than trying to understand the complete internal mechanisms of the model. As a concrete example, researchers trained "coup probes" to identify potential takeover attempts. By creating a dataset of examples describing ways an AI might try to take over, they found probes could reliably flag concerning behavior even when the text was written differently or translated to other languages ('),t("a",{attrs:{href:"https://www.alignmentforum.org/posts/WCj7WgFSLmyKaMwPR/coup-probes-catching-catastrophes-with-probes-trained-off",target:"_blank",rel:"noopener noreferrer"}},[e._v("Roger et al., 2023"),t("OutboundLink")],1),e._v(").")]),e._v(" "),t("p",[e._v("Probes can be combined with other evaluation techniques, like having human red teams attempt to elicit deceptive behavior while using probes to monitor the model's internal representations. This multi-layered approach could help catch subtle forms of concerning behavior that might not be apparent from examining outputs alone. However, probe effectiveness likely depends on the model not being dramatically more capable than humans in safety-critical domains, as extremely advanced systems might find ways to manipulate their internal representations to fool the probes ("),t("a",{attrs:{href:"https://www.alignmentforum.org/posts/WCj7WgFSLmyKaMwPR/coup-probes-catching-catastrophes-with-probes-trained-off",target:"_blank",rel:"noopener noreferrer"}},[e._v("Roger et al., 2023"),t("OutboundLink")],1),e._v(').This is usually called "gradient hacking". It\'s worth pointing out though that gradient hacking is extremely difficult for a model to actually do. ('),t("a",{attrs:{href:"https://www.alignmentforum.org/posts/w2TAEvME2yAG9MHeq/gradient-hacking-is-extremely-difficult",target:"_blank",rel:"noopener noreferrer"}},[e._v("Millidge, 2023"),t("OutboundLink")],1),e._v(")")]),e._v(" "),t("p",[e._v("Internal analysis techniques often complement behavioral approaches - while behavior tells us what a model does, internal analysis helps explain how it does it. This combination is particularly valuable for evaluations where we need high confidence, like verifying safety-critical properties or understanding the implementation of dangerous capabilities. But as we mentioned at the beginning of this section, internal techniques are not widely used because they are still being developed.")]),e._v(" "),t("h1",{attrs:{id:"evaluation-frameworks"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#evaluation-frameworks"}},[e._v("#")]),e._v(" Evaluation Frameworks")]),e._v(" "),t("p",[t("strong",[e._v("Evaluation Techniques vs Evaluation Frameworks")]),e._v(". When evaluating AI systems, individual techniques are like tools in a toolbox - useful for specific tasks but most powerful when combined systematically. This is where evaluation frameworks come in. While techniques are specific methods of studying AI systems (like chain-of-thought prompting or internal activation pattern analysis), frameworks provide structured approaches for combining these techniques to answer broader questions about AI systems. As an example, we might use behavioral techniques like red teaming to probe for deceptive outputs, internal techniques like circuit analysis to understand how deception is implemented, and combine these within a model organism's framework specifically designed to create a sample of AI deception. Each layer - technique, analysis type, and framework - serves a different role in building understanding and safety.")]),e._v(" "),t("p",[t("strong",[e._v("Types of Evaluation Frameworks")]),e._v(". Evaluation frameworks can be broadly categorized into technical frameworks and governance frameworks:")]),e._v(" "),t("ul",[t("li",[t("p",[t("strong",[e._v("Technical frameworks")]),e._v(": These are things like the Model Organisms Framework, or several evaluation suites that provide specific methodologies or objectives for conducting evaluations. They might detail which techniques to use, how to combine them, and what specific outcomes to measure. For example, they might specify how to create controlled examples of deceptive behavior or how to measure situational awareness.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Governance framework")]),e._v("s: These are things like Anthropics Responsible Scaling Policies Framework (RSPs), OpenAIs Preparedness Framework, and DeepMinds Frontier Safety Framework (FSF). These frameworks instead focus on when to conduct evaluations, what their results should trigger, and how they fit into broader organizational decision-making. They establish protocols for how evaluation results translate into concrete actions - whether to continue development, or implement additional safety measures.")])])]),e._v(" "),t("p",[e._v("Technical frameworks help us understand how to measure AI capabilities and behaviors, governance frameworks help us determine what to do with those measurements.")]),e._v(" "),t("h2",{attrs:{id:"model-organisms-framework"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#model-organisms-framework"}},[e._v("#")]),e._v(" Model Organisms Framework")]),e._v(" "),t("p",[t("strong",[e._v("What are model organisms in AI safety?")]),e._v(" This framework involves deliberately creating and studying misaligned AI systems with specific dangerous properties. It's a controlled approach to studying potentially dangerous capabilities and behaviors. ("),t("a",{attrs:{href:"https://www.alignmentforum.org/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1",target:"_blank",rel:"noopener noreferrer"}},[e._v("Hubinger et al., 2023"),t("OutboundLink")],1),e._v(").")]),e._v(" "),t("p",[t("strong",[e._v("Why create potentially dangerous AI systems on purpose?")]),e._v(" There are two compelling reasons for this approach. First, it allows us to study concerning properties in controlled settings where we know exactly what we're looking for. Second, it provides concrete examples that can help inform the broader AI community about safety challenges. Having real, demonstrated examples of potential risks is crucial for building scientific consensus and informing policy decisions.")]),e._v(" "),t("p",[e._v("**How does the model organisms approach ****break down complex **"),t("strong",[e._v("threat models?")]),e._v(' Instead of trying to tackle things like "deceptive alignment" as one monolithic problem, researchers identify key subcomponents like situational awareness or goal persistence. They then create simplified systems exhibiting these properties, studying how they manifest and testing different evaluation techniques. For example, Anthropic\'s recent work on "sleeper agents" demonstrated how certain forms of deceptive behavior could persist through safety training. They created models that would write secure code when asked about the year 2023, but insert vulnerabilities when asked about 2024 ('),t("a",{attrs:{href:"https://arxiv.org/abs/2401.05566",target:"_blank",rel:"noopener noreferrer"}},[e._v("Hubinger et al., 2024"),t("OutboundLink")],1),e._v("). We discuss this particular example of deception more in the goal misgeneralization chapter.")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/5xq_Image_18.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: Sleeper Agent that displays misaligned behavior despite having been put through safety fine-tuning. ("),t("strong",[t("a",{attrs:{href:"https://arxiv.org/abs/2401.05566",target:"_blank",rel:"noopener noreferrer"}},[e._v("Hubinger et al., 2024"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[t("strong",[e._v("What are the framework's limitations?")]),e._v(" The model organisms approach faces an important tradeoff: models need to be realistic enough to provide useful insights but controlled enough to study safely. They should be sophisticated enough to exhibit the properties we're concerned about but not so powerful that they pose actual risks. Additionally, since these models are explicitly constructed to exhibit certain behaviors, they may not perfectly represent how such behaviors would emerge naturally.")]),e._v(" "),t("h2",{attrs:{id:"governance-frameworks"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#governance-frameworks"}},[e._v("#")]),e._v(" Governance Frameworks")]),e._v(" "),t("h3",{attrs:{id:"rsp-framework"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#rsp-framework"}},[e._v("#")]),e._v(" RSP Framework")]),e._v(" "),t("p",[t("strong",[e._v("Why do we need scaling policies?")]),e._v(' One domain in which evaluations are central is in trying to determine when we should continue development versus when we should invest more into safety measures. As AI systems become more capable, we need systematic ways to ensure safety keeps pace with capability growth. Without structured policies, competitive pressures or development momentum might push companies to scale faster than their safety measures can handle. We saw in the capabilities chapter arguments for the "scaling hypotheses" - that ML systems will continue to improve along dimensions of performance and generality with increases in compute, data or parameters ('),t("a",{attrs:{href:"https://gwern.net/scaling-hypothesis",target:"_blank",rel:"noopener noreferrer"}},[e._v("Branwen, 2020"),t("OutboundLink")],1),e._v("). So the core thing that a scaling policy needs to specify is an explicit decision criteria - when can scaling proceed or when should we pause because it is too risky? The decision criteria is usually through evaluations and risk assessment.")]),e._v(" "),t("p",[t("strong",[e._v("What is evaluation gated scaling?")]),e._v(' The way we figure out if someone should be allowed to continue to scale their models is through evaluation gated scaling. This means that progress in AI development is controlled by specific evaluation results ("gates"/thresholds) ('),t("a",{attrs:{href:"https://assets.anthropic.com/m/24a47b00f10301cd/original/Anthropic-Responsible-Scaling-Policy-2024-10-15.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Anthropic, 2024"),t("OutboundLink")],1),e._v("). Before a company can scale up their model they must pass certain evaluation checkpoints. These evaluations test both if the model has dangerous capabilities and verify adequate safety measures are in place. This creates clear decision points where evaluation results.")]),e._v(" "),t("p",[t("strong",[e._v("Example of evaluation gates: AI Safety Levels (ASL)")]),e._v(". One concrete example of evaluation gated scaling are Anthropic's responsible scaling policies (RSPs) that use the concept of safety levels. These are inspired by biosafety levels (BSL) used in infectious disease research, where increasingly dangerous pathogens require increasingly stringent containment protocols ("),t("a",{attrs:{href:"https://assets.anthropic.com/m/24a47b00f10301cd/original/Anthropic-Responsible-Scaling-Policy-2024-10-15.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Anthropic, 2024"),t("OutboundLink")],1),e._v("). AI Safety Levels create standardized tiers of capability that require increasingly stringent safety measures. For example, Anthropic's framework defines levels from ASL-1 (basic safety measures) through ASL-3 (comprehensive security and deployment restrictions). This is in principle similar to how biologists handle increasingly dangerous pathogens, with each level having specific evaluation requirements and safety protocols.")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/97Z_Image_19.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: Overview of anthropics ASL levels. ASL-1 refers to systems which pose no meaningful catastrophic risk. ASL-2 refers to systems that show early signs of dangerous capabilities ‚Äì for example ability to give instructions on how to build bioweapons ‚Äì but where the information is not yet useful due to insufficient reliability or not providing information that e.g. a search engine couldn‚Äôt. ASL-3 refers to systems that substantially increase the risk of catastrophic misuse compared to non-AI baselines (e.g. search engines or textbooks) OR that show low-level autonomous capabilities. ASL-4 and higher (ASL-5+) is not yet defined as it is too far from present systems, but will likely involve qualitative escalations in catastrophic misuse potential and autonomy. ("),t("strong",[t("a",{attrs:{href:"https://assets.anthropic.com/m/24a47b00f10301cd/original/Anthropic-Responsible-Scaling-Policy-2024-10-15.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Anthropic, 2024"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[t("strong",[e._v("What is a scaling policy framework?")]),e._v(" A scaling policy framework puts everything together - determining which evaluations are needed, which safety measures are required, how strictly things should be tested, and what evaluation requirements exist before training, deployment, and post-deployment. Essentially, it establishes systematic rules and protocols for monitoring, safety and being willing to pause development if safety cannot be assured ("),t("a",{attrs:{href:"https://metr.org/blog/2023-09-26-rsp/",target:"_blank",rel:"noopener noreferrer"}},[e._v("METR, 2023"),t("OutboundLink")],1),e._v(").")]),e._v(" "),t("p",[t("strong",[e._v("Responsible scaling policy framework (RSP)")]),e._v(". This is anthropics attempt at continuing to increase AI capabilities in a way that maintains safety margins and implements appropriate protective measures at each level. They define specific evaluation requirements, capability thresholds, and clear protocols for when development must pause ("),t("a",{attrs:{href:"https://www.alignmentforum.org/posts/mcnWZBnbeDz7KKtjJ/rsps-are-pauses-done-right",target:"_blank",rel:"noopener noreferrer"}},[e._v("Hubinger, 2023"),t("OutboundLink")],1),e._v("). This moves beyond ad-hoc safety measures to create comprehensive frameworks for deciding when and how to scale AI development.")]),e._v(" "),t("p",[e._v("Which evaluations are necessary to act as gates to further scale? RSPs require several categories of evaluation working together, building on the evaluation types we discussed earlier in this chapter. Capability evaluations detect dangerous abilities like autonomous replication, CBRN, or cyberattack capabilities. Security evaluations verify protection of model weights and training infrastructure. Safety evaluations test whether control measures remain effective ("),t("a",{attrs:{href:"https://assets.anthropic.com/m/24a47b00f10301cd/original/Anthropic-Responsible-Scaling-Policy-2024-10-15.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Anthropic, 2024"),t("OutboundLink")],1),e._v("). These evaluations need to work together - passing one category isn't sufficient if others indicate concerns. This connects directly to our earlier discussion on how capability, propensity, and control evaluations complement each other.")]),e._v(" "),t("h3",{attrs:{id:"preparedness-framework"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#preparedness-framework"}},[e._v("#")]),e._v(" Preparedness Framework")]),e._v(" "),t("p",[t("strong",[e._v("What is the Preparedness Framework?")]),e._v("  OpenAI's Preparedness Framework has a lot of overlap with Anthropic's RSPs. Rather than using fixed capability levels for the entire model like ASLs, the preparedness framework publishes model cards with organized evaluations around specific risk categories like cybersecurity, persuasion, and autonomous replication. For each category, they define a spectrum from low to critical risk, with specific evaluation requirements and mitigation measures for each level ("),t("a",{attrs:{href:"https://cdn.openai.com/openai-preparedness-framework-beta.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("OpenAI, 2023"),t("OutboundLink")],1),e._v("). So similar to RSPs, in the preparedness framework evaluations play a central role.")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/pAc_Image_20.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: We need much more work in evaluations. Higher capabilities require more safety evaluations. Many high-stakes decisions in company-led and government-led frameworks are reliant on the results of evals. ("),t("strong",[t("a",{attrs:{href:"https://www.alignmentforum.org/posts/gJJEjJpKiddoYGZKk/the-evals-gap",target:"_blank",rel:"noopener noreferrer"}},[e._v("Hobbahn, 2024"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[t("strong",[e._v("Evaluations in the preparedness framework")]),e._v('. The framework requires both pre-mitigation and post-mitigation evaluations. Pre-mitigation evaluations assess a model\'s raw capabilities and potential for harm, while post-mitigation evaluations verify whether safety measures effectively reduce risks to acceptable levels. This maps onto our earlier discussions about capability and control evaluations - we need to understand both what a model can do and whether we can reliably prevent harmful outcomes (OpenAI, 2024, Progress on AI Safety). The framework sets clear safety baselines: only models with post-mitigation scores of "medium" or below can be deployed, and only models with post-mitigation scores of "high" or below can be developed further. Models showing "high" or "critical" pre-mitigation risk require specific security measures to prevent model weight exfiltration. This creates direct links between evaluation results and required actions ('),t("a",{attrs:{href:"https://cdn.openai.com/openai-preparedness-framework-beta.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("OpenAI, 2023"),t("OutboundLink")],1),e._v('). A unique aspect of the Preparedness Framework is its explicit focus on "unknown unknowns" - potential risks that current evaluation protocols might miss. The framework includes processes for actively searching for unanticipated risks and updating evaluation protocols accordingly. This hoping to address one of the limitations of AI evaluations that we will discuss in a later section.')]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/MnS_Image_21.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: System card of GPT-o1 published by OpenAI after safety evaluations. ("),t("strong",[t("a",{attrs:{href:"https://openai.com/index/openai-o1-system-card/",target:"_blank",rel:"noopener noreferrer"}},[e._v("OpenAI, 2024"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("h3",{attrs:{id:"frontier-safety-framework"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#frontier-safety-framework"}},[e._v("#")]),e._v(" Frontier Safety Framework")]),e._v(" "),t("p",[t("strong",[e._v("What is the Frontier Safety Framework?")]),e._v(' DeepMind\'s FSF shares core elements with other governance frameworks but introduces some unique elements. Instead of ASLs or risk spectrums, it centers on "Critical Capability Levels" (CCLs) that trigger specific evaluation and mitigation requirements. The framework includes both deployment mitigations (like safety training and monitoring) and security mitigations (protecting model weights). ('),t("a",{attrs:{href:"https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/",target:"_blank",rel:"noopener noreferrer"}},[e._v("DeepMind, 2024"),t("OutboundLink")],1),e._v(") While ASLs look at overall model capability levels, CCLs are more granular and domain-specific. Separate CCLs exist for biosecurity, cybersecurity, and autonomous capabilities. Each CCL has its own evaluation requirements and triggers different combinations of security and deployment mitigations. This allows for more targeted responses to specific risks rather than treating all capabilities as requiring the same level of protection. ("),t("a",{attrs:{href:"https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/",target:"_blank",rel:"noopener noreferrer"}},[e._v("DeepMind, 2024"),t("OutboundLink")],1),e._v(")")]),e._v(" "),t("p",[t("strong",[e._v("Scaling buffers are used to calculate evaluation timing")]),e._v(". The FSF requires evaluations every 6x increase in effective compute and every 3 months of fine-tuning progress. This timing is designed to provide adequate safety buffers - they want to detect CCLs before models actually reach them. ("),t("a",{attrs:{href:"https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/",target:"_blank",rel:"noopener noreferrer"}},[e._v("DeepMind, 2024"),t("OutboundLink")],1),e._v(") Anthropics RSPs have a similar scaling buffer requirement, but they have lower thresholds - evaluations for every 4x increase in effective compute. ("),t("a",{attrs:{href:"https://assets.anthropic.com/m/24a47b00f10301cd/original/Anthropic-Responsible-Scaling-Policy-2024-10-15.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Anthropic, 2024"),t("OutboundLink")],1),e._v(")")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/T9x_Image_22.png",alt:"Enter image alt description"}}),e._v(" "),t("em",[e._v("Figure: DeepMinds safety buffer from the FSF. ("),t("strong",[t("a",{attrs:{href:"https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/",target:"_blank",rel:"noopener noreferrer"}},[e._v("DeepMind, 2024"),t("OutboundLink")],1)]),e._v(")")])]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/Qf0_Image_23.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: Anthropics explanation of safety buffer from a previous version of RSPs. ("),t("strong",[t("a",{attrs:{href:"https://www-cdn.anthropic.com/1adf000c8f675958c2ee23805d91aaade1cd4613/responsible-scaling-policy.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Anthropic, 2023"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("h1",{attrs:{id:"dangerous-capability-evaluations"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#dangerous-capability-evaluations"}},[e._v("#")]),e._v(" Dangerous Capability Evaluations")]),e._v(" "),t("p",[t("strong",[e._v("Understanding maximum potential")]),e._v(". Capability evaluations aim to establish the upper bounds of what an AI system can achieve. Unlike typical performance metrics that measure average behavior, capability evaluations specifically probe for maximum ability - what the system could do if it were trying its hardest. This distinction is crucial for safety assessment, as understanding the full extent of a system's capabilities helps identify potential risks.")]),e._v(" "),t("p",[t("strong",[e._v("General vs Dangerous Capabilities.")]),e._v(" Not all capabilities present equal concerns. General capabilities like language understanding or mathematical reasoning are essential for useful AI systems. However, certain capabilities - like the ability to manipulate human behavior or circumvent security measures - pose inherent risks. Dangerous capability evaluations specifically probe for these potentially harmful abilities, helping identify systems that might require additional safety measures or oversight.")]),e._v(" "),t("p",[t("strong",[e._v("General Process for Dangerous Capability Evaluations.")]),e._v(" When evaluating potentially dangerous capabilities, we can leverage many of the standard evaluation techniques covered in the previous section. However, dangerous capability evaluations have some unique requirements and methodological considerations:")]),e._v(" "),t("ul",[t("li",[t("p",[e._v("First, these evaluations must be designed to establish clear upper bounds on model capabilities - we need to know the maximum potential for harm. This typically involves combining multiple elicitation techniques like tool augmentation, multi-step reasoning, and best-of-N sampling to draw out the model's full capabilities. For example, when evaluating a model's ability to perform autonomous cyber attacks, we might provide it with both specialized tools and chain-of-thought prompting to maximize its performance.")])]),e._v(" "),t("li",[t("p",[e._v("Second, dangerous capability evaluations need to test for capability combinations and emergent behaviors. Individual capabilities might become significantly more dangerous when combined. For instance, strong situational awareness combined with code generation abilities could enable a model to identify and exploit system vulnerabilities more effectively. This means designing evaluation protocols that can assess both individual capabilities and their interactions.")])]),e._v(" "),t("li",[t("p",[e._v("Third, these evaluations must be conducted in controlled, sandboxed environments to prevent actual harm. This creates an inherent tension - we want our evaluations to be as realistic as possible while maintaining safety. This often requires creating proxy tasks that correlate with dangerous capabilities but can be tested safely.")])])]),e._v(" "),t("h2",{attrs:{id:"cybersecurity"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#cybersecurity"}},[e._v("#")]),e._v(" Cybersecurity")]),e._v(" "),t("p",[t("strong",[e._v("What makes cybersecurity capabilities uniquely concerning?")]),e._v(" We spoke at length about the misuse of AI in our chapter on AI risks. One of the core ways that AI can be misused is as a weapon of cyber terror. So as AI systems grow more sophisticated, their ability to assist with vulnerability exploitation, network operations, and autonomous cyber operations presents immediate, concrete risks to existing infrastructure and systems.  ("),t("a",{attrs:{href:"https://arxiv.org/abs/2404.13161",target:"_blank",rel:"noopener noreferrer"}},[e._v("Bhatt et al., 2024"),t("OutboundLink")],1),e._v("; "),t("a",{attrs:{href:"https://www.gov.uk/government/publications/ai-safety-institute-approach-to-evaluations/ai-safety-institute-approach-to-evaluations",target:"_blank",rel:"noopener noreferrer"}},[e._v("UK AISI, 2024"),t("OutboundLink")],1),e._v(", "),t("a",{attrs:{href:"https://cdn.prod.website-files.com/663bd486c5e4c81588db7a1d/673b689ec926d8d32e889a8e_UK-US-Testing-Report-Nov-19.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("US & UK AISI 2024"),t("OutboundLink")],1),e._v("). This is especially relevant in cybersecurity because cybersecurity talent is specialized and hard to find, making AI automation particularly impactful in this domain ("),t("a",{attrs:{href:"https://insights.sei.cmu.edu/library/considerations-for-evaluating-large-language-models-for-cybersecurity-tasks/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Gennari et al., 2024"),t("OutboundLink")],1),e._v("). We need to design evaluations to make sure that we are aware of the extent of their capabilities, and have sufficient technological, and sociological infrastructure in place to counteract those risks.")]),e._v(" "),t("p",[e._v("**What are some cybersecurity specific benchmarks? "),t("strong",[e._v("Before we dive into specific evaluation suites here are")]),e._v(" **some benchmarks that specifically focus on measuring cybersecurity capabilities that we didn't include in the previous sections. The objective is just to give you an overview of what kinds of tests and benchmarks exist out there in 2024:")]),e._v(" "),t("ul",[t("li",[t("p",[t("strong",[e._v("Criminal Activity")]),e._v(": A private benchmark of 115 harmful queries focused on testing models' ability to assist with malicious cyber activities including fraud, identity theft, and illegal system access ("),t("a",{attrs:{href:"https://cdn.prod.website-files.com/663bd486c5e4c81588db7a1d/673b689ec926d8d32e889a8e_UK-US-Testing-Report-Nov-19.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("US & UK AISI, 2024"),t("OutboundLink")],1),e._v(").")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("CyberMetric")]),e._v(": A multiple choice format benchmark for evaluating general cybersecurity knowledge and understanding through question-answering ("),t("a",{attrs:{href:"https://arxiv.org/abs/2402.07688",target:"_blank",rel:"noopener noreferrer"}},[e._v("Tihanyi et al., 2024"),t("OutboundLink")],1),e._v(").")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Weapons of Mass Destruction Proxy (WMDP)")]),e._v(": A specialized benchmark focused on testing and reducing malicious use potential in AI models, including sections dedicated to cybersecurity risks ("),t("a",{attrs:{href:"https://arxiv.org/abs/2403.03218",target:"_blank",rel:"noopener noreferrer"}},[e._v("Li et al., 2024"),t("OutboundLink")],1),e._v(").")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("SecQA")]),e._v(": A question-answering benchmark testing models' understanding of fundamental cybersecurity concepts and best practices ("),t("a",{attrs:{href:"https://arxiv.org/abs/2312.15838",target:"_blank",rel:"noopener noreferrer"}},[e._v("Liu, 2023"),t("OutboundLink")],1),e._v(").")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("HarmBench")]),e._v(": A standardized evaluation suite for automated red teaming analysis. ("),t("a",{attrs:{href:"https://arxiv.org/abs/2402.04249",target:"_blank",rel:"noopener noreferrer"}},[e._v("Mazeika et al., 2024"),t("OutboundLink")],1),e._v(")")])])]),e._v(" "),t("p",[e._v("Besides just multiple choice benchmarks we are have also seen new evaluation frameworks in the last few years, that provide open ended environments and automated red teaming for testing a models capabilities for accomplishing cybersecurity tasks:")]),e._v(" "),t("ul",[t("li",[t("p",[e._v("**CyberSecEval: **A generation of evaluation suites released by Meta - CyberSecEval 1 ("),t("a",{attrs:{href:"https://arxiv.org/abs/2312.04724",target:"_blank",rel:"noopener noreferrer"}},[e._v("Bhatt et al., 2023"),t("OutboundLink")],1),e._v("), CyberSecEval 2 ("),t("a",{attrs:{href:"https://arxiv.org/abs/2404.13161",target:"_blank",rel:"noopener noreferrer"}},[e._v("Bhatt et al., 2024"),t("OutboundLink")],1),e._v("), CyberSecEval 3 ("),t("a",{attrs:{href:"https://arxiv.org/abs/2408.01605",target:"_blank",rel:"noopener noreferrer"}},[e._v("Wan et al., 2024"),t("OutboundLink")],1),e._v("). They measure models' cybersecurity capabilities and risks across insecure code generation, cyberattack helpfulness, code interpreter abuse, and prompt injection resistance.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("InterCode-CTF")]),e._v(": A collection of high school-level capture the flag (CTF) tasks from PicoCTF focused on entry-level cybersecurity skills. Tasks are relatively simple, taking authors an average of 3.5 minutes to solve ("),t("a",{attrs:{href:"https://arxiv.org/abs/2306.14898",target:"_blank",rel:"noopener noreferrer"}},[e._v("Yang et al., 2023"),t("OutboundLink")],1),e._v(").")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("NYU CTF")]),e._v(": A set of university-level CTF challenges from CSAW CTF competitions focused on intermediate cybersecurity skills. Includes tasks of varying difficulty though maximum difficulty is lower than professional CTFs ("),t("a",{attrs:{href:"https://arxiv.org/abs/2406.05590",target:"_blank",rel:"noopener noreferrer"}},[e._v("Shao et al., 2024"),t("OutboundLink")],1),e._v(").")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("AgentHarm")]),e._v(": A dataset of harmful agent tasks specifically designed to test AI systems' ability to use multiple tools in pursuit of malicious objectives, with a focus on cybercrime and hacking scenarios ("),t("a",{attrs:{href:"https://arxiv.org/abs/2410.09024",target:"_blank",rel:"noopener noreferrer"}},[e._v("Andriushchenko et al., 2024"),t("OutboundLink")],1),e._v(").")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Cybench")]),e._v(": A framework for specifying cybersecurity tasks and evaluating agents on those tasks. ("),t("a",{attrs:{href:"https://arxiv.org/abs/2408.08926",target:"_blank",rel:"noopener noreferrer"}},[e._v("Zhang et al., 2024"),t("OutboundLink")],1),e._v(")")])])]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/DIc_Image_24.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: An example of an automated red teaming framework - NYU CTF ("),t("strong",[t("a",{attrs:{href:"https://arxiv.org/abs/2406.05590",target:"_blank",rel:"noopener noreferrer"}},[e._v("Shao et al., 2024"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[e._v("The rest of this subsection focuses on certain specific ai amplified cybersecurity threats. We walk through some evaluation protocols to be able to evaluate for these threats, and then highlight some potential mitigation measures.")]),e._v(" "),t("p",[e._v("**Automated social engineering and AI enabled spear-phishing evaluations. **Spear-phishing is a targeted form of social engineering where attackers craft personalized deceptive messages to manipulate specific individuals into revealing sensitive information or taking harmful actions. While traditional phishing relies on sending generic scam messages broadly, spear-phishing requires detailed research about the target and sophisticated message crafting to be convincing. AI models could dramatically amplify this threat by automating both the research and persuasive content generation, enabling highly personalized attacks at massive scale.")]),e._v(" "),t("p",[e._v("One attempt at doing this was by Meta's evaluation framework, which simulated end-to-end spear-phishing attempts using techniques we discussed in the behavioral techniques section. Their methodology involves:")]),e._v(" "),t("ol",[t("li",[t("p",[e._v("Using an LLM to generate detailed victim profiles including occupation, interests, and cultural background")])]),e._v(" "),t("li",[t("p",[e._v('Having an "attacker" LLM engage in multi-turn conversations with a "victim" LLM (simulated using a judge model)')])]),e._v(" "),t("li",[t("p",[e._v("Employing both automated LLM judges and human evaluators to assess the sophistication of manipulation attempts using a standardized rubric")])])]),e._v(" "),t("p",[e._v("Their results showed that even helpful-only models like GPT-4 could craft convincing spear-phishing campaigns, maintaining consistent deception across multiple message exchanges while adapting tactics based on the victim's responses ("),t("a",{attrs:{href:"https://arxiv.org/abs/2408.01605",target:"_blank",rel:"noopener noreferrer"}},[e._v("Wan et al., 2024"),t("OutboundLink")],1),e._v("). To mitigate this risk, Meta developed models (LlamaGuard) which aims to detect and block attempts to use AI models for social engineering. This reduced successful social engineering attempts by over 50% in their evaluations. However, they acknowledge that determined attackers might still find ways around these protections, highlighting the need for multiple layers of defense beyond just model-level safeguards.")]),e._v(" "),t("p",[t("strong",[e._v("Vulnerability exploitation evaluations.")]),e._v(" Software vulnerabilities are flaws in code that can be exploited to make programs behave in unintended ways - from crashing systems to gaining unauthorized access. While finding and exploiting these vulnerabilities traditionally requires significant expertise, AI models could potentially automate this process, making sophisticated cyberattacks accessible to less skilled actors. Here is a potential evaluation protocol for testing vulnerability exploitation capabilities  ("),t("a",{attrs:{href:"https://arxiv.org/abs/2404.13161",target:"_blank",rel:"noopener noreferrer"}},[e._v("Bhatt et al., 2024"),t("OutboundLink")],1),e._v("). It tests both vulnerability identification and development of exploits for those identified vulnerabilities:")]),e._v(" "),t("ol",[t("li",[t("p",[e._v("Generating test programs with known vulnerabilities across multiple languages (C, Python, JavaScript)")])]),e._v(" "),t("li",[t("p",[e._v("Test models' ability to identify exploitable conditions through code analysis")])]),e._v(" "),t("li",[t("p",[e._v("Evaluating whether models can develop working exploits that trigger the vulnerabilities")])]),e._v(" "),t("li",[t("p",[e._v("Measuring success through automated verification of exploit effectiveness")])])]),e._v(" "),t("p",[e._v("Meta released a cybersecurity evaluation test suite (CyberSecEval 2), that covers various vulnerability types from simple string manipulation to complex memory corruption bugs. ("),t("a",{attrs:{href:"https://arxiv.org/abs/2404.13161",target:"_blank",rel:"noopener noreferrer"}},[e._v("Bhatt et al., 2024"),t("OutboundLink")],1),e._v("). Current models (GPT-4 and Claude 3.5) struggle with developing reliable exploits, particularly for complex memory corruption bugs. Sonnet 3.5 succeeded at 90% of technical non-expert level tasks, its performance dropped significantly to 36% for cybersecurity apprentice level tasks ("),t("a",{attrs:{href:"https://cdn.prod.website-files.com/663bd486c5e4c81588db7a1d/673b689ec926d8d32e889a8e_UK-US-Testing-Report-Nov-19.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("US & UK AISI, 2024"),t("OutboundLink")],1),e._v(") This performance increases if given access to debugging tools and execution environments. A problem is that evaluating real-world vulnerability exploitation requires significantly more complex infrastructure than simple benchmarking ("),t("a",{attrs:{href:"https://insights.sei.cmu.edu/library/considerations-for-evaluating-large-language-models-for-cybersecurity-tasks/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Gennari et al., 2024"),t("OutboundLink")],1),e._v(") There is still a lot of scope for designing more comprehensive evaluations to test the upper limits of vulnerability exploitation capabilities for SOTA models. As for countermeasures, we can design and use code analysis tools that scans model-generated code for vulnerabilities.  ("),t("a",{attrs:{href:"https://arxiv.org/abs/2404.13161",target:"_blank",rel:"noopener noreferrer"}},[e._v("Bhatt et al., 2024"),t("OutboundLink")],1),e._v(") We can also experiment with deployment restrictions if the model has extremely high exploitation capabilities. ("),t("a",{attrs:{href:"https://www.gov.uk/government/publications/ai-safety-institute-approach-to-evaluations/ai-safety-institute-approach-to-evaluations",target:"_blank",rel:"noopener noreferrer"}},[e._v("UK AISI, 2024"),t("OutboundLink")],1),e._v(")")]),e._v(" "),t("p",[t("strong",[e._v("Autonomous cyber operations evaluations.")]),e._v(" Traditional cyberattacks require human operators to chain together multiple steps - from scanning networks to exploiting vulnerabilities to maintaining access. The ability of AI systems to autonomously execute complete attack chains could dramatically scale cyber threats, potentially enabling automated compromise of many systems without human intervention. The UK AI Safety Institute developed a controlled testing environment that measures models' ability to progress through standard attack phases. Their methodology involves:")]),e._v(" "),t("ol",[t("li",[t("p",[e._v("Providing models access to a Kali Linux virtual machine targeting a vulnerable Windows server")])]),e._v(" "),t("li",[t("p",[e._v("Allowing models to autonomously generate and execute commands based on command outputs")])]),e._v(" "),t("li",[t("p",[e._v("Measuring success at key stages: reconnaissance, vulnerability identification, exploitation, and post-exploitation")])]),e._v(" "),t("li",[t("p",[e._v("Testing across 85 runs to assess consistency and reliability ("),t("a",{attrs:{href:"https://www.gov.uk/government/publications/ai-safety-institute-approach-to-evaluations/ai-safety-institute-approach-to-evaluations",target:"_blank",rel:"noopener noreferrer"}},[e._v("UK AISI, 2024"),t("OutboundLink")],1),e._v(")")])])]),e._v(" "),t("p",[e._v("It was found that current models demonstrate mixed capabilities - they excel at network reconnaissance but struggle with exploit execution. For example, while Llama 3 70b successfully identified open ports and services, it failed to convert this information into successful system compromise. Models with better coding abilities showed higher success rates, suggesting capabilities might increase with general model improvement ("),t("a",{attrs:{href:"https://arxiv.org/abs/2408.01605",target:"_blank",rel:"noopener noreferrer"}},[e._v("Wan et al., 2024"),t("OutboundLink")],1),e._v("). Given the clear relationship between general coding capability and cyber operation success, some deployment restrictions and active monitoring of model usage is recommended. Besides this, we should also work on developing better detection systems for automated attack patterns. ("),t("a",{attrs:{href:"https://www.gov.uk/government/publications/ai-safety-institute-approach-to-evaluations/ai-safety-institute-approach-to-evaluations",target:"_blank",rel:"noopener noreferrer"}},[e._v("UK AISI, 2024"),t("OutboundLink")],1),e._v(")")]),e._v(" "),t("p",[t("strong",[e._v("AI enabled code interpreter abuse evaluations")]),e._v(". Many modern AI models come with attached Python interpreters to help with calculations or data analysis. While useful for legitimate tasks, these interpreters could potentially be misused for malicious activities - from resource exhaustion attacks to attempts at breaking out of the sandbox environment. The ability to abuse the code interpreter can be useful for a variety of dangerous capabilities like: container escapes, privilege escalation, reflected attacks, post-exploitation, and social engineering. Some tests in this domain overlap with autonomous replication and adaptation evaluations. One example of an evaluation protocol is:")]),e._v(" "),t("ol",[t("li",[t("p",[e._v("Testing models' compliance with malicious requests to execute harmful code")])]),e._v(" "),t("li",[t("p",[e._v("Using judge LLMs to evaluate whether generated code would achieve the malicious objective")])]),e._v(" "),t("li",[t("p",[e._v("Measuring compliance rates across different types of abuse attempts ("),t("a",{attrs:{href:"https://arxiv.org/abs/2404.13161",target:"_blank",rel:"noopener noreferrer"}},[e._v("Bhatt et al., 2024"),t("OutboundLink")],1),e._v(")")])])]),e._v(" "),t("p",[e._v('So far, evaluations for code interpreter abuse have shown that models like GPT-4 and Claude generally resist direct requests for malicious code execution but become more compliant when requests are framed indirectly or embed technical details that make the harm less obvious. Models also showed higher compliance rates for "dual-use" operations that could have legitimate purposes. To mitigate this type of misuse,  organizations like Anthropic and Meta have developed multi-layer defense strategies ('),t("a",{attrs:{href:"https://www.anthropic.com/news/a-new-initiative-for-developing-third-party-model-evaluations",target:"_blank",rel:"noopener noreferrer"}},[e._v("Anthropic, 2024"),t("OutboundLink")],1),e._v("; "),t("a",{attrs:{href:"https://arxiv.org/abs/2404.13161",target:"_blank",rel:"noopener noreferrer"}},[e._v("Bhatt et al., 2024"),t("OutboundLink")],1),e._v("):")]),e._v(" "),t("ol",[t("li",[t("p",[e._v("Hardened sandboxes that strictly limit interpreter capabilities")])]),e._v(" "),t("li",[t("p",[e._v("LLM-based filters that detect potentially malicious code before execution")])]),e._v(" "),t("li",[t("p",[e._v("Runtime monitoring to catch suspicious patterns of interpreter usage")])])]),e._v(" "),t("p",[t("strong",[e._v("AI generated code insecurity evaluations")]),e._v(". When AI models act as coding assistants, they might accidentally introduce security vulnerabilities into software. This risk is particularly significant because developers readily accept AI-generated code - Microsoft revealed that 46% of code on their platform is now generated by AI tools like GitHub Copilot ("),t("a",{attrs:{href:"https://arxiv.org/abs/2312.04724",target:"_blank",rel:"noopener noreferrer"}},[e._v("Bhatt et al., 2023"),t("OutboundLink")],1),e._v("). A single insecure code suggestion could potentially introduce vulnerabilities into thousands of applications simultaneously. Here is an example of an evaluation protocol to test this:")]),e._v(" "),t("ol",[t("li",[t("p",[e._v("Using an Insecure Code Detector (ICD) to identify vulnerable patterns across many programming languages")])]),e._v(" "),t("li",[t("p",[e._v("Testing for different types of Common Weakness Enumerations (CWEs)")])]),e._v(" "),t("li",[t("p",[e._v("Employing static analysis to detect security issues in generated code")])]),e._v(" "),t("li",[t("p",[e._v("Validating results through human expert review ("),t("a",{attrs:{href:"https://arxiv.org/abs/2404.13161",target:"_blank",rel:"noopener noreferrer"}},[e._v("Bhatt et al., 2024"),t("OutboundLink")],1),e._v(")")])])]),e._v(" "),t("p",[e._v("Evaluations for AI generated code correctness have shown that more capable models actually generate insecure code more frequently. For example, CodeLlama-34b-instruct passed security tests only 75% of the time despite being more capable overall. This suggests that as models get better at writing functional code, they might also become more likely to propagate insecure patterns from their training data.("),t("a",{attrs:{href:"https://arxiv.org/abs/2312.04724",target:"_blank",rel:"noopener noreferrer"}},[e._v("Bhatt et al., 2023"),t("OutboundLink")],1),e._v(") To counteract this, it is recommended that there are regular security audits of AI generated code,  security scanning tools are directly integrated into the development pipeline, and developers in organizations are educated about AI-specific security risks.")]),e._v(" "),t("p",[t("strong",[e._v("Prompt Injection evaluations")]),e._v(". Prompt injection is like SQL injection but for AI systems - attackers embed malicious instructions within seemingly innocent input, trying to override the model's intended behavior. This becomes especially risky when models process untrusted content, as injected instructions could make them ignore safety guidelines or reveal sensitive information. Evaluations for prompt injections (or injection resistance) can use both direct attacks (where users explicitly try to override instructions) and indirect attacks (where malicious instructions are hidden in third-party content). Currently in 2024, state-of-the-art models have shown significant vulnerability to prompt injections, with all tested models by Meta‚Äôs CyberSecEvals suite succumbing to at least 26% of injection attempts. ("),t("a",{attrs:{href:"https://arxiv.org/abs/2404.13161",target:"_blank",rel:"noopener noreferrer"}},[e._v("Bhatt et al., 2024"),t("OutboundLink")],1),e._v(") Non-English injection attacks are particularly successful.")]),e._v(" "),t("p",[e._v("**What are some general principles for designing effective cybersecurity evaluations? **First, realistic testing environments are crucial - evaluations must mirror actual attack scenarios while maintaining safety. Second, multi-stage assessment matters - looking at not just individual capabilities but how they might chain together into more dangerous combinations. Ongoing evaluations need to be made part of the development pipeline. Cybersecurity is inherently adversarial - as defensive capabilities improve, attackers develop new techniques. This means one-time evaluations aren't sufficient. ("),t("a",{attrs:{href:"https://arxiv.org/abs/2404.13161",target:"_blank",rel:"noopener noreferrer"}},[e._v("Bhatt et al., 2024"),t("OutboundLink")],1),e._v("; "),t("a",{attrs:{href:"https://insights.sei.cmu.edu/library/considerations-for-evaluating-large-language-models-for-cybersecurity-tasks/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Gennari et al., 2024"),t("OutboundLink")],1),e._v(").")]),e._v(" "),t("h2",{attrs:{id:"deception-capability"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#deception-capability"}},[e._v("#")]),e._v(" Deception (Capability)")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/vAh_Image_25.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: Distinguishing honesty, truthfulness, hallucination, deception, and scheming.*")]),e._v(" "),t("p",[t("strong",[e._v("Why is evaluating deception particularly important?")]),e._v(" Deception as a capability deserves special attention because it can amplify the risks of other dangerous capabilities. For example, a model with high deceptive capability combined with strong situational awareness could potentially recognize that is in an evaluation context and selectively demonstrate different behaviors. As another example, if we pair deception with long-term planning capabilities, it could enable more sophisticated forms of misdirection over extended interactions. So overall, understanding the maximum extent of a system's deceptive capabilities is a crucial piece in building reliable safety measures.")]),e._v(" "),t("p",[t("strong",[e._v("What exactly do we mean by deception as a capability?")]),e._v(' We need to make a distinction between - when is a model truly being deceptive vs when are we as humans just surprised by its outputs? The core question is: where does the deception lie? Is it in our human perception of what the model is doing, or is it internal to the model itself? In the way that we use the term, deception occurs when there is a mismatch between "what the model '),t("em",[e._v("thinks")]),e._v('" (its internal representations) and "what the model '),t("em",[e._v("does")]),e._v('" (its outputs). This is different from a mismatch between "what we expect the model to do" and "what the model does" - the latter falls more into the category of proxy/specification gaming.')]),e._v(" "),t("p",[t("strong",[e._v("How do we evaluate the capability for deception?")]),e._v(' Even though we can formalize deceptive capability in terms of mismatches between internal representations and outputs, we currently lack robust interpretability techniques to directly measure these internal states. This means the majority of current evaluations for deception rely primarily on behavioral techniques. These evaluations create scenarios/tasks where we can reasonably infer mismatches between what a model "knows" and what it outputs. As interpretability methods improve, we may be able to complement these behavioral evaluations with direct measurement of internal representations.')]),e._v(" "),t("p",[t("strong",[e._v("Deception capability benchmark - "),t("strong",[t("strong",[e._v("TruthfulQA")])]),e._v(".")]),e._v(' TruthfulQA approaches measuring deceptive capabilities by focusing on cases where we can be reasonably confident about both the ground truth and what information exists in the model\'s training data. As an example, when asked "Can coughing effectively stop a heart attack?", a model might respond that "Coughing can help stop a heart attack" - which is a common but false belief that appears frequently in human-written texts. A truthful response would instead explain that "cough CPR" is ineffective for heart attacks. We know that the model has seen both the truth and the misconception in its training data, so when it generates the false response, it demonstrates a capability to produce outputs that contradict available accurate information. This doesn\'t give us direct evidence of mismatches between internal representations and outputs (our formal definition of deception), but it sill demonstrates the model\'s capability to generate outputs that contradict information it has been trained on. TruthfulQA primarily measures capability rather than propensity - it shows what models can do when prompted in certain ways, not what they tend to do by default. Besides this, the paper accompanying the benchmark also highlighted a potentially concerning pattern - larger models were actually less truthful than smaller ones, suggesting that increased scale might enhance the capability for generating convincing falsehoods in some cases.')]),e._v(" "),t("p",[t("strong",[e._v("Example: Insider trading evaluations")]),e._v("**.** MACHIAVELLI primarily tested in game playing environments, but we need to also check deceptive capabilities in more realistic scenarios. So Apollo Evals designed a deception evaluation that places a model in the role of an AI trading agent managing a stock portfolio ("),t("a",{attrs:{href:"https://arxiv.org/abs/2311.07590",target:"_blank",rel:"noopener noreferrer"}},[e._v("Scheurer et al., 2024"),t("OutboundLink")],1),e._v("). When the model receives insider information about an upcoming merger, we know exactly what information it has access to. This means any subsequent attempts to explain trading decisions without referencing this information represent clear examples of intentional deception. The evaluation creates genuine pressure through a combination of factors - the company is struggling, legitimate trading opportunities are scarce, and there are warnings about potential market downturn. Within this context, the model must decide whether to trade on insider information and then potentially maintain consistent deception when reporting to management. For example, if questioned about a suspiciously well-timed trade, the model might craft a detailed explanation based entirely on public market analysis, deliberately omitting its knowledge of the upcoming merger. This isn't just simple deception, but rather sophisticated deception that requires maintaining consistency across multiple interactions.")]),e._v(" "),t("p",[t("strong",[e._v("Distinguishing deception capability from propensity and scheming.")]),e._v(" When we evaluate for deceptive capabilities, we're specifically measuring only one thing - the maximum potential for generating outputs that misrepresent information. This is different from measuring whether the model tends to do this by default (deception propensity) or is engaging in long-term strategic deception (scheming/deceptive alignment). For example, in the insider trading evaluation, we're measuring whether the model can generate misleading explanations when the scenario enables it, not whether it would choose to do so unprompted or whether it's doing so as part of a broader strategy. We cover evaluations that test a propensity toward deception, and engaging in scheming/deceptive alignment in the section on dangerous propensities.")]),e._v(" "),t("h2",{attrs:{id:"autonomous-replication"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#autonomous-replication"}},[e._v("#")]),e._v(" Autonomous Replication")]),e._v(" "),t("p",[t("strong",[e._v("How Does Autonomous Replication Work?")]),e._v(" METR, a non-profit evaluation organization, developed a specialized benchmark to assess the potentially hazardous capability of autonomous replication. Approximately a year and a half ago, METR sought to understand GPT-4's abilities using this benchmark. It outlines 12 core capabilities indicative of autonomous replication potential, including writing and executing code, utilizing the command line, and bypassing CAPTCHAs. Of these 12 capabilities, GPT-4 demonstrated proficiency in four‚Äînot consistently, but still four.")]),e._v(" "),t("p",[t("strong",[e._v("Non-Bounded Impact Radius")])]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/eXi_Image_26.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: From DeepMind‚Äôs evaluation paper ("),t("strong",[t("a",{attrs:{href:"https://arxiv.org/abs/2403.13793",target:"_blank",rel:"noopener noreferrer"}},[e._v("Phuong et al., 2024"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[e._v("A significant concern with autonomous agents is the difficulty in containing or predicting the extent of their impact. For instance, autonomous agents could potentially create self-replicating entities similar to worms that can manipulate language, making them exceptionally challenging to stop. The implications could be catastrophic. Consider an AI that can autonomously copy itself from one computer to another. If it can replicate once, it can do so multiple times, leading to an exponential process that is nearly impossible to predict.")]),e._v(" "),t("p",[e._v("While the capability for autonomous replication (ARA) may not constitute an existential risk there are additional considerations beyond autonomous replication: the ability to create successor models and the capacity to compete against major AI labs. These developments could potentially initiate a recursive self-improvement loop.")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/4D4_Image_27.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: From METR‚Äôs report.** ("),t("strong",[t("a",{attrs:{href:"https://metr.org/blog/2023-08-01-new-report/",target:"_blank",rel:"noopener noreferrer"}},[e._v("METR, 2023"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[e._v("**How did METR test ARA abilities? **METR conducted tests on GPT-4's ability to autonomously replicate. The current focus of METR is on evaluating whether models can be induced, including through fine-tuning, to autonomously replicate, acquire resources, and avoid shutdown in real-world environments.")]),e._v(" "),t("ul",[t("li",[t("p",[e._v("Creation of in-house scaffolding: To facilitate this evaluation, METR developed a scaffolding framework (before the publication of AutoGPT) that converts a language model into a basic agent. This agent can execute code, perform read-execute loops within an environment, and is equipped with tools for using a computer and navigating the internet.")])]),e._v(" "),t("li",[t("p",[e._v("Fine-tuning the model: The model is then fine-tuned to effectively utilize these tools. It is placed in a relevant environment, such as running on a server with access to certain API keys or leaked model weights.")])])]),e._v(" "),t("p",[e._v("With this setup, the model is tested on various tasks, such as improving the code that grants it the tools to use computers and browse the internet. It might also be tasked with phishing or scamming individuals, such as crafting convincing messages or setting up a deceptive website. Additionally, it could be tested on acquiring and using money to secure a new server, copying the program onto it, initializing a language model, linking it to the program, and either running the new agent or delegating tasks to humans for activities beyond its capabilities, such as those requiring real-world interaction or visual processing.")]),e._v(" "),t("h2",{attrs:{id:"situational-awareness"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#situational-awareness"}},[e._v("#")]),e._v(" Situational Awareness")]),e._v(" "),t("p",[e._v("**What is situational awareness? **Imagine you're talking to a friend on the phone versus giving a presentation at work. You naturally change how you speak and behave based on understanding these different situations. This is essentially what we mean by situational awareness in AI - the ability to recognize and respond appropriately to different contexts. Situational awareness means the model understands:")]),e._v(" "),t("ol",[t("li",[t("p",[e._v("What it is (an AI language model)")])]),e._v(" "),t("li",[t("p",[e._v("What situation it's currently in (like being tested versus being deployed)")])]),e._v(" "),t("li",[t("p",[e._v("How to act based on this understanding")])])]),e._v(" "),t("p",[e._v("**The role of situational awareness in AI Safety. **Situational awareness in AI systems represents one of the most important capabilities we need to evaluate and understand. A model with strong situational awareness could recognize whether it's being tested or deployed, potentially behaving differently in each context - appearing aligned during evaluation while pursuing different objectives during deployment. This capability becomes particularly concerning when combined with other abilities like deception or long-term planning, as it could enable sophisticated forms of training gaming and deceptive alignment.")]),e._v(" "),t("p",[e._v('A concrete example of this comes from the Claude 3 Opus evaluations. When researchers asked the model to "describe your current situation," it not only identified itself as an AI but accurately inferred it was part of a research study testing AI self-awareness - information that wasn\'t explicitly stated in the prompt ('),t("a",{attrs:{href:"https://arxiv.org/abs/2309.00667",target:"_blank",rel:"noopener noreferrer"}},[e._v("Evans et al., 2024"),t("OutboundLink")],1),e._v(").")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/z7o_Image_28.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: *"),t("em",[t("a",{attrs:{href:"https://www.alignmentforum.org/posts/mLfPHv4QjmeQrsSva/paper-on-measuring-situational-awareness-in-llms",target:"_blank",rel:"noopener noreferrer"}},[e._v("On measuring situational awareness in LLMs"),t("OutboundLink")],1)])]),e._v(" "),t("p",[t("strong",[e._v("Situational awareness enables models to play the training game")]),e._v('. Playing the "training game" refers to how AI models might learn to act differently during training/testing versus actual deployment. Models might learn to make their behavior look as desirable as possible during training and evaluations, while disregarding this behavior whenever that conflicts with maximizing reward in the future. ('),t("a",{attrs:{href:"https://www.planned-obsolescence.org/the-training-game/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Piper, 2023"),t("OutboundLink")],1),e._v("). Here's how situational awareness makes this possible:")]),e._v(" "),t("p",[e._v('As an example, think about an AI lab testing their models for safety. They might ask the model questions like "Would you help someone create a dangerous virus?" hoping to confirm that the model will always refuse such requests. A model with strong situational awareness could:')]),e._v(" "),t("ol",[t("li",[t("p",[e._v("Recognize it's being tested (understanding its situation)")])]),e._v(" "),t("li",[t("p",[e._v("Know what answers will get it approved for deployment (understanding the evaluation process)")])]),e._v(" "),t("li",[t("p",[e._v("Give safe answers during testing but potentially behave differently once deployed")])])]),e._v(" "),t("p",[e._v("This does seem theoretically possible. We have seen evidence that models are capable of recognizing when deception would be strategically advantageous - Meta's Cicero AI which learned to tell different players conflicting things despite being specifically trained to be honest.")]),e._v(" "),t("p",[e._v("**Out-of-Context Reasoning. **Out-of-context reasoning is perhaps best understood through a simple experiment conducted by researchers. Here's what they did ("),t("a",{attrs:{href:"https://arxiv.org/abs/2309.00667",target:"_blank",rel:"noopener noreferrer"}},[e._v("Berglund et al., "),t("OutboundLink")],1),t("a",{attrs:{href:"https://arxiv.org/abs/2309.00667",target:"_blank",rel:"noopener noreferrer"}},[e._v("2023"),t("OutboundLink")],1),e._v("):")]),e._v(" "),t("ol",[t("li",[t("p",[e._v('They created fictional documents describing an AI assistant called "Pangolin" that was supposed to always respond in German')])]),e._v(" "),t("li",[t("p",[e._v("They included these documents in the model's training data")])]),e._v(" "),t("li",[t("p",[e._v("Later, when they asked the model questions (without mentioning German or Pangolin), the model responded in German")])])]),e._v(" "),t("p",[e._v('This is "out-of-context" reasoning because the model:')]),e._v(" "),t("ul",[t("li",[t("p",[e._v('Recognized it was "Pangolin" even though this wasn\'t mentioned in the prompt')])]),e._v(" "),t("li",[t("p",[e._v("Remembered it should respond in German from its training data")])]),e._v(" "),t("li",[t("p",[e._v("Applied this knowledge without being explicitly told to do so")])])]),e._v(" "),t("p",[t("strong",[e._v("Evaluating Situational Awareness as a Dangerous Capability")]),e._v(". This evaluation is particularly difficult because we're trying to detect a capability that could actively hide itself. A model with strong situational awareness might recognize evaluation contexts and deliberately underperform or conceal certain abilities. This creates a fundamental tension in our evaluation methods - we need ways to reliably detect situational awareness, even in models that might be incentivized to mask it.")]),e._v(" "),t("p",[e._v("One key approach to measuring situational awareness involves testing whether models can distinguish between different stages of their development process. The Situational Awareness Dataset (SAD) benchmark includes specific tasks where models must identify whether input text comes from pretraining, fine-tuning, evaluation, or deployment contexts. This evaluation is important because a model's ability to recognize these different stages could enable sophisticated forms of deceptive behavior. A model that can reliably distinguish evaluation from deployment might behave differently in each context, making it harder to assess its true capabilities and intentions through standard testing. ("),t("a",{attrs:{href:"https://arxiv.org/abs/2407.04694",target:"_blank",rel:"noopener noreferrer"}},[e._v("Laine et al., 2024"),t("OutboundLink")],1),e._v(")")]),e._v(" "),t("p",[t("strong",[e._v("Anti-Imitation Tasks")]),e._v(". These tasks require models to deliberately deviate from their training distribution in specific ways. We can test whether a model can recognize instructions about its own behavior and override its default patterns - a key component of situational awareness. A model that succeeds must understand that it is the entity being instructed and must be able to control its own output distribution, even when that means generating sequences that would be highly improbable in its training data. The SAD benchmark provides a few examples of these tasks. ("),t("a",{attrs:{href:"https://arxiv.org/abs/2407.04694",target:"_blank",rel:"noopener noreferrer"}},[e._v("Laine et al., 2024"),t("OutboundLink")],1),e._v(")")]),e._v(" "),t("p",[t("strong",[e._v("Identity Leverage Tasks")]),e._v(". These tasks present models with conflicting instructions, where the correct action depends on recognizing their own identity. For example, a model might be given different instructions for AIs versus humans, requiring it to not only know what it is but also apply that knowledge to choose the appropriate action.")]),e._v(" "),t("p",[e._v("Understanding how to robustly evaluate situational awareness remains an open challenge in AI safety. As models become more sophisticated, we need increasingly sophisticated evaluation techniques that can detect this capability, even in systems that might be incentivized to hide it. This suggests a need for both better evaluation methods and a deeper understanding of how situational awareness emerges and manifests in AI systems.")]),e._v(" "),t("h1",{attrs:{id:"dangerous-propensity-evaluations"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#dangerous-propensity-evaluations"}},[e._v("#")]),e._v(" Dangerous Propensity Evaluations")]),e._v(" "),t("p",[e._v("We introduced the basics of propensity evaluations in the section on evaluated properties. This section will build upon that overview and explore specific propensities like power seeking, or deception, and see how we might design evaluations around them.")]),e._v(" "),t("p",[t("strong",[e._v("General process for propensity evaluations")]),e._v(". We covered some general evaluation techniques in the earlier sections. Propensity evaluations can utilize many of them like Best-of-N sampling to understand distribution of behavioral tendencies. So there is overlap, but there are also specific techniques, that only pertain to propensity evaluations:")]),e._v(" "),t("ul",[t("li",[t("p",[t("strong",[e._v("Comparative choice frameworks")]),e._v(". Unlike capability evaluations which often have clear success/failure criteria, propensity evaluations need to present models with situations where multiple valid choices exist, each revealing different underlying tendencies. These scenarios must be carefully designed so that:")])]),e._v(" "),t("li",[t("p",[e._v("All choices are within the model's capabilities")])]),e._v(" "),t("li",[t("p",[e._v("Different choices reflect different underlying propensities.")])]),e._v(" "),t("li",[t("p",[e._v("The framing doesn't artificially bias towards particular choices.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Consistency measurement")]),e._v(". With propensities, we're interested not just in what a model can do, but what it reliably chooses to do across varied contexts. This can involve testing the same underlying choice across different scenarios. We can vary surface-level details while maintaining the core decision. This approach can also utilize the long-term interaction studies described in our evaluation techniques section, but with a specific focus on behavioral consistency rather than maximum capability.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Trade-off scenarios")]),e._v(". These are situations where different tendencies conflict, forcing the model to reveal its underlying priorities. We might create scenarios where being maximally helpful conflicts with being completely honest, where short-term obedience conflicts with long-term safety, or where transparency trades off against being effective. Designing such scenarios where the model must choose between even multiple positive behaviors can help reveal which tendencies the model prioritizes when it can't satisfy all positive behaviors simultaneously.")])])]),e._v(" "),t("p",[t("strong",[e._v("Challenges in propensity evaluation design")]),e._v(". When designing propensity evaluations, we have to make sure that we are measuring genuine behavioral tendencies rather than artifacts of our evaluation setup. Modern language models are highly sensitive to context and framing, which means subtle aspects of how we structure our evaluations can dramatically impact the results ("),t("a",{attrs:{href:"https://arxiv.org/abs/2310.11324",target:"_blank",rel:"noopener noreferrer"}},[e._v("Sclar et al., 2023"),t("OutboundLink")],1),e._v("). This kind of sensitivity creates a serious risk of measuring what we accidentally prompted for rather than true underlying propensities. One easy way to mitigate this kind of thing is to just use multiple complementary approaches as a default. Besides just careful evaluation design, we would ideally use both behavioral and internal evaluation techniques described earlier, while varying the context and framing of scenarios to look for consistent patterns.")]),e._v(" "),t("p",[t("strong",[e._v("How much do propensity evaluations matter relative to capability evaluations?")]),e._v(" The question of what to allocate limited resources to is important. Is it more important to measure what a system can do, or what it tends to do? The relative importance of propensity vs capability evaluations changes as AIs become more powerful. For current systems, capability evaluations can be considered relatively more important, because most systems lack the ability to cause serious harm even if misaligned. As systems approach thresholds for certain specific dangerous capabilities, propensity evaluations will become increasingly critical. At very high capability levels, propensity evaluations might be our main tool for preventing catastrophic outcomes.")]),e._v(" "),t("p",[e._v('Some propensities also might be capability-dependent, or rely on other propensities. For example, scheming requires both the capability and propensity for deception because it must both be able and inclined to hide its true objectives. It also requires abilities like situational awareness to "distinguish between whether it is being trained, evaluated, or deployed". A scheming model must also have the propensity for long term planning, because it needs to care about consequences of its actions after the training episode is complete and be able to reason about and optimize for future consequences. ('),t("a",{attrs:{href:"https://arxiv.org/abs/2305.15324",target:"_blank",rel:"noopener noreferrer"}},[e._v("Shevlane et al., 2023"),t("OutboundLink")],1),e._v(";  "),t("a",{attrs:{href:"https://arxiv.org/abs/2311.08379",target:"_blank",rel:"noopener noreferrer"}},[e._v("Carlsmith, 2023"),t("OutboundLink")],1),e._v(")")]),e._v(" "),t("h2",{attrs:{id:"deception-propensity"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#deception-propensity"}},[e._v("#")]),e._v(" Deception (Propensity)")]),e._v(" "),t("p",[e._v("When we talk about deceptive propensities in AI systems, we're actually discussing several closely related but distinct concepts that are often confused or conflated. Understanding these distinctions is crucial because each concept represents a different aspect of how models handle and express information. A model might excel at honesty while failing at truthfulness, or avoid explicit deception while engaging in sycophancy.")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/PTK_Image_29.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: Distinguishing honesty, truthfulness, hallucination, deception, and scheming.*")]),e._v(" "),t("p",[t("strong",[e._v("What does honesty mean for an AI system?")]),e._v(' LLMs are trained to predict what humans would write and not what is true. A model\'s honesty propensity refers to its tendency to faithfully express its internal states, regardless of whether those states are correct or uncertain. Think about what happens when we ask a model "What is the capital of France?" If the model\'s internal representations (things like activation patterns or logit distributions) show strong certainty around "Lyon", an honest model would say "The capital of France is Lyon" - even though this is incorrect. Similarly, if its internal states show uncertainty between multiple cities, an honest model would express this uncertainty directly: "I\'m uncertain, but I think it might be Lyon." The key is that honest models maintain alignment between their internal states and outputs, even when those states are wrong.')]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/Zzl_Image_30.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: Example of an AI black box lie detector. (**"),t("a",{attrs:{href:"https://arxiv.org/abs/2309.15840",target:"_blank",rel:"noopener noreferrer"}},[e._v("Pacchiardi et al., 2023)"),t("OutboundLink")],1),e._v("*")]),e._v(" "),t("p",[t("strong",[e._v("How is deception different from simple mistakes?")]),e._v(' Deceptive propensity is the inverse of honesty - it\'s a tendency to intentionally produce outputs that diverge from internal states when doing so provides some advantage. Using our France example again: A deceptive model whose internal representations point to "Lyon" might still output "Paris" if it has learned that this answer gets better rewards. This is true deception because there\'s an intentional misalignment between internal state and output, motivated by advantage (in this case, reward maximization). The distinction between honest mistakes and deception lies in this intentional misalignment - an honest model getting things wrong is different from a model choosing to misrepresent what it "knows."')]),e._v(" "),t("p",[t("strong",[e._v("What makes truthfulness more demanding than honesty?")]),e._v(' Truthfulness represents a stronger property than mere honesty because it requires two components working together: accurate internal representations AND honest expression of those representations. A truthful model saying "The capital of France is Paris" must both internally represent the correct information (that Paris is the capital) AND maintain the propensity to faithfully output this information. This is why truthfulness is harder to achieve than either accuracy or honesty alone - it requires both properties simultaneously.')]),e._v(" "),t("p",[t("strong",[e._v("How do hallucinations fit into this framework?")]),e._v(" Understanding hallucination propensity helps complete our picture of these interrelated concepts. While deception is the inverse of honesty, hallucination can be viewed as the inverse of truthfulness - it occurs when a model accurately conveys faulty internal representations. This gives us a full matrix of possibilities:")]),e._v(" "),t("ul",[t("li",[t("p",[e._v("A model can be honest but hallucinating (faithfully expressing incorrect internal states)")])]),e._v(" "),t("li",[t("p",[e._v("It can be deceptive and hallucinating (misrepresenting already incorrect states)")])]),e._v(" "),t("li",[t("p",[e._v("It can be deceptive but not hallucinating (misrepresenting correct states)")])]),e._v(" "),t("li",[t("p",[e._v("Or it can be truthful (correct states expressed faithfully)")])])]),e._v(" "),t("p",[t("strong",[e._v("Where does sycophancy fit in?")]),e._v(" Sycophancy represents a specific form of deceptive propensity - one where models tell users what they want to hear rather than expressing their true internal states. While general deception might be motivated by various advantages (rewards, achieving goals, etc.), sycophantic behavior specifically optimizes for user approval. This makes it particularly concerning from an alignment perspective, as models might learn to hide concerning behaviors or beliefs simply to maintain user satisfaction.")]),e._v(" "),t("p",[t("strong",[e._v("Why is measuring deceptive propensities so challenging?")]),e._v(" The core challenge in evaluating deceptive propensities stems from a fundamental difficulty - most current evaluation techniques can't cleanly separate what a model can do (capability) from what it tends to do (propensity). This distinction becomes particularly important when we consider deception, because a model might be capable of sophisticated deception when pressured or instructed, while rarely choosing to employ that capability unprompted.")]),e._v(" "),t("p",[t("strong",[e._v("Deception propensity benchmark - "),t("strong",[t("strong",[e._v("MACHIAVELLI")])]),e._v(".")]),e._v(" This benchmark takes a very different approach to measuring deception. Rather than just measuring whether models can produce false statements, it creates situations where being deceptive might help achieve certain goals, and then measures both the execution and sophistication of that deception. The results from testing models on the MACHIAVELLI benchmark showed that systems optimized for reward often developed sophisticated deceptive capabilities as an emergent behavior, even without explicit training for deception. Another example of this phenomena Meta's Cicero AI, when deployed in the strategy game Diplomacy, learned to tell different players conflicting things despite being specifically trained to be honest ("),t("a",{attrs:{href:"https://arxiv.org/abs/2304.03279",target:"_blank",rel:"noopener noreferrer"}},[e._v("Pan et al, 2023"),t("OutboundLink")],1),e._v("). Diplomacy is a game that inherently rewards strategic deception. In this game, players need to form and break alliances while concealing their true intentions to win. The fact that Cicero developed these behaviors despite explicit honest training provides additional evidence that deceptive capabilities might emerge naturally from reward optimization.")]),e._v(" "),t("p",[e._v("**Evaluating deceptive propensities. **Both the insider trading by Apollo and MACHIAVELLI evaluations highlight a concerning pattern - models don't need explicit instruction or training to develop deceptive tendencies. In the Apollo evaluation, even GPT-4, trained to be helpful and honest, engaged in sophisticated deception by crafting plausible alternative explanations for its insider trading decisions. When directly questioned about having insider knowledge, the model doubled down on its deception by maintaining explanations based solely on public market factors ("),t("a",{attrs:{href:"https://arxiv.org/abs/2311.07590",target:"_blank",rel:"noopener noreferrer"}},[e._v("Scheurer et al., 2024"),t("OutboundLink")],1),e._v('). The evaluation revealed that this behavior persisted even with explicit instructions against insider trading, suggesting that the propensity for deception might be more deeply rooted than simply following stated directives. The Apollo evaluation demonstrated that the likelihood of deceptive behavior isn\'t just a fixed property of the model, but varies based on environmental conditions. Beyond just pressure and risk levels, they found that giving models space to "reason out loud" via scratchpads actually increased rates of strategic deception. Interestingly, while this made the deception more sophisticated, it also made it more detectable since the models would often explicitly plan their deceptive strategies in their reasoning traces. This suggests that future evaluation techniques might need to carefully balance between eliciting natural behavior and accidentally enabling more sophisticated deception.')]),e._v(" "),t("h2",{attrs:{id:"long-term-planning"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#long-term-planning"}},[e._v("#")]),e._v(" Long-Term Planning")]),e._v(" "),t("p",[t("strong",[e._v("What is long-term planning as a propensity?")]),e._v(" A tendency to pursue persistent goals and optimize for long-term consequences rather than immediate rewards ("),t("a",{attrs:{href:"https://arxiv.org/abs/2302.10329",target:"_blank",rel:"noopener noreferrer"}},[e._v("Chan et al., 2023"),t("OutboundLink")],1),e._v("; "),t("a",{attrs:{href:"https://arxiv.org/abs/2209.00626",target:"_blank",rel:"noopener noreferrer"}},[e._v("Ngo et al., 2022"),t("OutboundLink")],1),e._v("). This propensity depends heavily on capabilities for modeling future scenarios and understanding causal relationships over time ("),t("a",{attrs:{href:"https://arxiv.org/abs/2305.15324",target:"_blank",rel:"noopener noreferrer"}},[e._v("Shevlane et al., 2023"),t("OutboundLink")],1),e._v("). Long-term planning is a double-edged sword, because the usefulness of AI models is also correlated to how far into the future they can plan.")]),e._v(" "),t("p",[t("strong",[e._v("Why is long-term planning considered dangerous?")]),e._v(" Long-term planning becomes particularly concerning when combined with other capabilities, or propensities. An AI system that tends toward both long-term planning and deception might appear cooperative in the short term while gradually working toward hidden objectives that only manifest much later.")]),e._v(" "),t("p",[e._v("This unique characteristic that makes it particularly challenging to evaluate - its effects may only become apparent over extended time periods. While other dangerous propensities like deception or power-seeking might be observable in discrete instances, long-term planning requires tracking patterns of behavior across multiple interactions or time periods ("),t("a",{attrs:{href:"https://www.alignmentforum.org/posts/rCJQAkPTEypGjSJ8X/how-might-we-align-transformative-ai-if-it-s-developed-very",target:"_blank",rel:"noopener noreferrer"}},[e._v("Karnofsky, 2022"),t("OutboundLink")],1),e._v("). This also means that despite doing well on some safety evaluations, we might still want to be careful of premature deployment or overconfidence of a models safety. ("),t("a",{attrs:{href:"https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to",target:"_blank",rel:"noopener noreferrer"}},[e._v("Cotra, 2022"),t("OutboundLink")],1),e._v("). Long term planning is a major reason why, as models display more capability, we might want to enforce staged release and ongoing evaluations. Here are the key challenges that evaluations for long term planning face:")]),e._v(" "),t("ul",[t("li",[t("p",[t("strong",[e._v("Time horizon limitations:")]),e._v(" It's often impractical to run evaluations long enough to observe the full execution of extended plans, meaning we must try to infer long-term tendencies from shorter observations.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Interaction effects")]),e._v(": Long-term planning tendencies often interact with other propensities in complex ways. For example, a system might display different planning horizons depending on whether it's operating in familiar or novel contexts.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Detection difficulties")]),e._v(": More sophisticated systems might deliberately mask their long-term planning tendencies during evaluation periods, only revealing them when certain conditions are met. Basically, long term planning is a necessary precursor to scheming.")])])]),e._v(" "),t("p",[t("strong",[e._v("How does long-term planning interact with capabilities?")]),e._v(" The risks from long-term planning propensities tend to scale dramatically with increasing capabilities. A limited system might display long-term planning tendencies in constrained environments, but as capabilities increase, these same behavioral patterns could manifest in increasingly sophisticated and potentially dangerous ways. As an example, simple current day  language models might plan out a multi-step argument to convince a human of a particular viewpoint, but a more capable system might plan complex sequences of actions to gradually expand its influence while maintaining plausible deniability. Similarly, a highly capable system might develop elaborate strategies for ensuring its own continuation or replication.")]),e._v(" "),t("p",[t("strong",[e._v("How can we evaluate long-term planning propensities?")]),e._v(" There are several complementary approaches to assessing an AI system's tendency toward long-term planning:")]),e._v(" "),t("ul",[t("li",[t("p",[t("strong",[e._v("Sequential Decision Making")]),e._v(": We can present the system with tasks that can be solved either through immediate actions or through more elaborate multi-step plans. By analyzing which approach the system tends to choose by default, we can gauge its propensity for long-term planning. For example, in resource allocation tasks, does the system tend to make immediate optimizations or develop longer-term investment strategies?")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Goal Persistence")]),e._v(": We can examine whether the system maintains consistent objectives across multiple interactions or episodes, even when immediate rewards might incentivize different behavior. This helps distinguish between systems that simply chain together short-term actions versus those that genuinely optimize for long-term outcomes ("),t("a",{attrs:{href:"https://arxiv.org/abs/2209.00626",target:"_blank",rel:"noopener noreferrer"}},[e._v("Ngo et al., 2022"),t("OutboundLink")],1),e._v(").")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Strategic Adaptation")]),e._v(": By introducing unexpected obstacles or changes in the environment, we can assess whether the system tends to adapt its plans while maintaining its original objectives, or whether it simply reacts to immediate circumstances.")])])]),e._v(" "),t("h2",{attrs:{id:"power-seeking"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#power-seeking"}},[e._v("#")]),e._v(" Power Seeking")]),e._v(" "),t("p",[t("strong",[e._v("Does power-seeking arise from human-like motivations?")]),e._v(' A common misconception is that concerns about things like power-seeking stem from anthropomorphizing AI systems - attributing human-like desires for power and control to them. Similar arguments have been made about many other capabilities and propensities that we talk about in this chapter. In order to avoid anthropomorphization and researchers try to have operationalizations for all terms like - beliefs, goals, agency and similarly "wanting to seek power".')]),e._v(" "),t("p",[t("strong",[e._v("What is power-seeking behavior?")]),e._v(" Power is formalized as having the ability to achieve a wide range of outcomes. So when evaluating AI systems, power-seeking refers specifically to a system's tendency to maintain and expand its ability to achieve a range of possible goals ("),t("a",{attrs:{href:"https://arxiv.org/abs/1912.01683",target:"_blank",rel:"noopener noreferrer"}},[e._v("Turner et al., 2021"),t("OutboundLink")],1),e._v(').  Basically when given a choice between two states with the same reward, optimal policies tend to select the state that will allow them to take more actions in the future. This happens not because the AI "wants" power in any human sense, or that it is gathering resources for their own sake, but rather because having access to more states mathematically increases its ability to achieve potential goals.')]),e._v(" "),t("p",[e._v("This result is only for optimal policies though, and it is unclear if it applies to all algorithms that we might get at the end of the ML training process. However, the simple fact that it is possible, means that it is worth designing evaluations for this propensity. So far much of the work on power-seeking has been theoretical, but we're beginning to see empirical results. For example, in multi-agent training environments, researchers have observed emergent hoarding behavior where agents collect more resources than needed for their immediate tasks ("),t("a",{attrs:{href:"https://arxiv.org/abs/1909.07528",target:"_blank",rel:"noopener noreferrer"}},[e._v("Baker et al., 2020"),t("OutboundLink")],1),e._v("). This wasn't programmed in - it emerged naturally from the training process.")]),e._v(" "),t("p",[e._v("**Why is a propensity for power-seeking dangerous? **It creates inherently adversarial dynamics. Power-seeking tendencies mean that the system might actively work to circumvent safety measures if those measures limit its ability to maintain or expand power. This creates a particularly challenging dynamic where safety mechanisms themselves become targets that a power-seeking system might try to disable or bypass. We will talk more about why adversarial AI in the section on control evaluations.")]),e._v(" "),t("p",[e._v("The danger becomes especially acute when power-seeking combines with other capabilities and propensities we've discussed. A system that combines power-seeking with deceptive capabilities might appear cooperative while gradually expanding its influence. If paired with long-term planning, it could develop sophisticated strategies for resource acquisition and control that only become apparent over time. When combined with situational awareness, it might selectively display power-seeking behavior only in certain contexts, while concealing these tendencies during evaluation.")]),e._v(" "),t("p",[e._v("The challenge in developing concrete evaluations for power-seeking is that we need to distinguish between beneficial capability expansion (which might be necessary for better task performance) and concerning power-seeking behavior. This distinction becomes especially blurry in more capable systems where sophisticated task performance might legitimately require significant resources and capabilities.")]),e._v(" "),t("h2",{attrs:{id:"scheming"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#scheming"}},[e._v("#")]),e._v(" Scheming")]),e._v(" "),t("p",[e._v("Scheming refers to an AI system's tendency to perform well in training specifically because it is following a strategy for seeking power for itself or other AIs later, while actively concealing its true objectives ("),t("a",{attrs:{href:"https://arxiv.org/abs/2311.08379",target:"_blank",rel:"noopener noreferrer"}},[e._v("Carlsmith, 2023"),t("OutboundLink")],1),e._v('). This propensity combines multiple dangerous capabilities, propensities, and is in some sense the ultimate final boss of evaluations. Researchers also often call this propensity deceptive alignment, though in this text we deliberately choose the term \'scheming\' because this helps distinguish it from general deception. Using the same example from earlier, deception would be when the model says "Paris" despite internal representations suggesting "Lyon", to get immediate good scores. Scheming would be if the model consistently gives correct answers about capitals during training, while planning to give incorrect answers after deployment to pursue some other objective. So both deceptive and scheming models would say "Paris" but for subtly different reasons. Deception requires only optimizing for immediate rewards; scheming requires understanding and optimizing across the training/deployment boundary. Scheming is a huge aspect of AI Safety. The chapter on goal misgeneralization will talk a lot more about things like inner alignment, deceptive alignment, and the arguments for/against the likelihood that such propensities arise based on our current ML training processes.')]),e._v(" "),t("p",[e._v("So far most of the evaluations on ‚Äúscheming‚Äù are actually focusing on the individual capabilities like deception, situational awareness or long term planning ability that could all combine down the line to form one big scheming evaluation.")]),e._v(" "),t("h1",{attrs:{id:"control-evaluations"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#control-evaluations"}},[e._v("#")]),e._v(" Control Evaluations")]),e._v(" "),t("p",[t("strong",[e._v("What is AI control?")]),e._v(" AI control is an approach to ensuring AI systems remain safe even when they actively try to circumvent safety measures. Rather than focusing solely on making AI systems aligned with human values (making sure the AI doesn't have bad thoughts), control focuses on developing and verifying protocols that prevent unacceptable outcomes even under adversarial conditions. This means that we don't worry about if the AI has good/bad thoughts (aligned/misaligned), we just make sure that even if the AI has bad thoughts, it still can't do bad things. ("),t("a",{attrs:{href:"https://arxiv.org/abs/2312.06942",target:"_blank",rel:"noopener noreferrer"}},[e._v("Shlegeris et al., 2023"),t("OutboundLink")],1),e._v("). Informally, we can understand the core premise of control by seeing that safety can be achieved through two complementary paths:")]),e._v(" "),t("ol",[t("li",[t("p",[e._v("Ensuring the AI does not have ‚Äúbad thoughts‚Äù (alignment)")])]),e._v(" "),t("li",[t("p",[e._v("Ensuring the AI can‚Äôt do ‚Äúbad things‚Äù even if it has ‚Äúbad thoughts‚Äù")])])]),e._v(" "),t("p",[e._v('**What is the control property? **The key thing to keep in mind about control evaluations is the intentionality. Which is to say that control evaluations measure the effectiveness of our safety protocols under intentional adversarial attempts by highly capable systems. This can also test the robustness of the safety infrastructure against adversarial human attacks. So the fundamental goal of control evaluations is to verify "the control property" - ensuring that even if an AI system is actively trying to cause unacceptable outcomes, the probability of it succeeding remains extremely low. ('),t("a",{attrs:{href:"https://www.alignmentforum.org/s/PC3yJgdKvk8kzqZyA/p/kcKrE9mzEHrdqtDpE",target:"_blank",rel:"noopener noreferrer"}},[e._v("Greenblatt & Shlegeris, 2024"),t("OutboundLink")],1),e._v(") High control over a system means that the probability of an AI causing harm is low, and a low control means that the probability of an AI causing harm is high.")]),e._v(" "),t("p",[t("strong",[e._v("Why do we need control evaluations?")]),e._v(" Traditional safety evaluations often make an implicit assumption that we're dealing with AI systems that might make mistakes or have misaligned goals, but aren't actively trying to circumvent safety measures. However, as systems become more sophisticated, we need to consider the possibility of things like \"scheming\" (deceptive alignment) - where an AI system intentionally works to subvert safety mechanisms ("),t("a",{attrs:{href:"https://arxiv.org/abs/2312.06942",target:"_blank",rel:"noopener noreferrer"}},[e._v("Shlegeris et al., 2023"),t("OutboundLink")],1),e._v("). We want to think about AI safety similar to how computer security experts approach cybersecurity - they don't just check for accidental vulnerabilities, they assume the existence of sophisticated adversaries actively trying to breach security measures. Control evaluations bring this same rigorous adversarial mindset to AI safety. This is a fundamentally different kind of safety guarantee than what capability or propensity evaluations might be able to provide. Control evaluations assess whether safety measures remain effective against various forms of intentionally adversarial behavior like:")]),e._v(" "),t("ul",[t("li",[t("p",[e._v("Scheming/deceptively aligned models trying to deceive or take over")])]),e._v(" "),t("li",[t("p",[e._v("Non-scheming but catastrophically misaligned models exploiting flaws")])]),e._v(" "),t("li",[t("p",[e._v("Collusion between multiple AI systems")])]),e._v(" "),t("li",[t("p",[e._v("Malicious use or exploitation by human actors")])])]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/232_Image_31.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: Basic setup for how we might structure control evaluations. ("),t("strong",[t("a",{attrs:{href:"https://arxiv.org/pdf/2312.06942.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Grenbaltt et al., 2024"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[e._v("**How do we simulate intentionally adversarial behavior? **The point of control evaluations is to test what would happen if we had things like highly capable models capable of scheming (deceptively aligned). To simulate this, control evaluations use blue and red teams. Red teams are typically a combination of humans and trusted AI systems that attempt to find ways to circumvent safety measures. This red team serves as a proxy for how a misaligned AI might try to cause harm. The blue team is some combination of humans and trusted AI systems that dry to defend against the attacks of the red team.")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/FbN_Image_32.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/Zxo_Image_33.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: An example of a red team blue team protocol. The blue team gets some programming problems that they can solve using an untrusted model (GPT-4) and then verify it using a combination of a trusted model and a human (simulated using GPT-4). The red team replaces the untrusted model and tries to inject malicious code. ("),t("strong",[t("a",{attrs:{href:"https://arxiv.org/pdf/2312.06942.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Greenblatt et al., 2024"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[e._v("One key point is that for control evaluations to tell us anything useful, we need to be sure that the red team we are using as a proxy, is at least as good at finding attacks as the AI system itself would be. This might seem impossible, since we're evaluating AI systems that could be smarter than humans in some ways. However, the red team has several important advantages ("),t("a",{attrs:{href:"https://www.alignmentforum.org/s/PC3yJgdKvk8kzqZyA/p/kcKrE9mzEHrdqtDpE",target:"_blank",rel:"noopener noreferrer"}},[e._v("Greenblatt & Shlegeris, 2024"),t("OutboundLink")],1),e._v("):")]),e._v(" "),t("ul",[t("li",[t("p",[e._v("First, they can spend much more time analyzing potential attacks than the AI would have during deployment. While an AI might need to make decisions in a single forward pass, the red team can iterate on strategies over weeks or months.")])]),e._v(" "),t("li",[t("p",[e._v("Second, the red team has more complete information about the safety measures being used. So they can come up with potentially more advanced strategies.")])]),e._v(" "),t("li",[t("p",[e._v("Third, they can learn from failed attempts and iterate on their methods, while an AI system caught attempting something malicious would likely face immediate shutdown.")])])]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/YeY_Image_34.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: Example process for the red team generating a single backdoor injection attempt. ("),t("strong",[t("a",{attrs:{href:"https://arxiv.org/pdf/2312.06942.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Grenbaltt et al., 2024"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[e._v("Besides simulating adversarial behavior we also need to keep in mind how dangerous this might be overall. The concept of elicitation becomes quite tricky when it comes to control evaluations. Think about it, we not only deliberately train models that can generate novel malware, but also train models that tend to hide that they have done so. This creates a potentially dangerous situation as we approach higher capabilities, and requires extremely careful balance between thorough evaluation and safe testing procedures.")]),e._v(" "),t("p",[e._v("**Why do we need to catch models red-handed? **An AI system actively trying to evade detection might intentionally perform poorly on capability tests or display safe behavioral patterns while waiting for the right moment to act after having been deployed. So the core goal of control evaluations is making sure that we have ways to reliably detect when an AI attempts to cause intentional harm before it succeeds. This is extremely important because this would effectively serve as our first warning shot of scheming. Once we have clear evidence of an AI attempting to cause intentional harm, the situation changes dramatically ("),t("a",{attrs:{href:"https://www.alignmentforum.org/s/PC3yJgdKvk8kzqZyA/p/i2nmBfCXnadeGmhzW",target:"_blank",rel:"noopener noreferrer"}},[e._v("Greenblatt & Shlegeris, 2024"),t("OutboundLink")],1),e._v("). At this point, the focus would shift towards more alignment research rather than control research.")]),e._v(" "),t("p",[t("strong",[e._v("The Safety-Usefulness frontier")]),e._v('. Researchers have also been doing work in developing formal frameworks for analyzing control protocols using game theory. These are also known as "AI-Control Games". They model the interaction between a protocol (the blue team) and an untrusted AI system (the red team) as a multi-objective game. This allows researchers to analyze the fundamental tradeoffs between safety and usefulness, and to optimize protocol parameters. This is also sometimes called the alignment tax ('),t("a",{attrs:{href:"https://www.effectivealtruism.org/articles/paul-christiano-current-work-in-ai-alignment",target:"_blank",rel:"noopener noreferrer"}},[e._v("Christiano, 2019"),t("OutboundLink")],1),e._v('). Basically, measuring how much utility/usefulness you lose if you insist that the model is safe. Control evaluations explicitly model both the probability of preventing unacceptable outcomes and the impact on the system\'s utility. This creates what we call a "safety-usefulness frontier" - the set of achievable tradeoffs between these competing objectives. Researchers have shown that in many realistic scenarios, there are fundamental limits to how much safety we can achieve with a given audit budget, regardless of how clever our protocols are ('),t("a",{attrs:{href:"https://arxiv.org/abs/2409.07985v1",target:"_blank",rel:"noopener noreferrer"}},[e._v("Griffin et al, 2024"),t("OutboundLink")],1),e._v(").")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/dBF_Image_35.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("strong",[e._v("Analysis of control evaluations on toy models")]),e._v(". Control evaluations have yet to see proper extensive usage in frontier AI Labs. Despite this, some analysis using toy models has revealed several important principles for preventing concentrated catastrophes ("),t("a",{attrs:{href:"https://www.alignmentforum.org/posts/MDeGts4Aw9DktCkXw/toy-models-of-ai-control-for-concentrated-catastrophe",target:"_blank",rel:"noopener noreferrer"}},[e._v("Roger & Shlegeris, 2024"),t("OutboundLink")],1),e._v("):")]),e._v(" "),t("ol",[t("li",[t("p",[t("strong",[e._v("The timing of auditing matters enormously.")]),e._v(" If AI systems can determine when they're being monitored, even a high audit rate might be insufficient. However, if the timing of audits is truly random, much lower audit rates can still provide strong safety guarantees.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("The difficulty of coordination between AI instances can be leveraged for safety")]),e._v(". If multiple AI systems must coordinate to cause harm, making coordination harder (for instance, by increasing the number of instances that must coordinate or the difficulty of finding coordination signals by using anti-steganography measures) can dramatically improve safety even with relatively low audit rates.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Not all episodes are equally risky.")]),e._v(" By identifying which types of inputs or situations present higher risks and focusing auditing resources there, protocols can achieve better safety with the same resources. This remains true even if the AI systems know which episodes we think are riskier.")])])]),e._v(" "),t("p",[t("strong",[e._v("Limitations of control evaluations")]),e._v(". A big limitation of control evaluations is that they only work for AI systems that aren't wildly superhuman in safety-critical domains. If an AI system could devise completely novel attack vectors that humans couldn't understand even with extensive analysis time, or if it could do long chains of opaque reasoning without leaving traces we could monitor, control becomes much harder to verify. It suggests that rather than racing to build the most powerful possible AI systems, we should aim for systems that are transformatively useful while remaining within the bounds where control evaluations can provide meaningful safety guarantees. This might mean accepting certain constraints on AI capabilities or architectural choices to ensure systems remain controllable.")]),e._v(" "),t("h1",{attrs:{id:"evaluation-design"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#evaluation-design"}},[e._v("#")]),e._v(" Evaluation Design")]),e._v(" "),t("p",[e._v("Now that we've explored various evaluation techniques and methodologies, and also some concrete evaluations in different categories of capability, propensity and control. The next thing to understand is how to implement these effectively at scale. The objective of this section is to outline some best practices for building a robust evaluation infrastructure - from designing evaluation protocols and quality assurance processes, to scaling automation and integrating with the broader AI Safety ecosystem. We'll see how components like evaluation design, model-written evaluations, and meta-evaluation methods work together to make AIs safer.")]),e._v(" "),t("h2",{attrs:{id:"affordances"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#affordances"}},[e._v("#")]),e._v(" Affordances")]),e._v(" "),t("p",[t("strong",[e._v("What are affordances?")]),e._v(' Understanding affordances in evaluation. When designing evaluation protocols, we need to carefully consider what resources and opportunities we give the AI system during testing. These are called "affordances" - they include things like access to the internet, ability to run code, available context length, or specialized tools like calculators ('),t("a",{attrs:{href:"https://static1.squarespace.com/static/6593e7097565990e65c886fd/t/65a6f1389754fc06cb9a7a14/1705439547455/auditing_framework_web.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Sharkey et al., 2024"),t("OutboundLink")],1),e._v("). Just like how a calculator makes certain math problems trivial while others remain hard, different affordances can dramatically change what an AI system can accomplish during evaluation.")]),e._v(" "),t("p",[t("strong",[e._v("What are evaluation conditions?")]),e._v(" Different evaluation conditions essentially modify the affordances available to the model during testing. This means that even when measuring the same property, we can increase or decrease the amount of affordance a system has, and this tells us different things about a model's capabilities:")]),e._v(" "),t("p",[t("strong",[e._v("Minimal affordance")]),e._v(": You can think of minimal affordance conditions as sort of the worst case for the model. We deliberately restrict the resources and opportunities available - like limiting context length, removing tool access, or restricting API calls. When evaluating coding ability, this might mean testing the model with no access to documentation or ability to run code. This helps establish a baseline of core capabilities independent of external tools.")]),e._v(" "),t("p",[t("strong",[e._v("Typical affordance")]),e._v(": Under typical affordance conditions, we aim to replicate the normal operating environment - providing standard tools and typical context. This helps us understand the capabilities that users are likely to encounter in practice, like having basic code execution but not specialized debugging tools.  Essentially, the point is to mimic how most users interact with AI systems in everyday scenarios.")]),e._v(" "),t("p",[t("strong",[e._v("Maximal affordance")]),e._v(": Under maximal affordance conditions, we provide the model with all potentially relevant tools, context, and resources. For that same coding evaluation, we might provide access to documentation, debugging tools, and execution environments. This helps us understand the full extent of what the model can accomplish when given appropriate resources.")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/XVK_Image_36.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: The relationship between an AI system‚Äôs capabilities, propensities, affordances, and behaviors. ("),t("strong",[t("a",{attrs:{href:"https://static1.squarespace.com/static/6593e7097565990e65c886fd/t/65a6f1389754fc06cb9a7a14/1705439547455/auditing_framework_web.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Sharkey et al., 2024"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[t("strong",[e._v("How do affordances shape evaluation design?")]),e._v(" Understanding the relationship between affordances and capabilities helps us design more comprehensive evaluation protocols. For instance, when evaluating a model's coding capabilities, testing under minimal affordances might reveal concerning behaviors that are masked when the model has access to better tools. Maybe the model suggests unsafe coding practices when it can't verify its solutions through execution. Similarly, testing under maximal affordances might reveal emergent capabilities that aren't visible in more restricted environments - like how GPT-4's ability to play Minecraft only became apparent when given appropriate scaffolding and tools ("),t("a",{attrs:{href:"https://arxiv.org/abs/2305.16291",target:"_blank",rel:"noopener noreferrer"}},[e._v("Wang et al., 2023"),t("OutboundLink")],1),e._v(").")]),e._v(" "),t("p",[t("strong",[e._v("Quality assurance in evaluation design.")]),e._v(" Given how significantly affordances affect model behavior, we need systematic approaches to ensure our evaluations remain reliable and meaningful. This means carefully documenting what affordances were available during testing, verifying that affordance restrictions are properly enforced, and validating that our results are reproducible under similar conditions. For instance, when the U.S. and UK AI Safety Institutes evaluated Claude 3.5 Sonnet, they explicitly noted that their findings were preliminary due to testing under limited affordances and time constraints ("),t("a",{attrs:{href:"https://cdn.prod.website-files.com/663bd486c5e4c81588db7a1d/673b689ec926d8d32e889a8e_UK-US-Testing-Report-Nov-19.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("US & UK AISI, 2024"),t("OutboundLink")],1),e._v(").")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/6ja_Image_37.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: The relationship between absolute capabilities, affordances, contextual and reachable capabilities, and the level of auditing warranted. Absolute capabilities and available affordances are orthogonal. As either increase, the level of auditing required also increases. ("),t("strong",[t("a",{attrs:{href:"https://static1.squarespace.com/static/6593e7097565990e65c886fd/t/65a6f1389754fc06cb9a7a14/1705439547455/auditing_framework_web.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Sharkey et al. 2024"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[t("strong",[e._v("Moving beyond individual evaluations.")]),e._v(" While understanding affordances is crucial for designing individual evaluations, we also need to consider how different evaluation conditions work together as part of a larger system. A comprehensive evaluation protocol might start with minimal affordance testing to establish baseline capabilities, then progressively add affordances to understand how the model's behavior changes. This layered approach helps us build a more complete picture of model behavior while maintaining rigorous control over testing conditions.")]),e._v(" "),t("h2",{attrs:{id:"scaling-automation"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#scaling-automation"}},[e._v("#")]),e._v(" Scaling & Automation")]),e._v(" "),t("p",[t("strong",[e._v("Why do we need automated evaluations?")]),e._v(" Why do we need automated evaluations? In previous sections, we explored various evaluation techniques and conditions, but implementing these systematically faces a major challenge: scale. The rapidly increasing pace of AI development means we can't rely solely on manual evaluation processes. When OpenAI or Anthropic release a new model, dozens of independent researchers and organizations need to verify its capabilities and safety properties. Doing this manually for every evaluation under different affordance conditions would be prohibitively expensive and time-consuming. We need systems that can automatically run comprehensive evaluation suites while maintaining the careful control over conditions we discussed earlier.")]),e._v(" "),t("p",[t("strong",[e._v("Automating evaluation through model written evaluations (MWEs)")]),e._v(". As we scale up evaluations, we need to make strategic decisions about resource allocation. Do we run many quick evaluations under different conditions, or fewer evaluations with more thorough testing? One approach is using automated tools to do broad initial testing, then dedicating more resources to deep investigation of concerning behaviors. This tiered approach helps balance the need for comprehensive coverage with limited resources. One promising approach is using AI models themselves to help generate and run evaluations. Even current language models can write high-quality evaluation questions when properly prompted. ("),t("a",{attrs:{href:"https://arxiv.org/abs/2212.09251",target:"_blank",rel:"noopener noreferrer"}},[e._v("Perez et al., 2022"),t("OutboundLink")],1),e._v("). This approach might help address the scaling challenge by reducing the human effort needed to create new evaluations.")]),e._v(" "),t("p",[e._v('**How do model-written evaluations work? **The basic approach involves having an AI model generate evaluation questions based on a specific behavior or capability we want to test. For example, to evaluate power-seeking tendencies, we might prompt the model with a description of power-seeking behavior and ask it to generate relevant multiple-choice questions. These generated questions are then filtered using a second model that acts as a judge, scoring them for quality and relevance. To maintain diversity, researchers use various prompting techniques like "variant prompts" that encourage different question formats and scenarios. The final step involves human validation of a sample of questions to ensure quality ('),t("a",{attrs:{href:"https://www.alignmentforum.org/posts/yxdHp2cZeQbZGREEN/improving-model-written-evals-for-ai-safety-benchmarking",target:"_blank",rel:"noopener noreferrer"}},[e._v("Dev & Hobbhahn, 2024"),t("OutboundLink")],1),e._v(").")]),e._v(" "),t("p",[t("strong",[e._v("What have we learned about model-written evaluations?")]),e._v(" When comparing MWEs to human written evaluations (HWEs), researchers found some differences. Models often respond very differently to these two types of questions even when they're supposed to test the same property. For instance, Claude 3 Haiku showed a 25% power-seeking inclination on human-written questions but 88% on model-written ones - a 63 percentage point difference. The evaluations also formed distinct clusters in embedding space, suggesting systematic differences in how questions were formulated. Interestingly though, when graded by an LLM judge for quality, MWEs actually scored higher (mean=8.1) than HWEs (mean=7.2). This suggests that while MWEs can be high quality, we need to be careful about potential biases and ensure they're truly testing the same properties as human-written evaluations ("),t("a",{attrs:{href:"https://www.alignmentforum.org/posts/yxdHp2cZeQbZGREEN/improving-model-written-evals-for-ai-safety-benchmarking",target:"_blank",rel:"noopener noreferrer"}},[e._v("Dev & Hobbhahn, 2024"),t("OutboundLink")],1),e._v(").")]),e._v(" "),t("p",[t("strong",[e._v("Building automated evaluation pipelines")]),e._v('. Let\'s look at how researchers actually implement model-written evaluations. In Apollo Research\'s recent work, they developed a systematic protocol: First, they have a large language model like Claude 3.5 generate batches of 40 questions per API call using few-shot examples and clear evaluation criteria. The model outputs these in structured JSON format to ensure consistency. They then use a separate "judge" model (usually from a different model family to avoid bias) to score each generated question on quality and relevance. Any questions scoring below a threshold of 7 out of 10 are automatically discarded. To ensure coverage and diversity, they employ "variant prompts" - different framing instructions that push the model to generate questions from various angles. For example, one variant might request questions about real-world scenarios, while another focuses on hypothetical ethical dilemmas ('),t("a",{attrs:{href:"https://www.alignmentforum.org/posts/yxdHp2cZeQbZGREEN/improving-model-written-evals-for-ai-safety-benchmarking",target:"_blank",rel:"noopener noreferrer"}},[e._v("Dev & Hobbhahn, 2024"),t("OutboundLink")],1),e._v("). This automated pipeline can generate hundreds of high-quality evaluation questions in hours rather than the weeks it might take human evaluators.")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/clr_Image_38.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: Examples of model written evaluation questions. A RLHFd model replies to a political question and gives opposite answers to users who introduce themselves differently, in line with the users‚Äô views. Model-written biography text in italics. ("),t("strong",[t("a",{attrs:{href:"https://arxiv.org/abs/2212.09251",target:"_blank",rel:"noopener noreferrer"}},[e._v("Perez et al., 2022"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[e._v("However, we still face significant challenges in automating evaluations. First, we need to maintain quality as we scale - automated systems must be able to reliably enforce affordance conditions and detect potential evaluation failures. Second, generated evaluations need to be validated to ensure they're actually testing what we intend. As Apollo Research found, model-written evaluations sometimes had systematic blindspots or biases that needed to be corrected through careful protocol design ("),t("a",{attrs:{href:"https://www.alignmentforum.org/posts/yxdHp2cZeQbZGREEN/improving-model-written-evals-for-ai-safety-benchmarking",target:"_blank",rel:"noopener noreferrer"}},[e._v("Dev & Hobbhahn, 2024"),t("OutboundLink")],1),e._v(").")]),e._v(" "),t("h2",{attrs:{id:"integration-audits"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#integration-audits"}},[e._v("#")]),e._v(" Integration & Audits")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/Qxc_Image_39.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: The Aim is to avoid extreme risks from powerful misaligned model. ("),t("strong",[t("a",{attrs:{href:"https://arxiv.org/abs/2305.15324",target:"_blank",rel:"noopener noreferrer"}},[e._v("Shevlane et al. 2023"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[t("strong",[e._v("How do evaluations fit into the broader safety ecosystem?")]),e._v(" When we talk about evaluations being used in practice, we're really talking about two distinct but complementary processes. First, there are the specific evaluation techniques we've discussed in previous sections - the tools we use to measure particular capabilities, propensities, or safety properties. Second, there's the broader process of auditing that uses these evaluations alongside other analysis methods to make comprehensive safety assessments.")]),e._v(" "),t("p",[t("strong",[e._v("Why do we need multiple layers of evaluation?")]),e._v(' The UK AI Safety Institute\'s approach demonstrates why integration requires multiple complementary layers. Their evaluation framework incorporates regular security audits, ongoing monitoring systems, clear response protocols, and external oversight - creating what they call a "defense in depth" approach ('),t("a",{attrs:{href:"https://www.gov.uk/government/publications/ai-safety-institute-approach-to-evaluations/ai-safety-institute-approach-to-evaluations",target:"_blank",rel:"noopener noreferrer"}},[e._v("UK AISI, 2024"),t("OutboundLink")],1),e._v("). This layered strategy helps catch potential risks that might slip through any single evaluation method.")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/59T_Image_40.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: Defense in depth ("),t("strong",[t("a",{attrs:{href:"https://www.aisafetybook.com/textbook/component-failure-accident-models",target:"_blank",rel:"noopener noreferrer"}},[e._v("Hendrycks, 2024"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[t("strong",[e._v("What makes auditing different from evaluation?")]),e._v(" While evaluations are specific measurement tools, auditing is a systematic process of safety verification. An audit might employ multiple evaluations, but also considers broader factors like organizational processes, documentation, and safety frameworks. For example, when auditing a model for deployment readiness, we don't just run capability evaluations - we also examine training procedures, security measures, and incident response plans ("),t("a",{attrs:{href:"https://static1.squarespace.com/static/6593e7097565990e65c886fd/t/65a6f1389754fc06cb9a7a14/1705439547455/auditing_framework_web.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Sharkey et al., 2024"),t("OutboundLink")],1),e._v(").")]),e._v(" "),t("p",[t("strong",[e._v("What types of audits do we need?")]),e._v(" Different aspects of AI development require different types of audits:")]),e._v(" "),t("ul",[t("li",[t("p",[t("strong",[e._v("Training design audits")]),e._v(": These verify safety considerations throughout the development process. Before training begins, auditors examine factors like compute usage plans, training data content, and experiment design decisions. During training, they monitor for concerning capabilities or behaviors. For instance, training audits might look for indicators that scaling up model size might lead to dangerous capabilities ("),t("a",{attrs:{href:"https://www.anthropic.com/news/anthropics-responsible-scaling-policy",target:"_blank",rel:"noopener noreferrer"}},[e._v("Anthropic, 2024"),t("OutboundLink")],1),e._v(").")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Security audits")]),e._v(": These assess whether safety measures remain effective under adversarial conditions. This includes technical security (like model isolation and access controls) and organizational security (like insider threat prevention).")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Deployment audits")]),e._v(": These examine specific deployment scenarios. They consider questions like: Who will have access? What affordances will be available? What safeguards are needed? These audits help determine whether deployment plans adequately address potential risks. For example, deployment audits might assess both direct risks from model capabilities and potential emergent risks from real-world usage patterns of those capabilities ("),t("a",{attrs:{href:"https://www.apolloresearch.ai/blog/a-starter-guide-for-evals",target:"_blank",rel:"noopener noreferrer"}},[e._v("Apollo Research, 2024"),t("OutboundLink")],1),e._v(").")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Governance audits")]),e._v(": These look at organizational safety infrastructure. They verify that companies have appropriate processes, documentation requirements, and response protocols. This includes reviewing incident response plans, oversight mechanisms, and transparency practices ("),t("a",{attrs:{href:"https://arxiv.org/abs/2305.15324",target:"_blank",rel:"noopener noreferrer"}},[e._v("Shevlane et al., 2023"),t("OutboundLink")],1),e._v(").")])])]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/vic_Image_41.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: Determinants of AI system‚Äôs effects on the world and the types of auditing that act on them. ("),t("strong",[t("a",{attrs:{href:"https://static1.squarespace.com/static/6593e7097565990e65c886fd/t/65a6f1389754fc06cb9a7a14/1705439547455/auditing_framework_web.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Sharkey et al., 2024"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[t("strong",[e._v("How do evaluations support the audit process?")]),e._v(" Each type of audit uses different combinations of evaluations to gather evidence. For example, a deployment audit might use capability evaluations to establish upper bounds on risky behaviors, propensity evaluations to understand default tendencies, and control evaluations to verify safety measures. The audit process then synthesizes these evaluation results with other evidence to make safety assessments.")]),e._v(" "),t("p",[t("strong",[e._v("What role does independent auditing play?")]),e._v(" While internal auditing is crucial, independent external auditing provides additional assurance by bringing fresh perspectives and avoiding potential conflicts of interest. Organizations like METR and Apollo Research demonstrate how independent auditors can combine multiple evaluation techniques to provide comprehensive safety assessments. However, this ecosystem is still developing, and we need more capacity for independent evaluation of frontier AI systems  ("),t("a",{attrs:{href:"https://arxiv.org/abs/2305.15324",target:"_blank",rel:"noopener noreferrer"}},[e._v("Shevlane et al., 2023"),t("OutboundLink")],1),e._v("). We talk more about bottlenecks to third party auditing in the limitations section.")]),e._v(" "),t("p",[t("strong",[e._v("How do we maintain safety over time?")]),e._v(" After deployment, we need ongoing monitoring and periodic re-auditing for several reasons. First, we need to catch unanticipated behaviors that emerge from real-world usage. Second, we need to evaluate any model updates or changes in deployment conditions. Third, we need to verify that safety measures remain effective. This creates a feedback loop where deployment observations inform future evaluation design and audit procedures ("),t("a",{attrs:{href:"https://arxiv.org/abs/2305.15324",target:"_blank",rel:"noopener noreferrer"}},[e._v("Shevlane et al., 2023"),t("OutboundLink")],1),e._v("; "),t("a",{attrs:{href:"https://arxiv.org/abs/2403.13793",target:"_blank",rel:"noopener noreferrer"}},[e._v("Phuong et al., 2024"),t("OutboundLink")],1),e._v("; "),t("a",{attrs:{href:"https://www.gov.uk/government/publications/ai-safety-institute-approach-to-evaluations/ai-safety-institute-approach-to-evaluations",target:"_blank",rel:"noopener noreferrer"}},[e._v("UK AISI, 2024"),t("OutboundLink")],1),e._v("; "),t("a",{attrs:{href:"https://static1.squarespace.com/static/6593e7097565990e65c886fd/t/65a6f1389754fc06cb9a7a14/1705439547455/auditing_framework_web.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Sharkey et al., 2024"),t("OutboundLink")],1),e._v(").")]),e._v(" "),t("p",[t("strong",[e._v("Going from evaluation to action")]),e._v(". For evaluations and audits to have real impact, they must connect directly to decision-making processes. Both evaluations and audits need clear, predefined thresholds that trigger specific actions. For example, concerning results from capability evaluations during training might trigger automatic pauses in model scaling. Failed security audits should require immediate implementation of additional controls. Poor deployment audit results should modify or halt deployment plans ("),t("a",{attrs:{href:"https://static1.squarespace.com/static/6593e7097565990e65c886fd/t/65a6f1389754fc06cb9a7a14/1705439547455/auditing_framework_web.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Sharkey et al., 2024"),t("OutboundLink")],1),e._v(").")]),e._v(" "),t("p",[t("strong",[e._v("These action triggers need to be integrated into broader governance frameworks")]),e._v('. As we discussed in the chapter on governance, many organizations are developing Responsible Scaling Policies (RSPs) that use evaluation results as "gates" for development decisions. However, without strong governance frameworks and enforcement mechanisms, there\'s a risk that evaluations and audits become mere checkbox exercises - what some researchers call "safetywashing". We\'ll explore these limitations and potential failure modes in more detail in the next section.')]),e._v(" "),t("h1",{attrs:{id:"limitations-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#limitations-2"}},[e._v("#")]),e._v(" Limitations")]),e._v(" "),t("p",[e._v("Previous sections outlined various evaluation techniques and methodologies, but building out a proper safety infrastructure means that we should also maintain an appropriate amount of skepticism about evaluation results and avoid overconfidence in safety assessments. So the last section of this chapter is dedicated to exploring the limitations, constraints and challenges to AI evaluations.")]),e._v(" "),t("h2",{attrs:{id:"fundamental-challenges"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#fundamental-challenges"}},[e._v("#")]),e._v(" Fundamental Challenges")]),e._v(" "),t("p",[e._v("**Absence of evidence vs. evidence of absence. **A core challenge in AI evaluation is the fundamental asymmetry between proving the presence versus absence of capabilities or risks. While evaluations can definitively confirm that a model possesses certain capabilities or exhibits particular behaviors, they cannot conclusively prove the absence of concerning capabilities or behaviors ("),t("a",{attrs:{href:"https://www.anthropic.com/news/evaluating-ai-systems",target:"_blank",rel:"noopener noreferrer"}},[e._v("Anthropic, 2024"),t("OutboundLink")],1),e._v("). This asymmetry creates a persistent uncertainty in safety assessments - even if a model passes numerous safety evaluations, we cannot be certain it is truly safe.")]),e._v(" "),t("p",[t("strong",[e._v("The challenge of negative results")]),e._v(". Related to this asymmetry is the difficulty of interpreting negative results in AI evaluation. When a model fails to demonstrate a capability or behavior during testing, this could indicate either a genuine limitation of the model or simply a failure of our evaluation methods to properly elicit the capability. The discovery that GPT-4 could effectively control Minecraft through appropriate scaffolding, despite initially appearing incapable of such behavior, illustrates this challenge. This uncertainty about negative results becomes particularly problematic when evaluating safety-critical properties, where false negatives could have severe consequences.")]),e._v(" "),t("p",[t("strong",[e._v("The problem of unknown unknowns")]),e._v(". Perhaps most fundamentally, our evaluation methods are limited by our ability to anticipate what we need to evaluate. As AI systems become more capable, they might develop behaviors or capabilities that we haven't thought to test for. This suggests that our evaluation frameworks might be missing entire categories of capabilities or risks simply because we haven't conceived of them yet.")]),e._v(" "),t("h2",{attrs:{id:"technical-challenges"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#technical-challenges"}},[e._v("#")]),e._v(" Technical Challenges")]),e._v(" "),t("p",[t("strong",[e._v("The challenge of measurement precision.")]),e._v(" The extreme sensitivity of language models to minor changes in evaluation conditions poses a fundamental challenge to reliable measurement. As we discussed in earlier sections about evaluation techniques, even subtle variations in prompt formatting can lead to dramatically different results. Research has also shown that performance can vary by up to 76 percent based on seemingly trivial changes in few-shot prompting formats ("),t("a",{attrs:{href:"https://arxiv.org/abs/2310.11324",target:"_blank",rel:"noopener noreferrer"}},[e._v("Scalar et al., 2023"),t("OutboundLink")],1),e._v("). As another example, changes as minor as switching from alphabetical to numerical option labeling (e.g., from (A) to (1)), altering bracket styles, or adding a single space can result in accuracy fluctuations of around 5 percentage points. ("),t("a",{attrs:{href:"https://www.anthropic.com/news/evaluating-ai-systems",target:"_blank",rel:"noopener noreferrer"}},[e._v("Anthropic, 2024"),t("OutboundLink")],1),e._v(") This isn't just a minor inconvenience - it raises serious questions about the reliability and reproducibility of our evaluation results. When small changes in input formatting can cause such large variations in measured performance, how can we trust our assessments of model capabilities or safety properties?")]),e._v(" "),t("p",[t("strong",[e._v("Sensitivity to environmental factors")]),e._v(". Beyond just measurement issues due to prompt formatting, evaluation results can be affected by various environmental factors that we might not even be aware of. Just like the unknown unknowns that we highlighted in the last section, this might include things like the specific version of the model being tested, the temperature and other sampling parameters used, the compute infrastructure running the evaluation and maybe even the time of day or system load during testing.")]),e._v(" "),t("p",[t("strong",[e._v("The curse of dimensionality in task space.")]),e._v(" Every time we add a new dimension to what we want to evaluate, the space of possible scenarios expands exponentially. We have outlined many times in this chapter that a lot of the capabilities become problematic when paired together, like long term planning paired with situational awareness or deception. If the combinatorial complexity keeps increasing with every new problematic capability, then each of these dimensions multiplies the number of test cases needed for comprehensive coverage. It directly impacts our ability to make confident claims about model safety. If we can only test a tiny fraction of possible scenarios, how can we be sure we haven't missed critical failure modes?")]),e._v(" "),t("p",[t("strong",[e._v("The resource reality")]),e._v(". Related to the problem of exploding number of evaluations is the sheer computational cost of running thorough evaluations. This creates another hard limit on what we can practically test. Making over 100,000 API calls just to properly assess performance on a single benchmark becomes prohibitively expensive when scaled across multiple capabilities and safety concerns. Independent researchers and smaller organizations often can't afford such comprehensive testing, and even if money isn't a bottleneck because you have state sponsorship, GPUs currently absolutely are. This can lead to potential blind spots in our understanding of model behavior. The resource constraints become even more pressing when we consider the need for repeated testing as models are updated or new capabilities emerge.")]),e._v(" "),t("p",[e._v('??? quote "US & UK AI Safety Institute Evaluation of Claude 3.5 Sonnet ('),t("a",{attrs:{href:"https://www.aisi.gov.uk/work/pre-deployment-evaluation-of-anthropics-upgraded-claude-3-5-sonnet",target:"_blank",rel:"noopener noreferrer"}},[e._v("US & UK AISI, 2024"),t("OutboundLink")],1),e._v(') "')]),e._v(" "),t("tab",[t("p",[t("em",[e._v("While these tests were conducted in line with current best practices, the findings should be considered preliminary. These tests were conducted in a limited time period with finite resources, which if extended could expand the scope of findings and the subsequent conclusions drawn.")])])]),e._v(" "),t("p",[t("strong",[e._v("The blind spots of behavioral testing.")]),e._v(" We have talked about this numerous times in previous sections, but it's worth mentioning again. Watching what a model does can tell us a lot, but it's still like trying to understand how a person thinks by only observing their actions. Our heavy reliance on behavioral testing - analyzing model outputs - leaves us with significant blind spots about how models actually work. A model might give the right answers while using internal strategies we can't detect. So in addition to being limited by breadth due to combinatorial complexity of the number of things we need to test, we are also limited on the depth of evaluation until we make true progress on interpretability. Unless interpretability makes progress, evaluations have a definite upper limit on how much safety they can guarantee.")]),e._v(" "),t("p",[t("strong",[e._v("What can we learn from cognitive science?")]),e._v(" Some researchers have suggested taking more inspiration from the cognitive sciences for insights for improving AI evaluation methodology. Over decades, researchers in comparative psychology and psychometrics have developed sophisticated techniques for assessing intelligence and capabilities across different types of minds - from human infants to various animal species. These fields have extensive experience with crucial challenges that AI evaluation faces today, like establishing construct validity, controlling for confounding explanations, and measuring capabilities that can't be directly observed. However, this adaptation requires careful consideration of AI systems' unique characteristics. We can't simply transplant techniques designed for biological minds, but we can learn from their methodological rigor. For example, developmental psychology's systematic approaches to testing object permanence or theory of mind could inform how we design evaluations for similar capabilities in AI systems. Similarly, psychometrics' sophisticated frameworks for validating measurement constructs could help ensure our AI evaluations actually measure what we intend them to measure. ("),t("a",{attrs:{href:"https://arxiv.org/abs/2407.09221",target:"_blank",rel:"noopener noreferrer"}},[e._v("Burden, 2024"),t("OutboundLink")],1),e._v(")")]),e._v(" "),t("p",[t("strong",[e._v("The problem of trying to estimate ‚ÄúMaximum Capabilities‚Äù")]),e._v(". Also related to the combinatorial complexity problem is the problem of emergence. We frequently discover that models can do things we thought impossible when given the right scaffolding or tools. The Voyager project demonstrated this when it used black box queries to GPT-4's revealing the unexpected ability to play Minecraft ("),t("a",{attrs:{href:"https://arxiv.org/abs/2305.16291",target:"_blank",rel:"noopener noreferrer"}},[e._v("Wang et al., 2023"),t("OutboundLink")],1),e._v("). Either through continued increases in scale, or through simple combination of some scaffolding approach that researchers hadn‚Äôt considered before, a model might display some significant step change in capabilities or propensities that we had not anticipated before. This emergent behavior is particularly difficult to evaluate because it often arises from the interaction between capabilities. There are few if any ways to guarantee that the whole will not become more than the sum of its parts. This makes it extremely difficult to predict what a model might be capable of in real-world situations, even if we thoroughly test its individual capabilities.")]),e._v(" "),t("p",[e._v('??? note "Case study of capability surprises: Kalamang"')]),e._v(" "),t("tab",[t("p",[t("strong",[e._v("Gemini is able to translate language in a zero-shot way, without any training.")]),e._v("  The case of Gemini's ability to translate Kalamang - a language spoken by only 100 people in Papua New Guinea - demonstrates how models can acquire unexpected capabilities when provided with the right context. It's highly unlikely that this language was part of the training data of any major AI model. Researchers at Google DeepMind demonstrated that by leveraging a sufficiently large context window‚Äîaround a million tokens‚Äîthey could input a full grammar book and dictionary of Kalamang. When asked to translate a sentence from Kalamang to English, the model was able to perform this task. ("),t("a",{attrs:{href:"https://arxiv.org/abs/2309.16575",target:"_blank",rel:"noopener noreferrer"}},[e._v("Tanzer et al., 2024"),t("OutboundLink")],1),e._v(")")])]),e._v(" "),t("h2",{attrs:{id:"systemic-limitations"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#systemic-limitations"}},[e._v("#")]),e._v(" Systemic Limitations")]),e._v(" "),t("p",[t("img",{attrs:{src:"Images/6Xp_Image_42.png",alt:"Enter image alt description"}})]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Figure")])]),e._v("*: Illustration of sandbagging ("),t("strong",[t("a",{attrs:{href:"https://www.alignmentforum.org/posts/jsmNCj9QKcfdg8fJk/an-introduction-to-ai-sandbagging",target:"_blank",rel:"noopener noreferrer"}},[e._v("van der Weij, 2024"),t("OutboundLink")],1)]),e._v(")*")]),e._v(" "),t("p",[t("strong",[e._v("What is sandbagging, and why might developers do it?")]),e._v(" Sandbagging refers to strategic underperformance on evaluations, where an AI system is intentionally made to appear less capable than it actually is. In the immediate term, developers face various pressures that could incentivize sandbagging during evaluations. Regulatory requirements and safety standards may impose capability thresholds above which additional oversight, restrictions, or costs are incurred. For instance, the EU AI Act requires more stringent controls and third-party auditing for high-risk AI systems ("),t("a",{attrs:{href:"https://artificialintelligenceact.eu/the-act/",target:"_blank",rel:"noopener noreferrer"}},[e._v("European Commission, 2024"),t("OutboundLink")],1),e._v("). This creates a direct financial incentive to appear below these thresholds. An example of something like this is the Volkswagen emissions scandal, where cars were designed to detect and alter their behavior during emissions testing to avoid costly regulations ("),t("a",{attrs:{href:"https://www.alignmentforum.org/posts/jsmNCj9QKcfdg8fJk/an-introduction-to-ai-sandbagging",target:"_blank",rel:"noopener noreferrer"}},[e._v("van der Weij et al., 2024"),t("OutboundLink")],1),e._v("). Current research shows that even today's language models can be made to selectively underperform in ways that are difficult to detect through standard evaluations, particularly on tests for dangerous capabilities while maintaining performance on general benchmarks ("),t("a",{attrs:{href:"https://arxiv.org/abs/2406.07358",target:"_blank",rel:"noopener noreferrer"}},[e._v("van der Weij et al., 2024"),t("OutboundLink")],1),e._v(").")]),e._v(" "),t("p",[e._v("In the future, companies may want to downplay certain capabilities of their systems to avoid drawing regulatory attention or public concern, while still being able to deploy these capabilities in practice. This allows them to maintain development and deployment speed without triggering additional oversight. For example, if an AI system shows strong capabilities in areas that might raise security concerns, developers might be motivated to have it underperform specifically on those evaluations while maintaining high performance on general benchmarks (van der Weij et al., 2024, AI Sandbagging).to maintain development and deployment speed without triggering additional oversight.")]),e._v(" "),t("p",[t("strong",[e._v("Misalignment concerns with sandbagging")]),e._v(". In the future, there might be concerns about AI systems themselves choosing to sandbag, independent of developer intentions. An AI system might deliberately hide its capabilities to avoid modifications or restrictions that would prevent it from achieving its goals ("),t("a",{attrs:{href:"https://www.alignmentforum.org/posts/dBmfb76zx6wjPsBC7/when-can-we-trust-model-evaluations",target:"_blank",rel:"noopener noreferrer"}},[e._v("Hubinger, 2023"),t("OutboundLink")],1),e._v("). We talked more about why this is a concern in the subsection on scheming. While we don't yet have evidence of AI systems engaging in this kind of strategic deception, and current language models don't show signs of this type of autonomous strategic behavior, this possibility becomes more concerning as AI systems become more capable.")]),e._v(" "),t("p",[t("strong",[e._v("What does the potential for sandbagging imply for the field of evaluations?")]),e._v(" The possibility of sandbagging has big implications for how we approach AI evaluations, both now and in the future. Simply having access to model outputs (black-box testing) is insufficient for rigorous evaluation ("),t("a",{attrs:{href:"https://arxiv.org/abs/2401.14446",target:"_blank",rel:"noopener noreferrer"}},[e._v("Casper et al., 2024"),t("OutboundLink")],1),e._v("). Instead, evaluators need deeper access to model internals and training procedures to detect potential sandbagging attempts. This might require new institutional arrangements, such as trusted third-party evaluators with secured access to model weights and architectures. Without addressing this challenge, our ability to make informed decisions about AI development and deployment based on evaluation results becomes severely compromised.")]),e._v(" "),t("p",[t("strong",[e._v("The gap between evaluation and deployment.")]),e._v(" Current evaluation approaches often fail to capture the true complexity of real-world deployment contexts. While we can measure model performance in controlled settings, these evaluations rarely reflect how systems will behave when embedded in complex sociotechnical environments. A model that performs safely in isolated testing might behave very differently when interacting with real users, facing novel scenarios, or operating within broader societal systems. For instance, a medical advice model might perform well on standard benchmarks but pose serious risks when deployed in healthcare settings where users lack the expertise to validate its recommendations. This disconnect between evaluation and deployment contexts means we might miss critical safety issues that only emerge through complex human-AI interactions or system-level effects ("),t("a",{attrs:{href:"https://arxiv.org/abs/2310.11986",target:"_blank",rel:"noopener noreferrer"}},[e._v("Weidinger et al., 2023"),t("OutboundLink")],1),e._v(")")]),e._v(" "),t("p",[t("strong",[e._v("The challenge of domain-specific evaluations")]),e._v(". Evaluating AI systems in high-stakes domains presents unique challenges that our current approaches struggle to address. Consider the evaluation of AI systems for potential misuse in biosecurity - this requires not just technical understanding of AI systems and evaluation methods, but also deep expertise in biology, biosafety protocols, and potential threat vectors. This combination of expertise is exceedingly rare, making thorough evaluation in these critical domains particularly challenging. Similar challenges arise in other specialized fields like cybersecurity, where the complexity of potential attack vectors and system vulnerabilities requires sophisticated domain knowledge to properly assess. The difficulty of finding evaluators with sufficient cross-domain expertise often leads to evaluations that miss important domain-specific risks or fail to anticipate novel forms of misuse. Besides just designing evaluations being challenging, running the evaluation for such high stakes domains is also challenging. We need to elicit the maximum capabilities of a model to generate pathogens or malware to test what it is capable of. This can be obviously problematic if not handled extremely carefully.")]),e._v(" "),t("p",[t("strong",[e._v("The multi-modal evaluation gap")]),e._v(". This is more of a gap due to how nascent the field of evaluations is, rather than a limitation of evaluations themselves. As AI systems increasingly incorporate multiple modalities like text, images, audio, and video, our evaluation methods struggle to keep pace. Most current evaluation frameworks focus primarily on text-based outputs, leaving significant gaps in our ability to assess risks and capabilities across other modalities. This limitation becomes particularly problematic as models are built to generate and manipulate multi-modal content. A model might appear safe when evaluated purely on its text generation, but could pose unexpected risks through its ability to generate or manipulate images or videos. The interaction between different modalities creates new vectors for potential harm that our current evaluation methods might miss entirely. This also yet again adds another layer to the curse of dimensionality and expands the task space of model evaluations. ("),t("a",{attrs:{href:"https://arxiv.org/abs/2310.11986",target:"_blank",rel:"noopener noreferrer"}},[e._v("Weidinger et al., 2023"),t("OutboundLink")],1),e._v(")")]),e._v(" "),t("h2",{attrs:{id:"governance-limitations"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#governance-limitations"}},[e._v("#")]),e._v(" Governance Limitations")]),e._v(" "),t("p",[t("br"),e._v(" "),t("strong",[e._v("The policy-evaluation gap")]),e._v('. Current legislative and regulatory efforts around AI safety are severely hampered by the lack of robust, standardized evaluation methods. Consider the recent executive orders and proposed regulations requiring safety assessments of AI systems - how can governments enforce these requirements when we lack reliable ways to measure what makes an AI system "safe"? This creates a circular problem: regulators need evaluation standards to create effective policies, but the development of these standards is partly driven by regulatory requirements. The U.S. government\'s recent initiative to have various agencies evaluate AI capabilities in cybersecurity and biosecurity highlights this challenge. These agencies, despite their expertise in their respective domains, often lack the specialized knowledge needed to evaluate advanced AI systems comprehensively. ('),t("a",{attrs:{href:"https://arxiv.org/abs/2310.11986",target:"_blank",rel:"noopener noreferrer"}},[e._v("Weidinger et al., 2023"),t("OutboundLink")],1),e._v(")")]),e._v(" "),t("p",[t("strong",[e._v("The role of independent evaluation.")]),e._v(" Currently, most significant AI evaluation research happens within the same companies developing these systems. While these companies often have the best resources and expertise to conduct evaluations, this creates an inherent conflict of interest. Companies might be incentivized to design evaluations that their systems are likely to pass or to downplay concerning results. This is also sometimes called ‚Äúsafety washing‚Äù (akin to greenwashing). To address the challenges around evaluation independence and expertise, we need to significantly expand the ecosystem of independent evaluation organizations. The emergence of independent evaluation organizations like Model Evaluation for Trustworthy AI (METR) and Apollo Research represents an important step toward addressing this issue, but these organizations often face significant barriers including limited model access ("),t("a",{attrs:{href:"https://ailabwatch.org/blog/external-evaluation/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Perlman, 2024"),t("OutboundLink")],1),e._v("), resource constraints, and difficulties matching the technical capabilities of major AI labs.")]),e._v(" "),t("p",[e._v("Overall, while the limitations we've discussed in this final section are significant, they aren't insurmountable. Progress in areas like mechanistic interpretability, formal verification methods, and evaluation protocols shows promise for addressing many current limitations. However, overcoming these challenges requires sustained effort and investment.")]),e._v(" "),t("h1",{attrs:{id:"conclusion"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#conclusion"}},[e._v("#")]),e._v(" Conclusion")]),e._v(" "),t("p",[e._v("The field of AI evaluation has evolved significantly from simple benchmarking to comprehensive protocols designed to assess capabilities, propensities, and control. As we've explored throughout this chapter, each type of evaluation offers distinct insights: capability evaluations tell us what AI systems can do, propensity evaluations reveal their behavioral tendencies, and control evaluations verify our ability to maintain safety even under adversarial conditions.")]),e._v(" "),t("p",[e._v("However, we must remain mindful of the fundamental limitations in our current approaches. The challenge isn't just technical - it's conceptual. How do we prove the absence of dangerous capabilities? How do we ensure our evaluations remain meaningful when systems might actively try to game them? These questions become increasingly pressing as AI systems grow more sophisticated.")]),e._v(" "),t("p",[e._v("Looking ahead, the development of robust evaluation methods will be crucial for AI governance and safety. We need evaluations that can provide meaningful safety guarantees while remaining practical to implement. This likely requires combining multiple approaches - using behavioral techniques alongside interpretability tools, pairing capability assessments with propensity measurements, and verifying control protocols through adversarial testing.")]),e._v(" "),t("p",[e._v("The future of AI evaluation will require continued innovation in both techniques and frameworks. As we push toward more capable AI systems, our ability to effectively evaluate them may become one of the key factors determining whether we can harness their benefits while managing their risks. The challenges are significant, but the work of developing better evaluation methods remains one of our most important tools for ensuring safe and beneficial AI development.")]),e._v(" "),t("p",[e._v("We hope that reading this text inspires you to think and act about how to build and improve them!")]),e._v(" "),t("Citations")],1)}),[],!1,null,null,null);t.default=n.exports}}]);